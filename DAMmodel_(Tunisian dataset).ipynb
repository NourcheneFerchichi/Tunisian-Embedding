{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DAMmodel.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "yn6sM7iI2t2F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# loading data"
      ]
    },
    {
      "metadata": {
        "id": "rNJ-Qy3RzwqY",
        "colab_type": "code",
        "outputId": "50f2cf24-e74b-484f-ad66-4e41a8b38d9b",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2b6653bc-9fbc-4851-96cf-f20362c07fe5\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-2b6653bc-9fbc-4851-96cf-f20362c07fe5\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving data_small.pkl to data_small.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "U0o3um_PoiMF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# ---------------------------------------------------------------------------------------------"
      ]
    },
    {
      "metadata": {
        "id": "rlsj4crAn6N0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# evaluation.py"
      ]
    },
    {
      "metadata": {
        "id": "_5bEDNmZn1Zg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## evaluation.py\n",
        "\n",
        "\n",
        "import sys\n",
        "\n",
        "def get_p_at_n_in_m(data, n, m, ind):\n",
        "    pos_score = data[ind][0]\n",
        "    curr = data[ind:ind+m]\n",
        "    curr = sorted(curr, key = lambda x:x[0], reverse=True)\n",
        "\n",
        "    if curr[n-1][0] <= pos_score:\n",
        "        return 1\n",
        "    return 0\n",
        "\n",
        "def evaluate(file_path):\n",
        "    data = []\n",
        "    with open(file_path, 'r') as file:\n",
        "        for line in file:\n",
        "            line = line.strip()\n",
        "            tokens = line.split(\"\\t\")\n",
        "        \n",
        "            if len(tokens) != 2:\n",
        "                continue\n",
        "            \n",
        "            data.append((float(tokens[0]), int(tokens[1])))\n",
        "\n",
        "\t#assert len(data) % 10 == 0\n",
        "\n",
        "    p_at_1_in_2  = 0.0\n",
        "    p_at_1_in_10 = 0.0\n",
        "    p_at_2_in_10 = 0.0\n",
        "    p_at_5_in_10 = 0.0\n",
        "    \n",
        "    length = len(data) // 10\n",
        "\n",
        "    for i in range(0, length):\n",
        "        ind = i * 10\n",
        "        assert data[ind][1] == 1\n",
        "        \n",
        "        p_at_1_in_2  += get_p_at_n_in_m(data, 1, 2, ind)\n",
        "        p_at_1_in_10 += get_p_at_n_in_m(data, 1, 10, ind)\n",
        "        p_at_2_in_10 += get_p_at_n_in_m(data, 2, 10, ind)\n",
        "        p_at_5_in_10 += get_p_at_n_in_m(data, 5, 10, ind)\n",
        "        \n",
        "    return (p_at_1_in_2/length, p_at_1_in_10/length, p_at_2_in_10/length, p_at_5_in_10/length)\n",
        "\t"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1xBjglvnnHrt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# reader.py"
      ]
    },
    {
      "metadata": {
        "id": "xt2L7e7RnAtd",
        "colab_type": "code",
        "outputId": "7f5d1427-d84b-4919-b3df-90eaafca1455",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "cell_type": "code",
      "source": [
        "## reader.py\n",
        "\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "def unison_shuffle(data, seed=None):\n",
        "    if seed is not None:\n",
        "        np.random.seed(seed)\n",
        "\n",
        "    y = np.array(data['y'])\n",
        "    c = np.array(data['c'])\n",
        "    r = np.array(data['r'])\n",
        "\n",
        "    assert len(y) == len(c) == len(r)\n",
        "    p = np.random.permutation(len(y))\n",
        "    shuffle_data = {'y': y[p], 'c': c[p], 'r': r[p]}\n",
        "    return shuffle_data\n",
        "\n",
        "def split_c(c, split_id):\n",
        "    '''c is a list, example context\n",
        "       split_id is a integer, conf[_EOS_]\n",
        "       return nested list\n",
        "    '''\n",
        "    turns = [[]]\n",
        "    for _id in c:\n",
        "        if _id != split_id:\n",
        "            turns[-1].append(_id)\n",
        "        else:\n",
        "            turns.append([])\n",
        "    if turns[-1] == [] and len(turns) > 1:\n",
        "        turns.pop()\n",
        "    return turns\n",
        "\n",
        "def normalize_length(_list, length, cut_type='tail'):\n",
        "    '''_list is a list or nested list, example turns/r/single turn c\n",
        "       cut_type is head or tail, if _list len > length is used\n",
        "       return a list len=length and min(read_length, length)\n",
        "    '''\n",
        "    real_length = len(_list)\n",
        "    if real_length == 0:\n",
        "        return [0]*length, 0\n",
        "\n",
        "    if real_length <= length:\n",
        "        if not isinstance(_list[0], list):\n",
        "            _list.extend([0]*(length - real_length))\n",
        "        else:\n",
        "            _list.extend([[]]*(length - real_length))\n",
        "        return _list, real_length\n",
        "\n",
        "    if cut_type == 'head':\n",
        "        return _list[:length], length\n",
        "    if cut_type == 'tail':\n",
        "        return _list[-length:], length\n",
        "\n",
        "def produce_one_sample(data, index, split_id, max_turn_num, max_turn_len, turn_cut_type='tail', term_cut_type='tail'):\n",
        "    '''max_turn_num=10\n",
        "       max_turn_len=50\n",
        "       return y, nor_turns_nor_c, nor_r, turn_len, term_len, r_len\n",
        "    '''\n",
        "    c = data['c'][index]\n",
        "    r = data['r'][index][:]\n",
        "    y = data['y'][index]\n",
        "\n",
        "    turns = split_c(c, split_id)\n",
        "    #normalize turns_c length, nor_turns length is max_turn_num\n",
        "    nor_turns, turn_len = normalize_length(turns, max_turn_num, turn_cut_type)\n",
        "\n",
        "    nor_turns_nor_c = []\n",
        "    term_len = []\n",
        "    #nor_turn_nor_c length is max_turn_num, element is a list length is max_turn_len\n",
        "    for c in nor_turns:\n",
        "        #nor_c length is max_turn_len\n",
        "        nor_c, nor_c_len = normalize_length(c, max_turn_len, term_cut_type)\n",
        "        nor_turns_nor_c.append(nor_c)\n",
        "        term_len.append(nor_c_len)\n",
        "\n",
        "    nor_r, r_len = normalize_length(r, max_turn_len, term_cut_type)\n",
        "\n",
        "    return y, nor_turns_nor_c, nor_r, turn_len, term_len, r_len\n",
        "\n",
        "def build_one_batch(data, batch_index, conf, turn_cut_type='tail', term_cut_type='tail'):\n",
        "    _turns = []\n",
        "    _tt_turns_len = []\n",
        "    _every_turn_len = []\n",
        "\n",
        "    _response = []\n",
        "    _response_len = []\n",
        "\n",
        "    _label = []\n",
        "\n",
        "    for i in range(conf['batch_size']):\n",
        "        index = batch_index * conf['batch_size'] + i\n",
        "        y, nor_turns_nor_c, nor_r, turn_len, term_len, r_len = produce_one_sample(data, index, conf['_EOS_'], conf['max_turn_num'],\n",
        "                conf['max_turn_len'], turn_cut_type, term_cut_type)\n",
        "\n",
        "        _label.append(y)\n",
        "        _turns.append(nor_turns_nor_c)\n",
        "        _response.append(nor_r)\n",
        "        _every_turn_len.append(term_len)\n",
        "        _tt_turns_len.append(turn_len)\n",
        "        _response_len.append(r_len)\n",
        "\n",
        "    return _turns, _tt_turns_len, _every_turn_len, _response, _response_len, _label\n",
        "\n",
        "def build_one_batch_dict(data, batch_index, conf, turn_cut_type='tail', term_cut_type='tail'):\n",
        "    _turns, _tt_turns_len, _every_turn_len, _response, _response_len, _label = build_one_batch(data, batch_index, conf, turn_cut_type, term_cut_type)\n",
        "    ans = {'turns': _turns,\n",
        "            'tt_turns_len': _tt_turns_len,\n",
        "            'every_turn_len': _every_turn_len,\n",
        "            'response': _response,\n",
        "            'response_len': _response_len,\n",
        "            'label': _label}\n",
        "    return ans\n",
        "    \n",
        "\n",
        "def build_batches(data, conf, turn_cut_type='tail', term_cut_type='tail'):\n",
        "    _turns_batches = []\n",
        "    _tt_turns_len_batches = []\n",
        "    _every_turn_len_batches = []\n",
        "\n",
        "    _response_batches = []\n",
        "    _response_len_batches = []\n",
        "\n",
        "    _label_batches = []\n",
        "\n",
        "    batch_len = len(data['y'])//conf['batch_size']\n",
        "    for batch_index in range(batch_len):\n",
        "        _turns, _tt_turns_len, _every_turn_len, _response, _response_len, _label = build_one_batch(data, batch_index, conf, turn_cut_type='tail', term_cut_type='tail')\n",
        "\n",
        "        _turns_batches.append(_turns)\n",
        "        _tt_turns_len_batches.append(_tt_turns_len)\n",
        "        _every_turn_len_batches.append(_every_turn_len)\n",
        "\n",
        "        _response_batches.append(_response)\n",
        "        _response_len_batches.append(_response_len)\n",
        "\n",
        "        _label_batches.append(_label)\n",
        "\n",
        "    ans = { \n",
        "        \"turns\": _turns_batches, \"tt_turns_len\": _tt_turns_len_batches, \"every_turn_len\":_every_turn_len_batches,\n",
        "        \"response\": _response_batches, \"response_len\": _response_len_batches, \"label\": _label_batches\n",
        "    }   \n",
        "\n",
        "    return ans "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-74cee042ce12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;34m\"_EOS_\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m28270\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     }\n\u001b[0;32m--> 152\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../../data/data_small.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'load data success'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../data/data_small.pkl'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "Jp1NxunmmD2V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# operations.py"
      ]
    },
    {
      "metadata": {
        "id": "ElViLTTQxAbF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##operations.py\n",
        "\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "from scipy.stats import multivariate_normal\n",
        "import tensorflow as tf\n",
        "\n",
        "def learning_rate(step_num, d_model=512, warmup_steps=4000):\n",
        "    a = step_num**(-0.5)\n",
        "    b = step_num*warmup_steps**(-1.5)\n",
        "    return a, b, d_model**(-0.5) * min(step_num**(-0.5), step_num*(warmup_steps**(-1.5))) \n",
        "\n",
        "def selu(x):\n",
        "    alpha = 1.6732632423543772848170429916717\n",
        "    scale = 1.0507009873554804934193349852946\n",
        "    print('use selu')\n",
        "    return scale*tf.where(x>=0.0, x, alpha*tf.nn.elu(x))\n",
        "\n",
        "def bilinear_sim_4d(x, y, is_nor=True):\n",
        "    '''calulate bilinear similarity with two 4d tensor.\n",
        "    \n",
        "    Args:\n",
        "        x: a tensor with shape [batch, time_x, dimension_x, num_stacks]\n",
        "        y: a tensor with shape [batch, time_y, dimension_y, num_stacks]\n",
        "\n",
        "    Returns:\n",
        "        a tensor with shape [batch, time_x, time_y, num_stacks]\n",
        "\n",
        "    Raises:\n",
        "        ValueError: if\n",
        "            the shapes of x and y are not match;\n",
        "            bilinear matrix reuse error.\n",
        "    '''\n",
        "    M = tf.get_variable(\n",
        "        name=\"bilinear_matrix\", \n",
        "        shape=[x.shape[2], y.shape[2], x.shape[3]],\n",
        "        dtype=tf.float32,\n",
        "        initializer=tf.orthogonal_initializer())\n",
        "    sim = tf.einsum('biks,kls,bjls->bijs', x, M, y)\n",
        "\n",
        "    if is_nor:\n",
        "        scale = tf.sqrt(tf.cast(x.shape[2] * y.shape[2], tf.float32))\n",
        "        scale = tf.maximum(1.0, scale)\n",
        "        return sim / scale\n",
        "    else:\n",
        "        return sim\n",
        "\n",
        "\n",
        "def bilinear_sim(x, y, is_nor=True):\n",
        "    '''calculate bilinear similarity with two tensor.\n",
        "    Args:\n",
        "        x: a tensor with shape [batch, time_x, dimension_x]\n",
        "        y: a tensor with shape [batch, time_y, dimension_y]\n",
        "    \n",
        "    Returns:\n",
        "        a tensor with shape [batch, time_x, time_y]\n",
        "    Raises:\n",
        "        ValueError: if\n",
        "            the shapes of x and y are not match;\n",
        "            bilinear matrix reuse error.\n",
        "    '''\n",
        "    M = tf.get_variable(\n",
        "        name=\"bilinear_matrix\", \n",
        "        shape=[x.shape[-1], y.shape[-1]],\n",
        "        dtype=tf.float32,\n",
        "        initializer=tf.orthogonal_initializer())\n",
        "    sim = tf.einsum('bik,kl,bjl->bij', x, M, y)\n",
        "\n",
        "    if is_nor:\n",
        "        scale = tf.sqrt(tf.cast(x.shape[-1] * y.shape[-1], tf.float32))\n",
        "        scale = tf.maximum(1.0, scale)\n",
        "        return sim / scale\n",
        "    else:\n",
        "        return sim\n",
        "\n",
        "def dot_sim(x, y, is_nor=True):\n",
        "    '''calculate dot similarity with two tensor.\n",
        "\n",
        "    Args:\n",
        "        x: a tensor with shape [batch, time_x, dimension]\n",
        "        y: a tensor with shape [batch, time_y, dimension]\n",
        "    \n",
        "    Returns:\n",
        "        a tensor with shape [batch, time_x, time_y]\n",
        "    Raises:\n",
        "        AssertionError: if\n",
        "            the shapes of x and y are not match.\n",
        "    '''\n",
        "    assert x.shape[-1] == y.shape[-1]\n",
        "\n",
        "    sim = tf.einsum('bik,bjk->bij', x, y)\n",
        "\n",
        "    if is_nor:\n",
        "        scale = tf.sqrt(tf.cast(x.shape[-1], tf.float32))\n",
        "        scale = tf.maximum(1.0, scale)\n",
        "        return sim / scale\n",
        "    else:\n",
        "        return sim\n",
        "\n",
        "def layer_norm(x, axis=None, epsilon=1e-6):\n",
        "    '''Add layer normalization.\n",
        "\n",
        "    Args:\n",
        "        x: a tensor\n",
        "        axis: the dimensions to normalize\n",
        "\n",
        "    Returns:\n",
        "        a tensor the same shape as x.\n",
        "\n",
        "    Raises:\n",
        "    '''\n",
        "    print('wrong version of layer_norm')\n",
        "    scale = tf.get_variable(\n",
        "        name='scale',\n",
        "        shape=[1],\n",
        "        dtype=tf.float32,\n",
        "        initializer=tf.ones_initializer())\n",
        "    bias = tf.get_variable(\n",
        "        name='bias',\n",
        "        shape=[1],\n",
        "        dtype=tf.float32,\n",
        "        initializer=tf.zeros_initializer())\n",
        "\n",
        "    if axis is None:\n",
        "        axis = [-1]\n",
        "\n",
        "    mean = tf.reduce_mean(x, axis=axis, keep_dims=True)\n",
        "    variance = tf.reduce_mean(tf.square(x - mean), axis=axis, keep_dims=True)\n",
        "    norm = (x-mean) * tf.rsqrt(variance + epsilon)\n",
        "    return scale * norm + bias\n",
        "\n",
        "def layer_norm_debug(x, axis = None, epsilon=1e-6):\n",
        "    '''Add layer normalization.\n",
        "\n",
        "    Args:\n",
        "        x: a tensor\n",
        "        axis: the dimensions to normalize\n",
        "\n",
        "    Returns:\n",
        "        a tensor the same shape as x.\n",
        "\n",
        "    Raises:\n",
        "    '''\n",
        "    if axis is None:\n",
        "        axis = [-1]\n",
        "    shape = [x.shape[i] for i in axis]\n",
        "\n",
        "    scale = tf.get_variable(\n",
        "        name='scale',\n",
        "        shape=shape,\n",
        "        dtype=tf.float32,\n",
        "        initializer=tf.ones_initializer())\n",
        "    bias = tf.get_variable(\n",
        "        name='bias',\n",
        "        shape=shape,\n",
        "        dtype=tf.float32,\n",
        "        initializer=tf.zeros_initializer())\n",
        "\n",
        "    mean = tf.reduce_mean(x, axis=axis, keep_dims=True)\n",
        "    variance = tf.reduce_mean(tf.square(x - mean), axis=axis, keep_dims=True)\n",
        "    norm = (x-mean) * tf.rsqrt(variance + epsilon)\n",
        "    return scale * norm + bias\n",
        "\n",
        "def dense(x, out_dimension=None, add_bias=True):\n",
        "    '''Add dense connected layer, Wx + b.\n",
        "\n",
        "    Args:\n",
        "        x: a tensor with shape [batch, time, dimension]\n",
        "        out_dimension: a number which is the output dimension\n",
        "\n",
        "    Return:\n",
        "        a tensor with shape [batch, time, out_dimension]\n",
        "\n",
        "    Raises:\n",
        "    '''\n",
        "    if out_dimension is None:\n",
        "        out_dimension = x.shape[-1]\n",
        "\n",
        "    W = tf.get_variable(\n",
        "        name='weights',\n",
        "        shape=[x.shape[-1], out_dimension],\n",
        "        dtype=tf.float32,\n",
        "        initializer=tf.orthogonal_initializer())\n",
        "    if add_bias:\n",
        "        bias = tf.get_variable(\n",
        "            name='bias',\n",
        "            shape=[1],\n",
        "            dtype=tf.float32,\n",
        "            initializer=tf.zeros_initializer())\n",
        "        return tf.einsum('bik,kj->bij', x, W) + bias\n",
        "    else:\n",
        "        return tf.einsum('bik,kj->bij', x, W)\n",
        "\n",
        "def matmul_2d(x, out_dimension, drop_prob=None):\n",
        "    '''Multiplies 2-d tensor by weights.\n",
        "\n",
        "    Args:\n",
        "        x: a tensor with shape [batch, dimension]\n",
        "        out_dimension: a number\n",
        "\n",
        "    Returns:\n",
        "        a tensor with shape [batch, out_dimension]\n",
        "\n",
        "    Raises:\n",
        "    '''\n",
        "    W = tf.get_variable(\n",
        "        name='weights',\n",
        "        shape=[x.shape[1], out_dimension],\n",
        "        dtype=tf.float32,\n",
        "        initializer=tf.orthogonal_initializer())\n",
        "    if drop_prob is not None:\n",
        "        W = tf.nn.dropout(W, drop_prob)\n",
        "        print('W is dropout')\n",
        "\n",
        "    return tf.matmul(x, W)\n",
        "\n",
        "def gauss_positional_encoding_vector(x, role=0, value=0):\n",
        "    position = int(x.shape[1])\n",
        "    dimension = int(x.shape[2])\n",
        "    print('position: %s' %position)\n",
        "    print('dimension: %s' %dimension)\n",
        "\n",
        "    _lambda = tf.get_variable(\n",
        "        name='lambda',\n",
        "        shape=[position],\n",
        "        dtype=tf.float32,\n",
        "        initializer=tf.constant_initializer(value))\n",
        "    _lambda = tf.expand_dims(_lambda, axis=-1)\n",
        "\n",
        "    mean = [position/2.0, dimension/2.0]\n",
        "\n",
        "    #cov = [[position/3.0, 0], [0, dimension/3.0]]\n",
        "    sigma_x = position/math.sqrt(4.0*dimension)\n",
        "    sigma_y = math.sqrt(dimension/4.0)\n",
        "    cov = [[sigma_x*sigma_x, role*sigma_x*sigma_y], \n",
        "            [role*sigma_x*sigma_y, sigma_y*sigma_y]]\n",
        "\n",
        "    pos = np.dstack(np.mgrid[0:position, 0:dimension])\n",
        "\n",
        "    \n",
        "    rv = multivariate_normal(mean, cov)\n",
        "    signal = rv.pdf(pos) \n",
        "    signal = signal - np.max(signal)/2.0\n",
        "\n",
        "    signal = tf.multiply(_lambda, signal)\n",
        "    signal = tf.expand_dims(signal, axis=0)\n",
        "\n",
        "    print('gauss positional encoding')\n",
        "\n",
        "    return x + _lambda * signal\n",
        "\n",
        "def positional_encoding(x, min_timescale=1.0, max_timescale=1.0e4, value=0):\n",
        "    '''Adds a bunch of sinusoids of different frequencies to a tensor.\n",
        "\n",
        "    Args:\n",
        "        x: a tensor with shape [batch, length, channels]\n",
        "        min_timescale: a float\n",
        "        max_timescale: a float\n",
        "\n",
        "    Returns:\n",
        "        a tensor the same shape as x.\n",
        "\n",
        "    Raises:\n",
        "    '''\n",
        "    length = x.shape[1]\n",
        "    channels = x.shape[2]\n",
        "    _lambda = tf.get_variable(\n",
        "        name='lambda',\n",
        "        shape=[1],\n",
        "        dtype=tf.float32,\n",
        "        initializer=tf.constant_initializer(value))\n",
        "\n",
        "    position = tf.to_float(tf.range(length))\n",
        "    num_timescales = channels // 2\n",
        "    log_timescale_increment = (\n",
        "        math.log(float(max_timescale) / float(min_timescale)) /\n",
        "        (tf.to_float(num_timescales) - 1))\n",
        "    inv_timescales = min_timescale * tf.exp(\n",
        "        tf.to_float(tf.range(num_timescales)) * -log_timescale_increment)\n",
        "    scaled_time = tf.expand_dims(position, 1) * tf.expand_dims(inv_timescales, 0)\n",
        "    signal = tf.concat([tf.sin(scaled_time), tf.cos(scaled_time)], axis=1)\n",
        "    signal = tf.pad(signal, [[0, 0], [0, tf.mod(channels, 2)]])\n",
        "    #signal = tf.reshape(signal, [1, length, channels])\n",
        "    signal = tf.expand_dims(signal, axis=0)\n",
        "\n",
        "    return x + _lambda * signal\n",
        "\n",
        "\n",
        "def positional_encoding_vector(x, min_timescale=1.0, max_timescale=1.0e4, value=0):\n",
        "    '''Adds a bunch of sinusoids of different frequencies to a tensor.\n",
        "\n",
        "    Args:\n",
        "        x: a tensor with shape [batch, length, channels]\n",
        "        min_timescale: a float\n",
        "        max_timescale: a float\n",
        "\n",
        "    Returns:\n",
        "        a tensor the same shape as x.\n",
        "\n",
        "    Raises:\n",
        "    '''\n",
        "    length = x.shape[1]\n",
        "    channels = x.shape[2]\n",
        "    _lambda = tf.get_variable(\n",
        "        name='lambda',\n",
        "        shape=[length],\n",
        "        dtype=tf.float32,\n",
        "        initializer=tf.constant_initializer(value))\n",
        "    _lambda = tf.expand_dims(_lambda, axis=-1)\n",
        "\n",
        "    position = tf.to_float(tf.range(length))\n",
        "    num_timescales = channels // 2\n",
        "    log_timescale_increment = (\n",
        "        math.log(float(max_timescale) / float(min_timescale)) /\n",
        "        (tf.to_float(num_timescales) - 1))\n",
        "    inv_timescales = min_timescale * tf.exp(\n",
        "        tf.to_float(tf.range(num_timescales)) * -log_timescale_increment)\n",
        "    scaled_time = tf.expand_dims(position, 1) * tf.expand_dims(inv_timescales, 0)\n",
        "    signal = tf.concat([tf.sin(scaled_time), tf.cos(scaled_time)], axis=1)\n",
        "    signal = tf.pad(signal, [[0, 0], [0, tf.mod(channels, 2)]])\n",
        "\n",
        "    signal = tf.multiply(_lambda, signal)\n",
        "    signal = tf.expand_dims(signal, axis=0)\n",
        "\n",
        "    return x + signal\n",
        "\n",
        "def opmask(row_lengths, col_lengths, max_row_length, max_col_length):\n",
        "    '''Return a mask tensor representing the first N positions of each row and each column.\n",
        "\n",
        "    Args:\n",
        "        row_lengths: a tensor with shape [batch]\n",
        "        col_lengths: a tensor with shape [batch]\n",
        "\n",
        "    Returns:\n",
        "        a mask tensor with shape [batch, max_row_length, max_col_length]\n",
        "\n",
        "    Raises:\n",
        "    '''\n",
        "    row_mask = tf.sequence_mask(row_lengths, max_row_length) #bool, [batch, max_row_len]\n",
        "    col_mask = tf.sequence_mask(col_lengths, max_col_length) #bool, [batch, max_col_len]\n",
        "\n",
        "    row_mask = tf.cast(tf.expand_dims(row_mask, -1), tf.float32)\n",
        "    col_mask = tf.cast(tf.expand_dims(col_mask, -1), tf.float32)\n",
        "\n",
        "    return tf.einsum('bik,bjk->bij', row_mask, col_mask)\n",
        "\n",
        "def weighted_sum(weight, values):\n",
        "    '''Calcualte the weighted sum.\n",
        "\n",
        "    Args:\n",
        "        weight: a tensor with shape [batch, time, dimension]\n",
        "        values: a tensor with shape [batch, dimension, values_dimension]\n",
        "\n",
        "    Return:\n",
        "        a tensor with shape [batch, time, values_dimension]\n",
        "\n",
        "    Raises:\n",
        "    '''\n",
        "    return tf.einsum('bij,bjk->bik', weight, values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fMud9hIGl1QF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# layers.py"
      ]
    },
    {
      "metadata": {
        "id": "modyg-TSwxaG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##layers.py\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "#import utils.operations as op\n",
        "\n",
        "def similarity(x, y, x_lengths, y_lengths):\n",
        "    '''calculate similarity with two 3d tensor.\n",
        "\n",
        "    Args:\n",
        "        x: a tensor with shape [batch, time_x, dimension]\n",
        "        y: a tensor with shape [batch, time_y, dimension]\n",
        "\n",
        "    Returns:\n",
        "        a tensor with shape [batch, time_x, time_y]\n",
        "\n",
        "    Raises:\n",
        "        ValueError: if\n",
        "            the dimenisons of x and y are not equal.\n",
        "    '''\n",
        "    with tf.variable_scope('x_attend_y'):\n",
        "        try:\n",
        "            x_a_y = block(\n",
        "                x, y, y,\n",
        "                Q_lengths=x_lengths, K_lengths=y_lengths)\n",
        "        except ValueError:\n",
        "            tf.get_variable_scope().reuse_variables()\n",
        "            x_a_y = block(\n",
        "                x, y, y,\n",
        "                Q_lengths=x_lengths, K_lengths=y_lengths)\n",
        "\n",
        "    with tf.variable_scope('y_attend_x'):\n",
        "        try:\n",
        "            y_a_x = block(\n",
        "                y, x, x,\n",
        "                Q_lengths=y_lengths, K_lengths=x_lengths)\n",
        "        except ValueError:\n",
        "            tf.get_variable_scope().reuse_variables()\n",
        "            y_a_x = block(\n",
        "                y, x, x,\n",
        "                Q_lengths=y_lengths, K_lengths=x_lengths)\n",
        "\n",
        "    return tf.matmul(x + x_a_y, y + y_a_x, transpose_b=True)\n",
        "\n",
        "\n",
        "def dynamic_L(x):\n",
        "    '''Attention machanism to combine the infomation, \n",
        "       from https://arxiv.org/pdf/1612.01627.pdf.\n",
        "\n",
        "    Args:\n",
        "        x: a tensor with shape [batch, time, dimension]\n",
        "\n",
        "    Returns:\n",
        "        a tensor with shape [batch, dimension]\n",
        "\n",
        "    Raises:\n",
        "    '''\n",
        "    key_0 = tf.get_variable(\n",
        "        name='key',\n",
        "        shape=[x.shape[-1]],\n",
        "        dtype=tf.float32,\n",
        "        initializer=tf.random_uniform_initializer(\n",
        "            -tf.sqrt(6./tf.cast(x.shape[-1], tf.float32)),\n",
        "            tf.sqrt(6./tf.cast(x.shape[-1], tf.float32))))\n",
        "\n",
        "    key = dense(x, add_bias=False) #[batch, time, dimension]\n",
        "    weight = tf.reduce_sum(tf.multiply(key, key_0), axis=-1)  #[batch, time]\n",
        "    weight = tf.expand_dims(tf.nn.softmax(weight), -1)  #[batch, time, 1]\n",
        "\n",
        "    L = tf.reduce_sum(tf.multiply(x, weight), axis=1) #[batch, dimension]\n",
        "    return L \n",
        "\n",
        "def loss(x, y, num_classes=2, is_clip=True, clip_value=10):\n",
        "    '''From info x calculate logits as return loss.\n",
        "\n",
        "    Args:\n",
        "        x: a tensor with shape [batch, dimension]\n",
        "        num_classes: a number\n",
        "\n",
        "    Returns:\n",
        "        loss: a tensor with shape [1], which is the average loss of one batch\n",
        "        logits: a tensor with shape [batch, 1]\n",
        "\n",
        "    Raises:\n",
        "        AssertionError: if\n",
        "            num_classes is not a int greater equal than 2.\n",
        "    TODO:\n",
        "        num_classes > 2 may be not adapted.\n",
        "    '''\n",
        "    assert isinstance(num_classes, int)\n",
        "    assert num_classes >= 2\n",
        "\n",
        "    W = tf.get_variable(\n",
        "        name='weights',\n",
        "        shape=[x.shape[-1], num_classes-1],\n",
        "        initializer=tf.orthogonal_initializer())\n",
        "    bias = tf.get_variable(\n",
        "        name='bias',\n",
        "        shape=[num_classes-1],\n",
        "        initializer=tf.zeros_initializer())\n",
        "\n",
        "    logits = tf.reshape(tf.matmul(x, W) + bias, [-1])\n",
        "    loss = tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "        labels=tf.cast(y, tf.float32),\n",
        "        logits=logits)\n",
        "    loss = tf.reduce_mean(tf.clip_by_value(loss, -clip_value, clip_value))\n",
        "\n",
        "    return loss, logits\n",
        "\n",
        "def attention(\n",
        "    Q, K, V, \n",
        "    Q_lengths, K_lengths, \n",
        "    attention_type='dot', \n",
        "    is_mask=True, mask_value=-2**32+1,\n",
        "    drop_prob=None):\n",
        "    '''Add attention layer.\n",
        "    Args:\n",
        "        Q: a tensor with shape [batch, Q_time, Q_dimension]\n",
        "        K: a tensor with shape [batch, time, K_dimension]\n",
        "        V: a tensor with shape [batch, time, V_dimension]\n",
        "\n",
        "        Q_length: a tensor with shape [batch]\n",
        "        K_length: a tensor with shape [batch]\n",
        "\n",
        "    Returns:\n",
        "        a tensor with shape [batch, Q_time, V_dimension]\n",
        "\n",
        "    Raises:\n",
        "        AssertionError: if\n",
        "            Q_dimension not equal to K_dimension when attention type is dot.\n",
        "    '''\n",
        "    assert attention_type in ('dot', 'bilinear')\n",
        "    if attention_type == 'dot':\n",
        "        assert Q.shape[-1] == K.shape[-1]\n",
        "\n",
        "    Q_time = Q.shape[1]\n",
        "    K_time = K.shape[1]\n",
        "\n",
        "    if attention_type == 'dot':\n",
        "        logits = dot_sim(Q, K) #[batch, Q_time, time]\n",
        "    if attention_type == 'bilinear':\n",
        "        logits = bilinear_sim(Q, K)\n",
        "\n",
        "    if is_mask:\n",
        "        mask = opmask(Q_lengths, K_lengths, Q_time, K_time) #[batch, Q_time, K_time]\n",
        "        logits = mask * logits + (1 - mask) * mask_value\n",
        "    \n",
        "    attention = tf.nn.softmax(logits)\n",
        "\n",
        "    if drop_prob is not None:\n",
        "        print('use attention drop')\n",
        "        attention = tf.nn.dropout(attention, drop_prob)\n",
        "\n",
        "    return weighted_sum(attention, V)\n",
        "\n",
        "def FFN(x, out_dimension_0=None, out_dimension_1=None):\n",
        "    '''Add two dense connected layer, max(0, x*W0+b0)*W1+b1.\n",
        "\n",
        "    Args:\n",
        "        x: a tensor with shape [batch, time, dimension]\n",
        "        out_dimension: a number which is the output dimension\n",
        "\n",
        "    Returns:\n",
        "        a tensor with shape [batch, time, out_dimension]\n",
        "\n",
        "    Raises:\n",
        "    '''\n",
        "    with tf.variable_scope('FFN_1'):\n",
        "        y = dense(x, out_dimension_0)\n",
        "        y = tf.nn.relu(y)\n",
        "    with tf.variable_scope('FFN_2'):\n",
        "        z = dense(y, out_dimension_1) #, add_bias=False)  #!!!!\n",
        "    return z\n",
        "\n",
        "def block(\n",
        "    Q, K, V, \n",
        "    Q_lengths, K_lengths, \n",
        "    attention_type='dot', \n",
        "    is_layer_norm=True, \n",
        "    is_mask=True, mask_value=-2**32+1,\n",
        "    drop_prob=None):\n",
        "    '''Add a block unit from https://arxiv.org/pdf/1706.03762.pdf.\n",
        "    Args:\n",
        "        Q: a tensor with shape [batch, Q_time, Q_dimension]\n",
        "        K: a tensor with shape [batch, time, K_dimension]\n",
        "        V: a tensor with shape [batch, time, V_dimension]\n",
        "\n",
        "        Q_length: a tensor with shape [batch]\n",
        "        K_length: a tensor with shape [batch]\n",
        "\n",
        "    Returns:\n",
        "        a tensor with shape [batch, time, dimension]\n",
        "\n",
        "    Raises:\n",
        "    '''\n",
        "    att = attention(Q, K, V, \n",
        "                    Q_lengths, K_lengths, \n",
        "                    attention_type='dot', \n",
        "                    is_mask=is_mask, mask_value=mask_value,\n",
        "                    drop_prob=drop_prob)\n",
        "    if is_layer_norm:\n",
        "        with tf.variable_scope('attention_layer_norm'):\n",
        "            y = layer_norm_debug(Q + att)\n",
        "    else:\n",
        "        y = Q + att\n",
        "\n",
        "    z = FFN(y)\n",
        "    if is_layer_norm:\n",
        "        with tf.variable_scope('FFN_layer_norm'):\n",
        "            w = layer_norm_debug(y + z)\n",
        "    else:\n",
        "        w = y + z\n",
        "    return w\n",
        "\n",
        "def CNN(x, out_channels, filter_size, pooling_size, add_relu=True):\n",
        "    '''Add a convlution layer with relu and max pooling layer.\n",
        "\n",
        "    Args:\n",
        "        x: a tensor with shape [batch, in_height, in_width, in_channels]\n",
        "        out_channels: a number\n",
        "        filter_size: a number\n",
        "        pooling_size: a number\n",
        "\n",
        "    Returns:\n",
        "        a flattened tensor with shape [batch, num_features]\n",
        "\n",
        "    Raises:\n",
        "    '''\n",
        "    #calculate the last dimension of return\n",
        "    num_features = ((tf.shape(x)[1]-filter_size+1)/pooling_size * \n",
        "        (tf.shape(x)[2]-filter_size+1)/pooling_size) * out_channels\n",
        "\n",
        "    in_channels = x.shape[-1]\n",
        "    weights = tf.get_variable(\n",
        "        name='filter',\n",
        "        shape=[filter_size, filter_size, in_channels, out_channels],\n",
        "        dtype=tf.float32,\n",
        "        initializer=tf.random_uniform_initializer(-0.01, 0.01))\n",
        "    bias = tf.get_variable(\n",
        "        name='bias',\n",
        "        shape=[out_channels],\n",
        "        dtype=tf.float32,\n",
        "        initializer=tf.zeros_initializer())\n",
        "\n",
        "    conv = tf.nn.conv2d(x, weights, strides=[1, 1, 1, 1], padding=\"VALID\")\n",
        "    conv = conv + bias\n",
        "\n",
        "    if add_relu:\n",
        "        conv = tf.nn.relu(conv)\n",
        "\n",
        "    pooling = tf.nn.max_pool(\n",
        "        conv, \n",
        "        ksize=[1, pooling_size, pooling_size, 1],\n",
        "        strides=[1, pooling_size, pooling_size, 1], \n",
        "        padding=\"VALID\")\n",
        "\n",
        "    return tf.contrib.layers.flatten(pooling)\n",
        "\n",
        "def CNN_3d(x, out_channels_0, out_channels_1, add_relu=True):\n",
        "    '''Add a 3d convlution layer with relu and max pooling layer.\n",
        "\n",
        "    Args:\n",
        "        x: a tensor with shape [batch, in_depth, in_height, in_width, in_channels]\n",
        "        out_channels: a number\n",
        "        filter_size: a number\n",
        "        pooling_size: a number\n",
        "\n",
        "    Returns:\n",
        "        a flattened tensor with shape [batch, num_features]\n",
        "\n",
        "    Raises:\n",
        "    '''\n",
        "    in_channels = x.shape[-1]\n",
        "    weights_0 = tf.get_variable(\n",
        "        name='filter_0',\n",
        "        shape=[3, 3, 3, in_channels, out_channels_0],\n",
        "        dtype=tf.float32,\n",
        "        initializer=tf.random_uniform_initializer(-0.01, 0.01))\n",
        "    bias_0 = tf.get_variable(\n",
        "        name='bias_0',\n",
        "        shape=[out_channels_0],\n",
        "        dtype=tf.float32,\n",
        "        initializer=tf.zeros_initializer())\n",
        "\n",
        "    conv_0 = tf.nn.conv3d(x, weights_0, strides=[1, 1, 1, 1, 1], padding=\"SAME\")\n",
        "    print('conv_0 shape: %s' %conv_0.shape)\n",
        "    conv_0 = conv_0 + bias_0\n",
        "\n",
        "    if add_relu:\n",
        "        conv_0 = tf.nn.elu(conv_0)\n",
        "\n",
        "    pooling_0 = tf.nn.max_pool3d(\n",
        "        conv_0, \n",
        "        ksize=[1, 3, 3, 3, 1],\n",
        "        strides=[1, 3, 3, 3, 1], \n",
        "        padding=\"SAME\")\n",
        "    print('pooling_0 shape: %s' %pooling_0.shape)\n",
        "\n",
        "    #layer_1\n",
        "    weights_1 = tf.get_variable(\n",
        "        name='filter_1',\n",
        "        shape=[3, 3, 3, out_channels_0, out_channels_1],\n",
        "        dtype=tf.float32,\n",
        "        initializer=tf.random_uniform_initializer(-0.01, 0.01))\n",
        "    bias_1 = tf.get_variable(\n",
        "        name='bias_1',\n",
        "        shape=[out_channels_1],\n",
        "        dtype=tf.float32,\n",
        "        initializer=tf.zeros_initializer())\n",
        "\n",
        "    conv_1 = tf.nn.conv3d(pooling_0, weights_1, strides=[1, 1, 1, 1, 1], padding=\"SAME\")\n",
        "    print('conv_1 shape: %s' %conv_1.shape)\n",
        "    conv_1 = conv_1 + bias_1\n",
        "\n",
        "    if add_relu:\n",
        "        conv_1 = tf.nn.elu(conv_1)\n",
        "\n",
        "    pooling_1 = tf.nn.max_pool3d(\n",
        "        conv_1, \n",
        "        ksize=[1, 3, 3, 3, 1],\n",
        "        strides=[1, 3, 3, 3, 1], \n",
        "        padding=\"SAME\")\n",
        "    print('pooling_1 shape: %s' %pooling_1.shape)\n",
        "\n",
        "    return tf.contrib.layers.flatten(pooling_1)\n",
        "\n",
        "def CNN_3d_2d(x, out_channels_0, out_channels_1, add_relu=True):\n",
        "    '''Add a 3d convlution layer with relu and max pooling layer.\n",
        "\n",
        "    Args:\n",
        "        x: a tensor with shape [batch, in_depth, in_height, in_width, in_channels]\n",
        "        out_channels: a number\n",
        "        filter_size: a number\n",
        "        pooling_size: a number\n",
        "\n",
        "    Returns:\n",
        "        a flattened tensor with shape [batch, num_features]\n",
        "\n",
        "    Raises:\n",
        "    '''\n",
        "    in_channels = x.shape[-1]\n",
        "    weights_0 = tf.get_variable(\n",
        "        name='filter_0',\n",
        "        shape=[1, 3, 3, in_channels, out_channels_0],\n",
        "        dtype=tf.float32,\n",
        "        initializer=tf.random_uniform_initializer(-0.01, 0.01))\n",
        "    bias_0 = tf.get_variable(\n",
        "        name='bias_0',\n",
        "        shape=[out_channels_0],\n",
        "        dtype=tf.float32,\n",
        "        initializer=tf.zeros_initializer())\n",
        "\n",
        "    conv_0 = tf.nn.conv3d(x, weights_0, strides=[1, 1, 1, 1, 1], padding=\"SAME\")\n",
        "    print('conv_0 shape: %s' %conv_0.shape)\n",
        "    conv_0 = conv_0 + bias_0\n",
        "\n",
        "    if add_relu:\n",
        "        conv_0 = tf.nn.elu(conv_0)\n",
        "\n",
        "    pooling_0 = tf.nn.max_pool3d(\n",
        "        conv_0, \n",
        "        ksize=[1, 1, 3, 3, 1],\n",
        "        strides=[1, 1, 3, 3, 1], \n",
        "        padding=\"SAME\")\n",
        "    print('pooling_0 shape: %s' %pooling_0.shape)\n",
        "\n",
        "    #layer_1\n",
        "    weights_1 = tf.get_variable(\n",
        "        name='filter_1',\n",
        "        shape=[1, 3, 3, out_channels_0, out_channels_1],\n",
        "        dtype=tf.float32,\n",
        "        initializer=tf.random_uniform_initializer(-0.01, 0.01))\n",
        "    bias_1 = tf.get_variable(\n",
        "        name='bias_1',\n",
        "        shape=[out_channels_1],\n",
        "        dtype=tf.float32,\n",
        "        initializer=tf.zeros_initializer())\n",
        "\n",
        "    conv_1 = tf.nn.conv3d(pooling_0, weights_1, strides=[1, 1, 1, 1, 1], padding=\"SAME\")\n",
        "    print('conv_1 shape: %s' %conv_1.shape)\n",
        "    conv_1 = conv_1 + bias_1\n",
        "\n",
        "    if add_relu:\n",
        "        conv_1 = tf.nn.elu(conv_1)\n",
        "\n",
        "    pooling_1 = tf.nn.max_pool3d(\n",
        "        conv_1, \n",
        "        ksize=[1, 1, 3, 3, 1],\n",
        "        strides=[1, 1, 3, 3, 1], \n",
        "        padding=\"SAME\")\n",
        "    print('pooling_1 shape: %s' %pooling_1.shape)\n",
        "\n",
        "    return tf.contrib.layers.flatten(pooling_1)\n",
        "\n",
        "def CNN_3d_change(x, out_channels_0, out_channels_1, add_relu=True):\n",
        "    '''Add a 3d convlution layer with relu and max pooling layer.\n",
        "\n",
        "    Args:\n",
        "        x: a tensor with shape [batch, in_depth, in_height, in_width, in_channels]\n",
        "        out_channels: a number\n",
        "        filter_size: a number\n",
        "        pooling_size: a number\n",
        "\n",
        "    Returns:\n",
        "        a flattened tensor with shape [batch, num_features]\n",
        "\n",
        "    Raises:\n",
        "    '''\n",
        "    in_channels = x.shape[-1]\n",
        "    weights_0 = tf.get_variable(\n",
        "        name='filter_0',\n",
        "        shape=[3, 3, 3, in_channels, out_channels_0],\n",
        "        dtype=tf.float32,\n",
        "        #initializer=tf.random_normal_initializer(0, 0.05))\n",
        "        initializer=tf.random_uniform_initializer(-0.01, 0.01))\n",
        "    bias_0 = tf.get_variable(\n",
        "        name='bias_0',\n",
        "        shape=[out_channels_0],\n",
        "        dtype=tf.float32,\n",
        "        initializer=tf.zeros_initializer())\n",
        "    #Todo\n",
        "    g_0 = tf.get_variable(name='scale_0',\n",
        "        shape = [out_channels_0],\n",
        "        dtype=tf.float32,\n",
        "        initializer=tf.ones_initializer())\n",
        "    weights_0 = tf.reshape(g_0, [1, 1, 1, out_channels_0]) * tf.nn.l2_normalize(weights_0, [0, 1, 2])\n",
        "\n",
        "    conv_0 = tf.nn.conv3d(x, weights_0, strides=[1, 1, 1, 1, 1], padding=\"VALID\")\n",
        "    print('conv_0 shape: %s' %conv_0.shape)\n",
        "    conv_0 = conv_0 + bias_0\n",
        "    #######\n",
        "    '''\n",
        "    with tf.variable_scope('layer_0'):\n",
        "        conv_0 = layer_norm(conv_0, axis=[1, 2, 3, 4])\n",
        "        print('layer_norm in cnn')\n",
        "    '''\n",
        "    if add_relu:\n",
        "        conv_0 = tf.nn.elu(conv_0)\n",
        "\n",
        "    pooling_0 = tf.nn.max_pool3d(\n",
        "        conv_0, \n",
        "        ksize=[1, 2, 3, 3, 1],\n",
        "        strides=[1, 2, 3, 3, 1], \n",
        "        padding=\"VALID\")\n",
        "    print('pooling_0 shape: %s' %pooling_0.shape)\n",
        "\n",
        "    #layer_1\n",
        "    weights_1 = tf.get_variable(\n",
        "        name='filter_1',\n",
        "        shape=[2, 2, 2, out_channels_0, out_channels_1],\n",
        "        dtype=tf.float32,\n",
        "        initializer=tf.random_uniform_initializer(-0.01, 0.01))\n",
        "    \n",
        "    bias_1 = tf.get_variable(\n",
        "        name='bias_1',\n",
        "        shape=[out_channels_1],\n",
        "        dtype=tf.float32,\n",
        "        initializer=tf.zeros_initializer())\n",
        "    \n",
        "    g_1 = tf.get_variable(name='scale_1',\n",
        "        shape = [out_channels_1],\n",
        "        dtype=tf.float32,\n",
        "        initializer=tf.ones_initializer())\n",
        "    weights_1 = tf.reshape(g_1, [1, 1, 1, out_channels_1]) * tf.nn.l2_normalize(weights_1, [0, 1, 2])\n",
        "\n",
        "    conv_1 = tf.nn.conv3d(pooling_0, weights_1, strides=[1, 1, 1, 1, 1], padding=\"VALID\")\n",
        "    print('conv_1 shape: %s' %conv_1.shape)\n",
        "    conv_1 = conv_1 + bias_1\n",
        "    #with tf.variable_scope('layer_1'):\n",
        "    #    conv_1 = layer_norm(conv_1, axis=[1, 2, 3, 4])\n",
        "\n",
        "    if add_relu:\n",
        "        conv_1 = tf.nn.elu(conv_1)\n",
        "\n",
        "    pooling_1 = tf.nn.max_pool3d(\n",
        "        conv_1, \n",
        "        ksize=[1, 3, 3, 3, 1],\n",
        "        strides=[1, 3, 3, 3, 1], \n",
        "        padding=\"VALID\")\n",
        "    print('pooling_1 shape: %s' %pooling_1.shape)\n",
        "\n",
        "    return tf.contrib.layers.flatten(pooling_1)\n",
        "\n",
        "def RNN_last_state(x, lengths, hidden_size):\n",
        "    '''encode x with a gru cell and return the last state.\n",
        "    \n",
        "    Args:\n",
        "        x: a tensor with shape [batch, time, dimension]\n",
        "        length: a tensor with shape [batch]\n",
        "\n",
        "    Return:\n",
        "        a tensor with shape [batch, hidden_size]\n",
        "\n",
        "    Raises:\n",
        "    '''\n",
        "    cell = tf.nn.rnn_cell.GRUCell(hidden_size)\n",
        "    outputs, last_states = tf.nn.dynamic_rnn(cell, x, lengths, dtype=tf.float32)\n",
        "    return outputs, last_states"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PPqFyM8PoYbQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# ---------------------------------------------------------------------------------------------"
      ]
    },
    {
      "metadata": {
        "id": "YfcEO06jpbAn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# train_and_evaluate.py"
      ]
    },
    {
      "metadata": {
        "id": "1vsS-FuupSCK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##train_and_evaluate \n",
        "\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import time\n",
        "\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "#import utils.reader as reader\n",
        "#import utils.evaluation as eva\n",
        "\n",
        "\n",
        "def train(conf, _model):\n",
        "    \n",
        "    if conf['rand_seed'] is not None:\n",
        "        np.random.seed(conf['rand_seed'])\n",
        "\n",
        "    if not os.path.exists(conf['save_path']):\n",
        "        os.makedirs(conf['save_path'])\n",
        "\n",
        "    # load data\n",
        "    print('starting loading data')\n",
        "    print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())))\n",
        "    files = 'data.pkl'    \n",
        "    with open(files, mode='rb') as f:\n",
        "        train_data, val_data, test_data = pickle.load(f)    \n",
        "    print('finish loading data')\n",
        "    print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())))\n",
        "    \n",
        "    \n",
        "    print('starting building validation batches')\n",
        "    val_batches = build_batches(val_data, conf)\n",
        "    print(\"finish building validation batches\")\n",
        "    print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())))\n",
        "\n",
        "    # refine conf\n",
        "    batch_num = len(train_data['y']) // conf[\"batch_size\"]\n",
        "    val_batch_num = len(val_batches[\"response\"])\n",
        "\n",
        "    conf[\"train_steps\"] = conf[\"num_scan_data\"] * batch_num\n",
        "    conf[\"save_step\"] = max(1, batch_num / 10)\n",
        "    conf[\"print_step\"] = max(1, batch_num / 100)\n",
        "    print('configurations: %s' %conf)\n",
        "\n",
        "    print('model sucess')\n",
        "    print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())))\n",
        "\n",
        "    _graph = _model.build_graph()\n",
        "    print('build graph sucess')\n",
        "    print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())))\n",
        "\n",
        "    with tf.Session(graph=_graph) as sess:\n",
        "        _model.init.run();\n",
        "        if conf[\"init_model\"]:\n",
        "            _model.saver.restore(sess, conf[\"init_model\"])\n",
        "            print(\"sucess init %s\" %conf[\"init_model\"])\n",
        "\n",
        "        average_loss = 0.0\n",
        "        batch_index = 0\n",
        "        step = 0\n",
        "        best_result = [0, 0, 0, 0]\n",
        "\n",
        "        for step_i in range(conf[\"num_scan_data\"]):\n",
        "            #for batch_index in rng.permutation(range(batch_num)):\n",
        "            print('starting shuffle train data')\n",
        "            shuffle_train = unison_shuffle(train_data)\n",
        "            train_batches = build_batches(shuffle_train, conf)\n",
        "            print('finish building train data')\n",
        "            for batch_index in range(batch_num):\n",
        "\n",
        "                feed = {\n",
        "                    _model.turns: train_batches[\"turns\"][batch_index], \n",
        "                    _model.tt_turns_len: train_batches[\"tt_turns_len\"][batch_index],\n",
        "                    _model.every_turn_len: train_batches[\"every_turn_len\"][batch_index],\n",
        "                    _model.response: train_batches[\"response\"][batch_index], \n",
        "                    _model.response_len: train_batches[\"response_len\"][batch_index],\n",
        "                    _model.label: train_batches[\"label\"][batch_index]\n",
        "                }\n",
        "\n",
        "                batch_index = (batch_index + 1) % batch_num;\n",
        "\n",
        "                _, curr_loss = sess.run([_model.g_updates, _model.loss], feed_dict = feed)\n",
        "\n",
        "                \n",
        "                average_loss += curr_loss\n",
        "\n",
        "                step += 1\n",
        "\n",
        "                if step % conf[\"print_step\"] == 0 and step > 0:\n",
        "                    g_step, lr = sess.run([_model.global_step, _model.learning_rate])\n",
        "                    print('step: %s, lr: %s' %(g_step, lr))\n",
        "                    print(\"processed: [\" + str(step * 1.0 / batch_num) + \"] loss: [\" + str(average_loss / conf[\"print_step\"]) + \"]\" )\n",
        "                    average_loss = 0\n",
        "\n",
        "                #print(\"Saving steps???\", step % conf[\"save_step\"] == 0)\n",
        "                if step % conf[\"save_step\"] == 0 and step > 0:\n",
        "                    index = step // conf['save_step']\n",
        "                    score_file_path = conf['save_path'] + 'score.' + str(index)\n",
        "                    score_file = open(score_file_path, 'w')\n",
        "                    print('save step: %s' %index)\n",
        "                    print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())))\n",
        "\n",
        "                    for batch_index in range(val_batch_num):\n",
        "                \n",
        "                        feed = { \n",
        "                            _model.turns: val_batches[\"turns\"][batch_index],\n",
        "                            _model.tt_turns_len: val_batches[\"tt_turns_len\"][batch_index],\n",
        "                            _model.every_turn_len: val_batches[\"every_turn_len\"][batch_index],\n",
        "                            _model.response: val_batches[\"response\"][batch_index],\n",
        "                            _model.response_len: val_batches[\"response_len\"][batch_index],\n",
        "                            _model.label: val_batches[\"label\"][batch_index]\n",
        "                        }   \n",
        "                \n",
        "                        scores = sess.run(_model.logits, feed_dict = feed)\n",
        "                    \n",
        "                        for i in range(conf[\"batch_size\"]):\n",
        "                            score_file.write(\n",
        "                                str(scores[i]) + '\\t' + \n",
        "                                str(val_batches[\"label\"][batch_index][i]) + '\\n')\n",
        "                    score_file.close()\n",
        "\n",
        "                    #write evaluation result\n",
        "                    result = evaluate(score_file_path)\n",
        "                    result_file_path = conf[\"save_path\"] + \"result.\" + str(index)\n",
        "                    with open(result_file_path, 'w') as out_file:\n",
        "                        for p_at in result:\n",
        "                            out_file.write(str(p_at) + '\\n')\n",
        "                    print('result = ', result)\n",
        "                    print('finish evaluation')\n",
        "                    print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())))\n",
        "                    \n",
        "                    if result[1] + result[2] > best_result[1] + best_result[2]:\n",
        "                        best_result = result\n",
        "                        _save_path = _model.saver.save(sess, conf[\"save_path\"] + \"model.ckpt.\" + str(step / conf[\"save_step\"]))\n",
        "                        print(\"succ saving model in \" + _save_path)\n",
        "                        print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5VhTzE27lizT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# test_and_evaluate.py"
      ]
    },
    {
      "metadata": {
        "id": "fre9ubVS0Edt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## test_and_evaluate\n",
        "\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import time\n",
        "\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "#import utils.reader as reader\n",
        "#import utils.evaluation as eva\n",
        "\n",
        "\n",
        "def test(conf, _model):\n",
        "    \n",
        "    if not os.path.exists(conf['save_path']):\n",
        "        os.makedirs(conf['save_path'])\n",
        "\n",
        "    # load data\n",
        "    print('starting loading data')\n",
        "    print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())))\n",
        "    files = 'data.pkl'    \n",
        "    with open(files, mode='rb') as f:\n",
        "        train_data, val_data, test_data = pickle.load(f)    \n",
        "    print('finish loading data')\n",
        "    print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())))\n",
        "    \n",
        "    \n",
        "    print('starting building validation batches')\n",
        "    test_batches = build_batches(test_data, conf)\n",
        "    print(\"finish building validation batches\")\n",
        "    print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())))\n",
        "\n",
        "\n",
        "    # refine conf\n",
        "    test_batch_num = len(test_batches[\"response\"])\n",
        "\n",
        "    print('configurations: %s' %conf)\n",
        "\n",
        "\n",
        "    _graph = _model.build_graph()\n",
        "    print('build graph sucess')\n",
        "    print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())))\n",
        "\n",
        "    with tf.Session(graph=_graph) as sess:\n",
        "        #_model.init.run();\n",
        "        _model.saver.restore(sess, conf[\"init_model\"])\n",
        "        print(\"sucess init %s\" %conf[\"init_model\"])\n",
        "\n",
        "        batch_index = 0\n",
        "        step = 0\n",
        "\n",
        "        score_file_path = conf['save_path'] + 'score.test'\n",
        "        score_file = open(score_file_path, 'w')\n",
        "\n",
        "        print('starting test')\n",
        "        print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())))\n",
        "        for batch_index in range(test_batch_num):\n",
        "                \n",
        "            feed = { \n",
        "                _model.turns: test_batches[\"turns\"][batch_index],\n",
        "                _model.tt_turns_len: test_batches[\"tt_turns_len\"][batch_index],\n",
        "                _model.every_turn_len: test_batches[\"every_turn_len\"][batch_index],\n",
        "                _model.response: test_batches[\"response\"][batch_index],\n",
        "                _model.response_len: test_batches[\"response_len\"][batch_index],\n",
        "                _model.label: test_batches[\"label\"][batch_index]\n",
        "                }   \n",
        "                \n",
        "            scores = sess.run(_model.logits, feed_dict = feed)\n",
        "                    \n",
        "            for i in range(conf[\"batch_size\"]):\n",
        "                score_file.write(\n",
        "                    str(scores[i]) + '\\t' + \n",
        "                    str(test_batches[\"label\"][batch_index][i]) + '\\n')\n",
        "                    #str(sum(test_batches[\"every_turn_len\"][batch_index][i]) / test_batches['tt_turns_len'][batch_index][i]) + '\\t' +\n",
        "                    #str(test_batches['tt_turns_len'][batch_index][i]) + '\\n') \n",
        "\n",
        "        score_file.close()\n",
        "        print('finish test')\n",
        "        print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())))\n",
        "\n",
        "        \n",
        "        #write evaluation result\n",
        "        result = evaluate(score_file_path)\n",
        "        result_file_path = conf[\"save_path\"] + \"result.test\"\n",
        "        with open(result_file_path, 'w') as out_file:\n",
        "            for p_at in result:\n",
        "                out_file.write(str(p_at) + '\\n')\n",
        "        print('finish evaluation')\n",
        "        print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VPnq2n0Ck1kj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# net.py"
      ]
    },
    {
      "metadata": {
        "id": "x9d0WMSxrbko",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##net.py\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "#import utils.layers as layers\n",
        "#import utils.operations as op\n",
        "\n",
        "class Net(object):\n",
        "    '''Add positional encoding(initializer lambda is 0),\n",
        "       cross-attention, cnn integrated and grad clip by value.\n",
        "\n",
        "    Attributes:\n",
        "        conf: a configuration paramaters dict\n",
        "        word_embedding_init: a 2-d array with shape [vocab_size+1, emb_size]\n",
        "    '''\n",
        "    def __init__(self, conf):\n",
        "        self._graph = tf.Graph()\n",
        "        self._conf = conf\n",
        "\n",
        "        if self._conf['word_emb_init'] is not None:\n",
        "            print('loading word emb init')\n",
        "            self._word_embedding_init = pickle.load(open(self._conf['word_emb_init'], 'rb'), encoding='latin1')\n",
        "        else:\n",
        "            print('word emb init was set to None')\n",
        "            self._word_embedding_init = None\n",
        "\n",
        "    def build_graph(self):\n",
        "        with self._graph.as_default():\n",
        "            if self._conf['rand_seed'] is not None:\n",
        "                rand_seed = self._conf['rand_seed']\n",
        "                tf.set_random_seed(rand_seed)\n",
        "                print('set tf random seed: %s' %self._conf['rand_seed'])\n",
        "\n",
        "            #word embedding\n",
        "            if self._word_embedding_init is not None:\n",
        "                word_embedding_initializer = tf.constant_initializer(self._word_embedding_init)\n",
        "            else:\n",
        "                word_embedding_initializer = tf.random_normal_initializer(stddev=0.1)\n",
        "\n",
        "            self._word_embedding = tf.get_variable(\n",
        "                name='word_embedding',\n",
        "                shape=[self._conf['vocab_size']+1, self._conf['emb_size']],\n",
        "                dtype=tf.float32,\n",
        "                initializer=word_embedding_initializer)\n",
        "\n",
        "\n",
        "            #define placehloders\n",
        "            self.turns = tf.placeholder(\n",
        "                tf.int32,\n",
        "                shape=[self._conf[\"batch_size\"], self._conf[\"max_turn_num\"], self._conf[\"max_turn_len\"]])\n",
        "\n",
        "            self.tt_turns_len = tf.placeholder(\n",
        "                tf.int32,\n",
        "                shape=[self._conf[\"batch_size\"]])\n",
        "\n",
        "            self.every_turn_len = tf.placeholder(\n",
        "                tf.int32,\n",
        "                shape=[self._conf[\"batch_size\"], self._conf[\"max_turn_num\"]])\n",
        "    \n",
        "            self.response = tf.placeholder(\n",
        "                tf.int32, \n",
        "                shape=[self._conf[\"batch_size\"], self._conf[\"max_turn_len\"]])\n",
        "\n",
        "            self.response_len = tf.placeholder(\n",
        "                tf.int32, \n",
        "                shape=[self._conf[\"batch_size\"]])\n",
        "\n",
        "            self.label = tf.placeholder(\n",
        "                tf.float32, \n",
        "                shape=[self._conf[\"batch_size\"]])\n",
        "\n",
        "\n",
        "            #define operations\n",
        "            #response part\n",
        "            Hr = tf.nn.embedding_lookup(self._word_embedding, self.response)\n",
        "\n",
        "            if self._conf['is_positional'] and self._conf['stack_num'] > 0:\n",
        "                with tf.variable_scope('positional'):\n",
        "                    Hr = positional_encoding_vector(Hr, max_timescale=10)\n",
        "            Hr_stack = [Hr]\n",
        "\n",
        "            for index in range(self._conf['stack_num']):\n",
        "                with tf.variable_scope('self_stack_' + str(index)):\n",
        "                    Hr = block(\n",
        "                        Hr, Hr, Hr, \n",
        "                        Q_lengths=self.response_len, K_lengths=self.response_len)\n",
        "                    Hr_stack.append(Hr)\n",
        "\n",
        "\n",
        "            #context part\n",
        "            #a list of length max_turn_num, every element is a tensor with shape [batch, max_turn_len]\n",
        "            list_turn_t = tf.unstack(self.turns, axis=1) \n",
        "            list_turn_length = tf.unstack(self.every_turn_len, axis=1)\n",
        "            \n",
        "            sim_turns = []\n",
        "            #for every turn_t calculate matching vector\n",
        "            for turn_t, t_turn_length in zip(list_turn_t, list_turn_length):\n",
        "                Hu = tf.nn.embedding_lookup(self._word_embedding, turn_t) #[batch, max_turn_len, emb_size]\n",
        "\n",
        "                if self._conf['is_positional'] and self._conf['stack_num'] > 0:\n",
        "                    with tf.variable_scope('positional', reuse=True):\n",
        "                        Hu = positional_encoding_vector(Hu, max_timescale=10)\n",
        "                Hu_stack = [Hu]\n",
        "\n",
        "                for index in range(self._conf['stack_num']):\n",
        "\n",
        "                    with tf.variable_scope('self_stack_' + str(index), reuse=True):\n",
        "                        Hu = block(\n",
        "                            Hu, Hu, Hu,\n",
        "                            Q_lengths=t_turn_length, K_lengths=t_turn_length)\n",
        "\n",
        "                        Hu_stack.append(Hu)\n",
        "\n",
        "\n",
        "\n",
        "                r_a_t_stack = []\n",
        "                t_a_r_stack = []\n",
        "                for index in range(self._conf['stack_num']+1):\n",
        "\n",
        "                    with tf.variable_scope('t_attend_r_' + str(index)):\n",
        "                        try:\n",
        "                            t_a_r = block(\n",
        "                                Hu_stack[index], Hr_stack[index], Hr_stack[index],\n",
        "                                Q_lengths=t_turn_length, K_lengths=self.response_len)\n",
        "                        except ValueError:\n",
        "                            tf.get_variable_scope().reuse_variables()\n",
        "                            t_a_r = block(\n",
        "                                Hu_stack[index], Hr_stack[index], Hr_stack[index],\n",
        "                                Q_lengths=t_turn_length, K_lengths=self.response_len)\n",
        "\n",
        "\n",
        "                    with tf.variable_scope('r_attend_t_' + str(index)):\n",
        "                        try:\n",
        "                            r_a_t = block(\n",
        "                                Hr_stack[index], Hu_stack[index], Hu_stack[index],\n",
        "                                Q_lengths=self.response_len, K_lengths=t_turn_length)\n",
        "                        except ValueError:\n",
        "                            tf.get_variable_scope().reuse_variables()\n",
        "                            r_a_t = block(\n",
        "                                Hr_stack[index], Hu_stack[index], Hu_stack[index],\n",
        "                                Q_lengths=self.response_len, K_lengths=t_turn_length)\n",
        "\n",
        "                    t_a_r_stack.append(t_a_r)\n",
        "                    r_a_t_stack.append(r_a_t)\n",
        "\n",
        "                t_a_r_stack.extend(Hu_stack)\n",
        "                r_a_t_stack.extend(Hr_stack)\n",
        "                \n",
        "                t_a_r = tf.stack(t_a_r_stack, axis=-1)\n",
        "                r_a_t = tf.stack(r_a_t_stack, axis=-1)\n",
        "\n",
        "                            \n",
        "                #calculate similarity matrix\n",
        "                with tf.variable_scope('similarity'):\n",
        "                    # sim shape [batch, max_turn_len, max_turn_len, 2*stack_num+1]\n",
        "                    # divide sqrt(200) to prevent gradient explosion\n",
        "                    sim = tf.einsum('biks,bjks->bijs', t_a_r, r_a_t) / tf.sqrt(200.0)\n",
        "\n",
        "                sim_turns.append(sim)\n",
        "\n",
        "\n",
        "            #cnn and aggregation\n",
        "            sim = tf.stack(sim_turns, axis=1)\n",
        "            print('sim shape: %s' %sim.shape)\n",
        "            with tf.variable_scope('cnn_aggregation'):\n",
        "                final_info = CNN_3d(sim, 32, 16)\n",
        "                #for douban\n",
        "                #final_info = CNN_3d(sim, 16, 16)\n",
        "\n",
        "            #loss and train\n",
        "            with tf.variable_scope('loss'):\n",
        "                self.loss, self.logits = loss(final_info, self.label)\n",
        "\n",
        "                self.global_step = tf.Variable(0, trainable=False)\n",
        "                initial_learning_rate = self._conf['learning_rate']\n",
        "                self.learning_rate = tf.train.exponential_decay(\n",
        "                    initial_learning_rate,\n",
        "                    global_step=self.global_step,\n",
        "                    decay_steps=400,\n",
        "                    decay_rate=0.9,\n",
        "                    staircase=True)\n",
        "\n",
        "                Optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
        "                self.optimizer = Optimizer.minimize(\n",
        "                    self.loss,\n",
        "                    global_step=self.global_step)\n",
        "\n",
        "                self.init = tf.global_variables_initializer()\n",
        "                self.saver = tf.train.Saver(max_to_keep = self._conf[\"max_to_keep\"])\n",
        "                self.all_variables = tf.global_variables() \n",
        "                self.all_operations = self._graph.get_operations()\n",
        "                self.grads_and_vars = Optimizer.compute_gradients(self.loss)\n",
        "\n",
        "                for grad, var in self.grads_and_vars:\n",
        "                    if grad is None:\n",
        "                        print (var)\n",
        "\n",
        "                self.capped_gvs = [(tf.clip_by_value(grad, -1, 1), var) for grad, var in self.grads_and_vars]\n",
        "                self.g_updates = Optimizer.apply_gradients(\n",
        "                    self.capped_gvs,\n",
        "                    global_step=self.global_step)\n",
        "    \n",
        "        return self._graph"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2OQOw3afoEbj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# ---------------------------------------------------------------------------------------------"
      ]
    },
    {
      "metadata": {
        "id": "ytMzWH1OhOQm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Main.py  for training\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "DTCG9ErywYe8",
        "colab_type": "code",
        "outputId": "dd1db19d-e56d-4549-f11c-2caa0ec7b1cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2944
        }
      },
      "cell_type": "code",
      "source": [
        "#main.py for train\n",
        "\n",
        "#import models.net as net\n",
        "#import bin.train_and_evaluate as train\n",
        "#import bin.test_and_evaluate as test\n",
        "\n",
        "# configure\n",
        "\n",
        "conf = {\n",
        "    \"data_path\": \"./data.pkl\",\n",
        "    \"save_path\": \"./output/ubuntu/temp/\",\n",
        "#    \"word_emb_init\": None,\n",
        "    \"word_emb_init\": \"./word_embedding.pkl\",\n",
        "    \"init_model\": None, #should be set for test\n",
        "\n",
        "    \"rand_seed\": None, \n",
        "\n",
        "    \"drop_dense\": None,\n",
        "    \"drop_attention\": None,\n",
        "\n",
        "    \"is_mask\": True,\n",
        "    \"is_layer_norm\": True,\n",
        "    \"is_positional\": False,  \n",
        "\n",
        "    \"stack_num\": 5,  \n",
        "    \"attention_type\": \"dot\",\n",
        "\n",
        "    \"learning_rate\": 1e-3,\n",
        "    \"vocab_size\": 434512,\n",
        "    \"emb_size\": 200,\n",
        "    \"batch_size\": 36, #200 for test\n",
        "\n",
        "    \"max_turn_num\": 9,  \n",
        "    \"max_turn_len\": 50, \n",
        "\n",
        "    \"max_to_keep\": 1,\n",
        "    \"num_scan_data\": 2,\n",
        "    \"_EOS_\": 28270, #1 for douban data\n",
        "    \"final_n_class\": 1,\n",
        "}\n",
        "\n",
        "\n",
        "model = Net(conf)\n",
        "train(conf, model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "starting loading data\n",
            "2019-03-18 12:07:20\n",
            "finish loading data\n",
            "2019-03-18 12:07:20\n",
            "starting spliting data\n",
            "2019-03-18 12:07:20\n",
            "finish spliting data\n",
            "2019-03-18 12:07:20\n",
            "finish building test batches\n",
            "2019-03-18 12:07:20\n",
            "configurations: {'data_path': './data/ubuntu/data_small.pkl', 'save_path': './output/ubuntu/temp/', 'word_emb_init': None, 'init_model': None, 'rand_seed': None, 'drop_dense': None, 'drop_attention': None, 'is_mask': True, 'is_layer_norm': True, 'is_positional': False, 'stack_num': 5, 'attention_type': 'dot', 'learning_rate': 0.001, 'vocab_size': 434512, 'emb_size': 200, 'batch_size': 10, 'max_turn_num': 9, 'max_turn_len': 50, 'max_to_keep': 1, 'num_scan_data': 2, '_EOS_': 28270, 'final_n_class': 1, 'train_steps': 20, 'save_step': 1, 'print_step': 1}\n",
            "model sucess\n",
            "2019-03-18 12:07:20\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "sim shape: (10, 9, 50, 50, 12)\n",
            "conv_0 shape: (10, 9, 50, 50, 32)\n",
            "pooling_0 shape: (10, 3, 17, 17, 32)\n",
            "conv_1 shape: (10, 3, 17, 17, 16)\n",
            "pooling_1 shape: (10, 1, 6, 6, 16)\n",
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/layers/python/layers/layers.py:1624: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "build graph sucess\n",
            "2019-03-18 12:09:22\n",
            "starting shuffle train data\n",
            "finish building train data\n",
            "step: 1, lr: 0.001\n",
            "processed: [0.1] loss: [0.6884599924087524]\n",
            "save step: 1\n",
            "2019-03-18 12:09:57\n",
            "finish evaluation\n",
            "2019-03-18 12:10:03\n",
            "succ saving model in ./output/ubuntu/temp/model.ckpt.1.0\n",
            "2019-03-18 12:10:21\n",
            "step: 2, lr: 0.001\n",
            "processed: [0.2] loss: [0.820839524269104]\n",
            "save step: 2\n",
            "2019-03-18 12:10:21\n",
            "finish evaluation\n",
            "2019-03-18 12:10:23\n",
            "step: 3, lr: 0.001\n",
            "processed: [0.3] loss: [0.5034263730049133]\n",
            "save step: 3\n",
            "2019-03-18 12:10:23\n",
            "finish evaluation\n",
            "2019-03-18 12:10:24\n",
            "step: 4, lr: 0.001\n",
            "processed: [0.4] loss: [1.7838115692138672]\n",
            "save step: 4\n",
            "2019-03-18 12:10:24\n",
            "finish evaluation\n",
            "2019-03-18 12:10:25\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "succ saving model in ./output/ubuntu/temp/model.ckpt.4.0\n",
            "2019-03-18 12:10:40\n",
            "step: 5, lr: 0.001\n",
            "processed: [0.5] loss: [0.7524126172065735]\n",
            "save step: 5\n",
            "2019-03-18 12:10:41\n",
            "finish evaluation\n",
            "2019-03-18 12:10:42\n",
            "succ saving model in ./output/ubuntu/temp/model.ckpt.5.0\n",
            "2019-03-18 12:10:56\n",
            "step: 6, lr: 0.001\n",
            "processed: [0.6] loss: [0.6639571189880371]\n",
            "save step: 6\n",
            "2019-03-18 12:10:56\n",
            "finish evaluation\n",
            "2019-03-18 12:10:57\n",
            "succ saving model in ./output/ubuntu/temp/model.ckpt.6.0\n",
            "2019-03-18 12:11:12\n",
            "step: 7, lr: 0.001\n",
            "processed: [0.7] loss: [0.694038450717926]\n",
            "save step: 7\n",
            "2019-03-18 12:11:12\n",
            "finish evaluation\n",
            "2019-03-18 12:11:13\n",
            "step: 8, lr: 0.001\n",
            "processed: [0.8] loss: [0.7273024320602417]\n",
            "save step: 8\n",
            "2019-03-18 12:11:14\n",
            "finish evaluation\n",
            "2019-03-18 12:11:15\n",
            "step: 9, lr: 0.001\n",
            "processed: [0.9] loss: [0.7092913389205933]\n",
            "save step: 9\n",
            "2019-03-18 12:11:15\n",
            "finish evaluation\n",
            "2019-03-18 12:11:16\n",
            "step: 10, lr: 0.001\n",
            "processed: [1.0] loss: [0.7212758660316467]\n",
            "save step: 10\n",
            "2019-03-18 12:11:17\n",
            "finish evaluation\n",
            "2019-03-18 12:11:18\n",
            "starting shuffle train data\n",
            "finish building train data\n",
            "step: 11, lr: 0.001\n",
            "processed: [1.1] loss: [0.7184184789657593]\n",
            "save step: 11\n",
            "2019-03-18 12:11:18\n",
            "finish evaluation\n",
            "2019-03-18 12:11:19\n",
            "step: 12, lr: 0.001\n",
            "processed: [1.2] loss: [0.6469032168388367]\n",
            "save step: 12\n",
            "2019-03-18 12:11:20\n",
            "finish evaluation\n",
            "2019-03-18 12:11:21\n",
            "step: 13, lr: 0.001\n",
            "processed: [1.3] loss: [0.7097250819206238]\n",
            "save step: 13\n",
            "2019-03-18 12:11:21\n",
            "finish evaluation\n",
            "2019-03-18 12:11:22\n",
            "step: 14, lr: 0.001\n",
            "processed: [1.4] loss: [0.7251368761062622]\n",
            "save step: 14\n",
            "2019-03-18 12:11:22\n",
            "finish evaluation\n",
            "2019-03-18 12:11:24\n",
            "step: 15, lr: 0.001\n",
            "processed: [1.5] loss: [0.7014233469963074]\n",
            "save step: 15\n",
            "2019-03-18 12:11:24\n",
            "finish evaluation\n",
            "2019-03-18 12:11:25\n",
            "step: 16, lr: 0.001\n",
            "processed: [1.6] loss: [0.6658536195755005]\n",
            "save step: 16\n",
            "2019-03-18 12:11:25\n",
            "finish evaluation\n",
            "2019-03-18 12:11:27\n",
            "step: 17, lr: 0.001\n",
            "processed: [1.7] loss: [0.680726170539856]\n",
            "save step: 17\n",
            "2019-03-18 12:11:27\n",
            "finish evaluation\n",
            "2019-03-18 12:11:28\n",
            "step: 18, lr: 0.001\n",
            "processed: [1.8] loss: [0.6742277145385742]\n",
            "save step: 18\n",
            "2019-03-18 12:11:28\n",
            "finish evaluation\n",
            "2019-03-18 12:11:29\n",
            "step: 19, lr: 0.001\n",
            "processed: [1.9] loss: [0.7117820978164673]\n",
            "save step: 19\n",
            "2019-03-18 12:11:30\n",
            "finish evaluation\n",
            "2019-03-18 12:11:31\n",
            "step: 20, lr: 0.001\n",
            "processed: [2.0] loss: [0.7148741483688354]\n",
            "save step: 20\n",
            "2019-03-18 12:11:31\n",
            "finish evaluation\n",
            "2019-03-18 12:11:32\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xGyPME75c0SU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# ---------------------------------------------------------------------------------------------"
      ]
    },
    {
      "metadata": {
        "id": "E8wwJEYxdclF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Main.py  for test"
      ]
    },
    {
      "metadata": {
        "id": "dF4CL240db1O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#main.py for test\n",
        "\n",
        "#import models.net as net\n",
        "#import bin.train_and_evaluate as train\n",
        "#import bin.test_and_evaluate as test\n",
        "\n",
        "# configure\n",
        "\n",
        "conf = {\n",
        "    \"data_path\": \"./data.pkl\",\n",
        "    \"save_path\": \"./output/ubuntu/temp/\",\n",
        "    \"word_emb_init\": \"./word_embedding.pkl\",\n",
        "    \"init_model\":\"./output/ubuntu/temp/model.ckpt.1.0\", \n",
        "    \n",
        "    \"rand_seed\": None, \n",
        "\n",
        "    \"drop_dense\": None,\n",
        "    \"drop_attention\": None,\n",
        "\n",
        "    \"is_mask\": True,\n",
        "    \"is_layer_norm\": True,\n",
        "    \"is_positional\": False,  \n",
        "\n",
        "    \"stack_num\": 5,  \n",
        "    \"attention_type\": \"dot\",\n",
        "\n",
        "    \"learning_rate\": 1e-3,\n",
        "    \"vocab_size\": 434512,\n",
        "    \"emb_size\": 200,\n",
        "    \"batch_size\": 32, #200 for test\n",
        "\n",
        "    \"max_turn_num\": 9,  \n",
        "    \"max_turn_len\": 50, \n",
        "\n",
        "    \"max_to_keep\": 1,\n",
        "    \"num_scan_data\": 2,\n",
        "    \"_EOS_\": 28270, #1 for douban data\n",
        "    \"final_n_class\": 1,\n",
        "}\n",
        "\n",
        "\n",
        "model = Net(conf)\n",
        "\n",
        "#test and evaluation, init_model in conf should be set\n",
        "test(conf, model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oJZH9ptAdk3_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# ---------------------------------------------------------------------------------------------\n"
      ]
    }
  ]
}
