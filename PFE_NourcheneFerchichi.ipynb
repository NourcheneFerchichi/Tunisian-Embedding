{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PFE_NourcheneFerchichi.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbCyrBGDuTd-",
        "colab_type": "text"
      },
      "source": [
        "# Word Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eq7cB9onuajg",
        "colab_type": "text"
      },
      "source": [
        "## Importing Libraries and Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7t78S3G7sDTA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# imports needed library\n",
        "\n",
        "import gensim \n",
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zg0pk7lisEda",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_file_tn='conversation.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEi_cjXUsHHz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# visualize txt conversations data\n",
        "\n",
        "with open(data_file_tn, encoding=\"utf8\", errors='ignore') as f:\n",
        "    for i,line in enumerate (f):\n",
        "        print(line)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyomRFc4uf8C",
        "colab_type": "text"
      },
      "source": [
        "## Cleaning Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qa5AxFb9sH5S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# remove specific ponctuation from txt\n",
        "import re\n",
        "\n",
        "punctuation = r\"\"\"!\"$%&'()+,-;<=>?[\\]^`{|}~،؟\"\"\"  #only \".*#@:/ \" signs are kept\n",
        "RE_PUNCT = re.compile(r'([%s])+' % re.escape(punctuation), re.UNICODE)\n",
        "\n",
        "def strip_punctuation(text):\n",
        "    txt = utils.to_unicode(text)\n",
        "    return RE_PUNCT.sub(\" \", text)\n",
        "\n",
        "print('Those sign of punctuation were removed: \\n', '\"!$%&()+,-;<=>?[\\]^{|}~،؟' )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDGmz-2vsKvC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# install emoji package\n",
        "pip install emoji"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pd5HX6gPsMlF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# remove emoji from txt\n",
        "import emoji\n",
        "\n",
        "def strip_emoji(text):\n",
        "    allchars = [str for str in text]\n",
        "    emoji_list = [c for c in allchars if c in emoji.UNICODE_EMOJI]\n",
        "    clean_text = ' '.join([str for str in text.split() if not any(i in str for i in emoji_list)])\n",
        "    return clean_text\n",
        "  \n",
        "print('Those emojis were removed: \\n')  \n",
        "#emoji.UNICODE_EMOJI"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILkDT20WsPBD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def strip_multichar(text):\n",
        "    clean_text = re.sub(r'(.)\\1+', r'\\1\\1', text)   \n",
        "    return clean_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-3ykEj8sSMj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim import utils\n",
        "from gensim.parsing.preprocessing import preprocess_string\n",
        "from gensim.parsing.preprocessing import strip_short\n",
        "\n",
        "# order in filters' function counts!\n",
        "filters = [lambda x: x.lower(), lambda x: strip_punctuation(x),  lambda x: strip_emoji(x), lambda x : strip_multichar(x), lambda x: strip_short(x, minsize=4)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_EejQP-uluT",
        "colab_type": "text"
      },
      "source": [
        "## Tokenizing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttsnCelvsSwj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_input(input_file, filters):\n",
        "    \"\"\"This method reads the input file which is in .txt format\"\"\"\n",
        "    \n",
        "    logging.info(\"reading file {0}...this may take a while\".format(input_file))\n",
        "    \n",
        "    with open(input_file, 'rb') as f:\n",
        "        for i, line in enumerate(f): \n",
        "            if (i%10000==0):\n",
        "                logging.info (\"read {0} conversations\".format (i))\n",
        "            # do some pre-processing and return a list of words for each review text\n",
        "            yield preprocess_string(line, filters)\n",
        "\n",
        "# read the tokenized conversations into a list\n",
        "# each review item becomes a serries of words\n",
        "documents_tn = list (read_input(data_file_tn, filters))\n",
        "logging.info (\"Done reading Tunisian data file\")\n",
        "print(\"\\n Done lowcasing, removing punctuation, emojis, multiplicated characters, and words that have a size less than 3\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOyzWsEBuqgE",
        "colab_type": "text"
      },
      "source": [
        "## Running Word2Vect Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUXp0sqxsWUD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models.word2vec import Word2Vec\n",
        "\n",
        "# create vocaubulary = a set of unique words\n",
        "model_tn = Word2Vec(documents_tn, size=200, window=5, min_count=5, workers=10, sg=1, iter=10, compute_loss=True)\n",
        "# train w2v model\n",
        "model_tn.train(documents_tn,total_examples=len(documents_tn),epochs=10)\n",
        "\n",
        "# seave the trained model into .model file\n",
        "# by default, the model is saved in a binary format to save space!\n",
        "model_tn.save('model_tn.model')\n",
        "print('Saved model:', model_tn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NY3KcQ0puuBa",
        "colab_type": "text"
      },
      "source": [
        "# Pre-Processing Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFms0EOsuw-U",
        "colab_type": "text"
      },
      "source": [
        "## Importing Libraries and Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wK_XuakosYYU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "from collections import defaultdict\n",
        "import logging\n",
        "import gensim\n",
        "import numpy as np\n",
        "from random import shuffle\n",
        "import codecs\n",
        "logger = logging.getLogger('relevance_logger')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "se9TSG-xsZNH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_file_labeled_tn ='conversation_labeled.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TXBAssSsblU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# visualize txt conversations_labeled data\n",
        "\n",
        "with open(data_file_labeled_tn, encoding=\"utf8\", errors='ignore') as f:\n",
        "    for i,line in enumerate (f):\n",
        "        print(line)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfI7WdcAu0ui",
        "colab_type": "text"
      },
      "source": [
        "## Creating Multi-Turn Response Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwBQkQi5sdIr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean(x):\n",
        "    x.lower()\n",
        "    x = strip_punctuation(x)\n",
        "    x = strip_emoji(x)\n",
        "    x = strip_multichar(x)\n",
        "    x = strip_short(x, minsize=4)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aN1O_rnVsf-0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_multiturn_data_tn(trainfile, max_len=100, isshuffle=False):\n",
        "    \"\"\"\n",
        "    Transforms data.txt into a dictionary data={\"y\" : lable, \"c\":context, \"r\": response}\n",
        "    Creates vacabulary dictionary vocab={\"word\": occurance}\n",
        "    \"\"\"\n",
        "    lables = []\n",
        "    contexts = []\n",
        "    responses = []\n",
        "    vocab = defaultdict(float)\n",
        "    total = 1\n",
        "    \n",
        "    with codecs.open(trainfile,'r', encoding='utf-8-sig') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split(\"\\t\") # data template: label \\t conversation utterances (splited by \\t) \\t response \n",
        "            if (len(parts)>=3) :\n",
        "                lable = int(parts[0])  #int 0 or 1    \n",
        "            \n",
        "                utterance = \"\"\n",
        "                words = set()\n",
        "                for i in range(1,len(parts)-1,1):\n",
        "                    utterance += \" _t_ \"\n",
        "                    utterance += clean(str(parts[i]))\n",
        "                \n",
        "                    list_utterance = [utterance]\n",
        "                    words.update(set( clean(str(parts[i])).split()))\n",
        "                \n",
        "                response = clean(str(parts[-1]))\n",
        "                list_response = [response] \n",
        "            \n",
        "                lables.append(lable)\n",
        "                contexts.append(list_utterance)\n",
        "                responses.append(list_response)\n",
        "            \n",
        "                total += 1\n",
        "                if total % 1000 == 0:\n",
        "                    print (total)\n",
        "\n",
        "                words.update(set(response.split()))\n",
        "            \n",
        "                for word in words:\n",
        "                    vocab[word] += 1                      \n",
        "                       \n",
        "        data = {\"y\" : lables, \"c\":contexts,\"r\": responses}\n",
        "                \n",
        "    return data, vocab, max_len"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnkWZozQsi4T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "data_tn, vocab_tn, max_len = build_multiturn_data_tn(data_file_labeled_tn,isshuffle=False)\n",
        "logger.info(\"multiturn Tunisian dataset built!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4FLeqf0sknO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = np.array(data_tn['y'])\n",
        "c = np.array(data_tn['c'])\n",
        "r = np.array(data_tn['r'])\n",
        "\n",
        "print(len(y) == len(c) == len(r))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEhp3lxXu5Sz",
        "colab_type": "text"
      },
      "source": [
        "# Building Training, Validation and Test Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0a105ozSsnL3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "\n",
        "def build_train_set(data):\n",
        "    \"\"\"\n",
        "    Creates train with 1:1 positive-negative ratios\n",
        "    \"\"\"\n",
        "    #responses\n",
        "    negative_sample = [] #creat all negative samples from context messages \n",
        "    for i in range(len(data_tn['c'])):\n",
        "        nsample = data_tn['c'][i][0].split(\"_t_\")\n",
        "        negative_sample.append(nsample)\n",
        "        \n",
        "    responses = [] #genertae random negative responses from negative samples  \n",
        "    for i in range(len(data['r'])):\n",
        "        responses.append(data['r'][i])\n",
        "        randi = random.randint(1,len(negative_sample)-1) #generate a random i number\n",
        "        randj = random.randint(1,len(negative_sample[randi])-1) #generate a random j number  \n",
        "        responses.append([negative_sample[randi][randj]])\n",
        "            \n",
        "    #labeles\n",
        "    labeles = []\n",
        "    for i in range(len(data['y'])):\n",
        "        labeles.append(data['y'][i])\n",
        "        labeles.append(0)\n",
        "    \n",
        "    #contextes\n",
        "    contextes = []\n",
        "    for i in range(len(data['c'])):\n",
        "        contextes.append(data['c'][i])\n",
        "        contextes.append(data['c'][i])\n",
        "    \n",
        "    train_data = {\"y\":labeles, \"c\":contextes,\"r\":responses}\n",
        "    \n",
        "    return train_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O60ZIQdbsph8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train= {\"y\":data_tn['y'][:8400], \"c\":data_tn['c'][:8400],\"r\":data_tn['r'][:8400]}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIaOTGlRsrIK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_tn = build_train_set(train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5KwTFZnssp0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = np.array(train_data_tn['y'])\n",
        "c = np.array(train_data_tn['c'])\n",
        "r = np.array(train_data_tn['r'])\n",
        "\n",
        "print(len(y) == len(c) == len(r))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLEFclrosu_l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "\n",
        "def build_prediction_set(data):\n",
        "    \"\"\"\n",
        "    Creates train with 1:9 positive-negative ratios\n",
        "    \"\"\"    \n",
        "    \n",
        "    #creat all negative samples from context messages \n",
        "    negative_sample = [] \n",
        "    for i in range(len(data_tn['c'])):\n",
        "        nsample = data_tn['c'][i][0].split(\"_t_\")\n",
        "        negative_sample.append(nsample)       \n",
        "\n",
        "    #responses            \n",
        "    responses = [] #genertae random negative responses from negative samples  \n",
        "    for i in range(len(data['r'])):\n",
        "        responses.append(data['r'][i])\n",
        "        for k in range(9):\n",
        "            randi = random.randint(1,len(negative_sample)-1) #generate a random i number\n",
        "            randj = random.randint(1,len(negative_sample[randi])-1) #generate a random j number  \n",
        "            responses.append([negative_sample[randi][randj]])\n",
        "    \n",
        "    #labeles\n",
        "    labeles = []\n",
        "    for i in range(len(data['y'])):\n",
        "        labeles.append(data['y'][i])\n",
        "        for i in range(9):\n",
        "            labeles.append(0)\n",
        "      \n",
        "    #context\n",
        "    contextes = []\n",
        "    for i in range(len(data['c'])):\n",
        "        for k in range(10):\n",
        "            contextes.append(data['c'][i])\n",
        "    \n",
        "    prediction_data = {\"y\":labeles, \"c\":contextes,\"r\":responses}\n",
        "    \n",
        "    return prediction_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sooIVYT4sxLq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "valid= {\"y\":data_tn['y'][8400:10200], \"c\":data_tn['c'][8400:10200],\"r\":data_tn['r'][8400:10200]}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAwwS8OSszD9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "valid_data_tn = build_prediction_set(valid)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXmzp_aZs0ou",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = np.array(valid_data_tn['y'])\n",
        "c = np.array(valid_data_tn['c'])\n",
        "r = np.array(valid_data_tn['r'])\n",
        "\n",
        "print(len(y) == len(c) == len(r))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFQLTb3Ws2Tu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test= {\"y\":data_tn['y'][10200:], \"c\":data_tn['c'][10200:],\"r\":data_tn['r'][10200:]}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m377N0-cs4ON",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data_tn = build_prediction_set(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlKq3T0ss5qW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = np.array(test_data_tn['y'])\n",
        "c = np.array(test_data_tn['c'])\n",
        "r = np.array(test_data_tn['r'])\n",
        "\n",
        "print(len(y) == len(c) == len(r))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06Y-Bnaru_M6",
        "colab_type": "text"
      },
      "source": [
        "# Creating Word to Vect Indexes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0Y8YUAqs8Qi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class WordVecs(object):\n",
        "    def __init__(self, fname, vocab, binary, gensim):\n",
        "        if gensim:\n",
        "            word_vecs = self.load_gensim(fname,vocab)\n",
        "        self.k =  len(list(word_vecs.values())[0])\n",
        "        self.W, self.word_idx_map = self.get_W(word_vecs, k=self.k)\n",
        "\n",
        "    def load_gensim(self, fname, vocab):\n",
        "        model = Word2Vec.load(fname)\n",
        "        weights = [[0.] * model.vector_size]\n",
        "        word_vecs = {}\n",
        "        total_inside_new_embed = 0\n",
        "        miss= 0\n",
        "        for pair in vocab:\n",
        "            word = gensim.utils.to_unicode(pair)\n",
        "            if word in model:\n",
        "                total_inside_new_embed += 1\n",
        "                word_vecs[pair] = np.array([w for w in model[word]])\n",
        "                #weights.append([w for w in model[word]])\n",
        "            else:\n",
        "                miss = miss + 1\n",
        "                word_vecs[pair] = np.array([0.] * model.vector_size)\n",
        "                #weights.append([0.] * model.vector_size)\n",
        "        print ('transfer', total_inside_new_embed, 'words from the embedding file, with a total of', len(vocab), 'candidate')\n",
        "        print ('missing word2vec', miss)\n",
        "        return word_vecs\n",
        "      \n",
        "      \n",
        "    def get_W(self, word_vecs, k=200):\n",
        "        \"\"\"\n",
        "        Get word matrix. W[i] is the vector for the ith word indexed by i\n",
        "        \"\"\"\n",
        "        vocab_size = len(word_vecs)\n",
        "        word_idx_map = dict()\n",
        "        W = np.zeros(shape=(vocab_size+1, k))\n",
        "        W[0] = np.zeros(k)\n",
        "        i = 1\n",
        "        for word in word_vecs:\n",
        "            W[i] = word_vecs[word]\n",
        "            word_idx_map[word] = i\n",
        "            i += 1\n",
        "        return W, word_idx_map"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W01gGpqEs-x0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pickle.dump(word2vec_tn.W, open(\"embedding_tn.pkl\",'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FV-1lDnptGd9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models.word2vec import Word2Vec\n",
        "\n",
        "word2vec_tn = WordVecs('model_tn.model', vocab_tn, True, True)\n",
        "print('WordVec object created, it contains: word_vectors, word_idx_map')\n",
        "pickle.dump([data_tn, word2vec_tn, max_len], open(\"data_tn.test\",'wb'))\n",
        "logger.info(\"data_tn.test file uploaded! it contains a dictionary of question-answer pairs, WordVec object and max_len \")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msonSfR9vEEw",
        "colab_type": "text"
      },
      "source": [
        "## Creating Indexed Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpUH_bkPtJV2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_idx_r(response, word_idx_map):\n",
        "    \"\"\"\n",
        "    Transforms a response into a list of indexs. \n",
        "    \"\"\"\n",
        "    words = response.split() #list of words from response\n",
        "    r_idx = []  \n",
        "    for i, word in enumerate(words):\n",
        "        if word in word_idx_map:\n",
        "            r_idx.append(word_idx_map[word])\n",
        "        else:\n",
        "            r_idx.append(0)\n",
        "    return r_idx\n",
        "\n",
        "\n",
        "def get_idx_c(context, word_idx_map):\n",
        "    \"\"\"\n",
        "    Transforms a response into a list of indexs. \n",
        "    \"\"\"\n",
        "    words = context.split() #list of words from context\n",
        "    c_idx = []  \n",
        "    for i, word in enumerate(words):\n",
        "        if word in word_idx_map:\n",
        "            c_idx.append(word_idx_map[word])\n",
        "        elif word == \"_t_\":\n",
        "            c_idx.append(1000000)\n",
        "        else:\n",
        "            c_idx.append(0)\n",
        "    return c_idx\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHqeTCcEtLcr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_indexed_data_tn(data_tn, word_idx_map, max_l=50, filter_h=3, val_test_splits=[2,3],max_turn=10):\n",
        "    \"\"\"\n",
        "    Transforms data into lists of indices.\n",
        "    \"\"\"\n",
        "    #response\n",
        "    response_idx = []\n",
        "    for i in range(len(data_tn['r'])):\n",
        "        for responses in data_tn['r'][i]:\n",
        "            response_idx.append(get_idx_r(responses, word_idx_map))\n",
        "    \n",
        "    #context\n",
        "    context_idx = []\n",
        "    for i in range(len(data_tn['c'])):\n",
        "        for contexts in data_tn['c'][i]:\n",
        "            context_idx.append(get_idx_c(contexts, word_idx_map))\n",
        "    \n",
        "    #lable\n",
        "    lable = data_tn['y']\n",
        "    \n",
        "    #final indexed data\n",
        "    data = {\"y\" : lable, \"c\":context_idx,\"r\": response_idx}\n",
        "    \n",
        "    print('Indexed data prepeared!')\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtZ265WJtNjS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_word_per_utterence = 50\n",
        "dataset = r\"data_tn.test\"\n",
        "x = pickle.load(open(dataset,\"rb\"))\n",
        "data_tn, word2vec_tn, max_len = x[0], x[1], x[2]\n",
        "indexed_data_tn = make_indexed_data_tn(data_tn,word2vec_tn.word_idx_map,max_l=max_word_per_utterence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3ILElT8tPW_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Building indexed training data\n",
        "indexed_train_tn = make_indexed_data_tn(train_data_tn,word2vec_tn.word_idx_map,max_l=max_word_per_utterence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBNNMMlktRfL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Building indexed validation data\n",
        "indexed_valid_tn = make_indexed_data_tn(valid_data_tn,word2vec_tn.word_idx_map,max_l=max_word_per_utterence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIQAaaCUtTMf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Buildinging indexed test data\n",
        "indexed_test_tn = make_indexed_data_tn(test_data_tn,word2vec_tn.word_idx_map,max_l=max_word_per_utterence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0-DgnWvtU3Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#pickle hole final data\n",
        "pickle.dump([indexed_train_tn, indexed_valid_tn, indexed_test_tn], open(\"data_tn.pkl\",'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gdme8IgZtVmG",
        "colab_type": "text"
      },
      "source": [
        "# DAM Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_0_jWeAvQBz",
        "colab_type": "text"
      },
      "source": [
        "## Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InaIMvN-tW2k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## evaluation.py\n",
        "\n",
        "\n",
        "import sys\n",
        "\n",
        "def get_p_at_n_in_m(data, n, m, ind):\n",
        "    pos_score = data[ind][0]\n",
        "    curr = data[ind:ind+m]\n",
        "    curr = sorted(curr, key = lambda x:x[0], reverse=True)\n",
        "\n",
        "    if curr[n-1][0] <= pos_score:\n",
        "        return 1\n",
        "    return 0\n",
        "\n",
        "def evaluate(file_path):\n",
        "    data = []\n",
        "    with open(file_path, 'r') as file:\n",
        "        for line in file:\n",
        "            line = line.strip()\n",
        "            tokens = line.split(\"\\t\")\n",
        "        \n",
        "            if len(tokens) != 2:\n",
        "                continue\n",
        "            \n",
        "            data.append((float(tokens[0]), int(tokens[1])))\n",
        "\n",
        "\t#assert len(data) % 10 == 0\n",
        "\n",
        "    p_at_1_in_2  = 0.0\n",
        "    p_at_1_in_10 = 0.0\n",
        "    p_at_2_in_10 = 0.0\n",
        "    p_at_5_in_10 = 0.0\n",
        "    \n",
        "    length = len(data) // 10\n",
        "\n",
        "    for i in range(0, length):\n",
        "        ind = i * 10\n",
        "        assert data[ind][1] == 1\n",
        "        \n",
        "        p_at_1_in_2  += get_p_at_n_in_m(data, 1, 2, ind)\n",
        "        p_at_1_in_10 += get_p_at_n_in_m(data, 1, 10, ind)\n",
        "        p_at_2_in_10 += get_p_at_n_in_m(data, 2, 10, ind)\n",
        "        p_at_5_in_10 += get_p_at_n_in_m(data, 5, 10, ind)\n",
        "        \n",
        "    return (p_at_1_in_2/length, p_at_1_in_10/length, p_at_2_in_10/length, p_at_5_in_10/length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXs25H8qta3n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## reader.py\n",
        "\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "def unison_shuffle(data, seed=None):\n",
        "    if seed is not None:\n",
        "        np.random.seed(seed)\n",
        "\n",
        "    y = np.array(data['y'])\n",
        "    c = np.array(data['c'])\n",
        "    r = np.array(data['r'])\n",
        "\n",
        "    assert len(y) == len(c) == len(r)\n",
        "    p = np.random.permutation(len(y))\n",
        "    shuffle_data = {'y': y[p], 'c': c[p], 'r': r[p]}\n",
        "    return shuffle_data\n",
        "\n",
        "def split_c(c, split_id):\n",
        "    '''c is a list, example context\n",
        "       split_id is a integer, conf[_EOS_]\n",
        "       return nested list\n",
        "    '''\n",
        "    turns = [[]]\n",
        "    for _id in c:\n",
        "        if _id != split_id:\n",
        "            turns[-1].append(_id)\n",
        "        else:\n",
        "            turns.append([])\n",
        "    if turns[-1] == [] and len(turns) > 1:\n",
        "        turns.pop()\n",
        "    return turns\n",
        "\n",
        "def normalize_length(_list, length, cut_type='tail'):\n",
        "    '''_list is a list or nested list, example turns/r/single turn c\n",
        "       cut_type is head or tail, if _list len > length is used\n",
        "       return a list len=length and min(read_length, length)\n",
        "    '''\n",
        "    real_length = len(_list)\n",
        "    if real_length == 0:\n",
        "        return [0]*length, 0\n",
        "\n",
        "    if real_length <= length:\n",
        "        if not isinstance(_list[0], list):\n",
        "            _list.extend([0]*(length - real_length))\n",
        "        else:\n",
        "            _list.extend([[]]*(length - real_length))\n",
        "        return _list, real_length\n",
        "\n",
        "    if cut_type == 'head':\n",
        "        return _list[:length], length\n",
        "    if cut_type == 'tail':\n",
        "        return _list[-length:], length\n",
        "\n",
        "def produce_one_sample(data, index, split_id, max_turn_num, max_turn_len, turn_cut_type='tail', term_cut_type='tail'):\n",
        "    '''max_turn_num=10\n",
        "       max_turn_len=50\n",
        "       return y, nor_turns_nor_c, nor_r, turn_len, term_len, r_len\n",
        "    '''\n",
        "    c = data['c'][index]\n",
        "    r = data['r'][index][:]\n",
        "    y = data['y'][index]\n",
        "\n",
        "    turns = split_c(c, split_id)\n",
        "    #normalize turns_c length, nor_turns length is max_turn_num\n",
        "    nor_turns, turn_len = normalize_length(turns, max_turn_num, turn_cut_type)\n",
        "\n",
        "    nor_turns_nor_c = []\n",
        "    term_len = []\n",
        "    #nor_turn_nor_c length is max_turn_num, element is a list length is max_turn_len\n",
        "    for c in nor_turns:\n",
        "        #nor_c length is max_turn_len\n",
        "        nor_c, nor_c_len = normalize_length(c, max_turn_len, term_cut_type)\n",
        "        nor_turns_nor_c.append(nor_c)\n",
        "        term_len.append(nor_c_len)\n",
        "\n",
        "    nor_r, r_len = normalize_length(r, max_turn_len, term_cut_type)\n",
        "\n",
        "    return y, nor_turns_nor_c, nor_r, turn_len, term_len, r_len\n",
        "\n",
        "def build_one_batch(data, batch_index, conf, turn_cut_type='tail', term_cut_type='tail'):\n",
        "    _turns = []\n",
        "    _tt_turns_len = []\n",
        "    _every_turn_len = []\n",
        "\n",
        "    _response = []\n",
        "    _response_len = []\n",
        "\n",
        "    _label = []\n",
        "\n",
        "    for i in range(conf['batch_size']):\n",
        "        index = batch_index * conf['batch_size'] + i\n",
        "        y, nor_turns_nor_c, nor_r, turn_len, term_len, r_len = produce_one_sample(data, index, conf['_EOS_'], conf['max_turn_num'],\n",
        "                conf['max_turn_len'], turn_cut_type, term_cut_type)\n",
        "\n",
        "        _label.append(y)\n",
        "        _turns.append(nor_turns_nor_c)\n",
        "        _response.append(nor_r)\n",
        "        _every_turn_len.append(term_len)\n",
        "        _tt_turns_len.append(turn_len)\n",
        "        _response_len.append(r_len)\n",
        "\n",
        "    return _turns, _tt_turns_len, _every_turn_len, _response, _response_len, _label\n",
        "\n",
        "def build_one_batch_dict(data, batch_index, conf, turn_cut_type='tail', term_cut_type='tail'):\n",
        "    _turns, _tt_turns_len, _every_turn_len, _response, _response_len, _label = build_one_batch(data, batch_index, conf, turn_cut_type, term_cut_type)\n",
        "    ans = {'turns': _turns,\n",
        "            'tt_turns_len': _tt_turns_len,\n",
        "            'every_turn_len': _every_turn_len,\n",
        "            'response': _response,\n",
        "            'response_len': _response_len,\n",
        "            'label': _label}\n",
        "    return ans\n",
        "    \n",
        "\n",
        "def build_batches(data, conf, turn_cut_type='tail', term_cut_type='tail'):\n",
        "    _turns_batches = []\n",
        "    _tt_turns_len_batches = []\n",
        "    _every_turn_len_batches = []\n",
        "\n",
        "    _response_batches = []\n",
        "    _response_len_batches = []\n",
        "\n",
        "    _label_batches = []\n",
        "\n",
        "    batch_len = len(data['y'])//conf['batch_size']\n",
        "    for batch_index in range(batch_len):\n",
        "        _turns, _tt_turns_len, _every_turn_len, _response, _response_len, _label = build_one_batch(data, batch_index, conf, turn_cut_type='tail', term_cut_type='tail')\n",
        "\n",
        "        _turns_batches.append(_turns)\n",
        "        _tt_turns_len_batches.append(_tt_turns_len)\n",
        "        _every_turn_len_batches.append(_every_turn_len)\n",
        "\n",
        "        _response_batches.append(_response)\n",
        "        _response_len_batches.append(_response_len)\n",
        "\n",
        "        _label_batches.append(_label)\n",
        "\n",
        "    ans = { \n",
        "        \"turns\": _turns_batches, \"tt_turns_len\": _tt_turns_len_batches, \"every_turn_len\":_every_turn_len_batches,\n",
        "        \"response\": _response_batches, \"response_len\": _response_len_batches, \"label\": _label_batches\n",
        "    }   \n",
        "\n",
        "    return ans "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zK0g7gnlthHd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##operations.py\n",
        "\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "from scipy.stats import multivariate_normal\n",
        "import tensorflow as tf\n",
        "\n",
        "def learning_rate(step_num, d_model=512, warmup_steps=4000):\n",
        "    a = step_num**(-0.5)\n",
        "    b = step_num*warmup_steps**(-1.5)\n",
        "    return a, b, d_model**(-0.5) * min(step_num**(-0.5), step_num*(warmup_steps**(-1.5))) \n",
        "\n",
        "def selu(x):\n",
        "    alpha = 1.6732632423543772848170429916717\n",
        "    scale = 1.0507009873554804934193349852946\n",
        "    print('use selu')\n",
        "    return scale*tf.where(x>=0.0, x, alpha*tf.nn.elu(x))\n",
        "\n",
        "def bilinear_sim_4d(x, y, is_nor=True):\n",
        "    '''calulate bilinear similarity with two 4d tensor.\n",
        "    \n",
        "    Args:\n",
        "        x: a tensor with shape [batch, time_x, dimension_x, num_stacks]\n",
        "        y: a tensor with shape [batch, time_y, dimension_y, num_stacks]\n",
        "\n",
        "    Returns:\n",
        "        a tensor with shape [batch, time_x, time_y, num_stacks]\n",
        "\n",
        "    Raises:\n",
        "        ValueError: if\n",
        "            the shapes of x and y are not match;\n",
        "            bilinear matrix reuse error.\n",
        "    '''\n",
        "    M = tf.get_variable(\n",
        "        name=\"bilinear_matrix\", \n",
        "        shape=[x.shape[2], y.shape[2], x.shape[3]],\n",
        "        dtype=tf.float32,\n",
        "        initializer=tf.orthogonal_initializer())\n",
        "    sim = tf.einsum('biks,kls,bjls->bijs', x, M, y)\n",
        "\n",
        "    if is_nor:\n",
        "        scale = tf.sqrt(tf.cast(x.shape[2] * y.shape[2], tf.float32))\n",
        "        scale = tf.maximum(1.0, scale)\n",
        "        return sim / scale\n",
        "    else:\n",
        "        return sim\n",
        "\n",
        "\n",
        "def bilinear_sim(x, y, is_nor=True):\n",
        "    '''calculate bilinear similarity with two tensor.\n",
        "    Args:\n",
        "        x: a tensor with shape [batch, time_x, dimension_x]\n",
        "        y: a tensor with shape [batch, time_y, dimension_y]\n",
        "    \n",
        "    Returns:\n",
        "        a tensor with shape [batch, time_x, time_y]\n",
        "    Raises:\n",
        "        ValueError: if\n",
        "            the shapes of x and y are not match;\n",
        "            bilinear matrix reuse error.\n",
        "    '''\n",
        "    M = tf.get_variable(\n",
        "        name=\"bilinear_matrix\", \n",
        "        shape=[x.shape[-1], y.shape[-1]],\n",
        "        dtype=tf.float32,\n",
        "        initializer=tf.orthogonal_initializer())\n",
        "    sim = tf.einsum('bik,kl,bjl->bij', x, M, y)\n",
        "\n",
        "    if is_nor:\n",
        "        scale = tf.sqrt(tf.cast(x.shape[-1] * y.shape[-1], tf.float32))\n",
        "        scale = tf.maximum(1.0, scale)\n",
        "        return sim / scale\n",
        "    else:\n",
        "        return sim\n",
        "\n",
        "def dot_sim(x, y, is_nor=True):\n",
        "    '''calculate dot similarity with two tensor.\n",
        "\n",
        "    Args:\n",
        "        x: a tensor with shape [batch, time_x, dimension]\n",
        "        y: a tensor with shape [batch, time_y, dimension]\n",
        "    \n",
        "    Returns:\n",
        "        a tensor with shape [batch, time_x, time_y]\n",
        "    Raises:\n",
        "        AssertionError: if\n",
        "            the shapes of x and y are not match.\n",
        "    '''\n",
        "    assert x.shape[-1] == y.shape[-1]\n",
        "\n",
        "    sim = tf.einsum('bik,bjk->bij', x, y)\n",
        "\n",
        "    if is_nor:\n",
        "        scale = tf.sqrt(tf.cast(x.shape[-1], tf.float32))\n",
        "        scale = tf.maximum(1.0, scale)\n",
        "        return sim / scale\n",
        "    else:\n",
        "        return sim\n",
        "\n",
        "def layer_norm(x, axis=None, epsilon=1e-6):\n",
        "    '''Add layer normalization.\n",
        "\n",
        "    Args:\n",
        "        x: a tensor\n",
        "        axis: the dimensions to normalize\n",
        "\n",
        "    Returns:\n",
        "        a tensor the same shape as x.\n",
        "\n",
        "    Raises:\n",
        "    '''\n",
        "    print('wrong version of layer_norm')\n",
        "    scale = tf.get_variable(\n",
        "        name='scale',\n",
        "        shape=[1],\n",
        "        dtype=tf.float32,\n",
        "        initializer=tf.ones_initializer())\n",
        "    bias = tf.get_variable(\n",
        "        name='bias',\n",
        "        shape=[1],\n",
        "        dtype=tf.float32,\n",
        "        initializer=tf.zeros_initializer())\n",
        "\n",
        "    if axis is None:\n",
        "        axis = [-1]\n",
        "\n",
        "    mean = tf.reduce_mean(x, axis=axis, keep_dims=True)\n",
        "    variance = tf.reduce_mean(tf.square(x - mean), axis=axis, keep_dims=True)\n",
        "    norm = (x-mean) * tf.rsqrt(variance + epsilon)\n",
        "    return scale * norm + bias\n",
        "\n",
        "def layer_norm_debug(x, axis = None, epsilon=1e-6):\n",
        "    '''Add layer normalization.\n",
        "\n",
        "    Args:\n",
        "        x: a tensor\n",
        "        axis: the dimensions to normalize\n",
        "\n",
        "    Returns:\n",
        "        a tensor the same shape as x.\n",
        "\n",
        "    Raises:\n",
        "    '''\n",
        "    if axis is None:\n",
        "        axis = [-1]\n",
        "    shape = [x.shape[i] for i in axis]\n",
        "\n",
        "    scale = tf.get_variable(\n",
        "        name='scale',\n",
        "        shape=shape,\n",
        "        dtype=tf.float32,\n",
        "        initializer=tf.ones_initializer())\n",
        "    bias = tf.get_variable(\n",
        "        name='bias',\n",
        "        shape=shape,\n",
        "        dtype=tf.float32,\n",
        "        initializer=tf.zeros_initializer())\n",
        "\n",
        "    mean = tf.reduce_mean(x, axis=axis, keep_dims=True)\n",
        "    variance = tf.reduce_mean(tf.square(x - mean), axis=axis, keep_dims=True)\n",
        "    norm = (x-mean) * tf.rsqrt(variance + epsilon)\n",
        "    return scale * norm + bias\n",
        "\n",
        "def dense(x, out_dimension=None, add_bias=True):\n",
        "    '''Add dense connected layer, Wx + b.\n",
        "\n",
        "    Args:\n",
        "        x: a tensor with shape [batch, time, dimension]\n",
        "        out_dimension: a number which is the output dimension\n",
        "\n",
        "    Return:\n",
        "        a tensor with shape [batch, time, out_dimension]\n",
        "\n",
        "    Raises:\n",
        "    '''\n",
        "    if out_dimension is None:\n",
        "        out_dimension = x.shape[-1]\n",
        "\n",
        "    W = tf.get_variable(\n",
        "        name='weights',\n",
        "        shape=[x.shape[-1], out_dimension],\n",
        "        dtype=tf.float32,\n",
        "        initializer=tf.orthogonal_initializer())\n",
        "    if add_bias:\n",
        "        bias = tf.get_variable(\n",
        "            name='bias',\n",
        "            shape=[1],\n",
        "            dtype=tf.float32,\n",
        "            initializer=tf.zeros_initializer())\n",
        "        return tf.einsum('bik,kj->bij', x, W) + bias\n",
        "    else:\n",
        "        return tf.einsum('bik,kj->bij', x, W)\n",
        "\n",
        "def matmul_2d(x, out_dimension, drop_prob=None):\n",
        "    '''Multiplies 2-d tensor by weights.\n",
        "\n",
        "    Args:\n",
        "        x: a tensor with shape [batch, dimension]\n",
        "        out_dimension: a number\n",
        "\n",
        "    Returns:\n",
        "        a tensor with shape [batch, out_dimension]\n",
        "\n",
        "    Raises:\n",
        "    '''\n",
        "    W = tf.get_variable(\n",
        "        name='weights',\n",
        "        shape=[x.shape[1], out_dimension],\n",
        "        dtype=tf.float32,\n",
        "        initializer=tf.orthogonal_initializer())\n",
        "    if drop_prob is not None:\n",
        "        W = tf.nn.dropout(W, drop_prob)\n",
        "        print('W is dropout')\n",
        "\n",
        "    return tf.matmul(x, W)\n",
        "\n",
        "def gauss_positional_encoding_vector(x, role=0, value=0):\n",
        "    position = int(x.shape[1])\n",
        "    dimension = int(x.shape[2])\n",
        "    print('position: %s' %position)\n",
        "    print('dimension: %s' %dimension)\n",
        "\n",
        "    _lambda = tf.get_variable(\n",
        "        name='lambda',\n",
        "        shape=[position],\n",
        "        dtype=tf.float32,\n",
        "        initializer=tf.constant_initializer(value))\n",
        "    _lambda = tf.expand_dims(_lambda, axis=-1)\n",
        "\n",
        "    mean = [position/2.0, dimension/2.0]\n",
        "\n",
        "    #cov = [[position/3.0, 0], [0, dimension/3.0]]\n",
        "    sigma_x = position/math.sqrt(4.0*dimension)\n",
        "    sigma_y = math.sqrt(dimension/4.0)\n",
        "    cov = [[sigma_x*sigma_x, role*sigma_x*sigma_y], \n",
        "            [role*sigma_x*sigma_y, sigma_y*sigma_y]]\n",
        "\n",
        "    pos = np.dstack(np.mgrid[0:position, 0:dimension])\n",
        "\n",
        "    \n",
        "    rv = multivariate_normal(mean, cov)\n",
        "    signal = rv.pdf(pos) \n",
        "    signal = signal - np.max(signal)/2.0\n",
        "\n",
        "    signal = tf.multiply(_lambda, signal)\n",
        "    signal = tf.expand_dims(signal, axis=0)\n",
        "\n",
        "    print('gauss positional encoding')\n",
        "\n",
        "    return x + _lambda * signal\n",
        "\n",
        "def positional_encoding(x, min_timescale=1.0, max_timescale=1.0e4, value=0):\n",
        "    '''Adds a bunch of sinusoids of different frequencies to a tensor.\n",
        "\n",
        "    Args:\n",
        "        x: a tensor with shape [batch, length, channels]\n",
        "        min_timescale: a float\n",
        "        max_timescale: a float\n",
        "\n",
        "    Returns:\n",
        "        a tensor the same shape as x.\n",
        "\n",
        "    Raises:\n",
        "    '''\n",
        "    length = x.shape[1]\n",
        "    channels = x.shape[2]\n",
        "    _lambda = tf.get_variable(\n",
        "        name='lambda',\n",
        "        shape=[1],\n",
        "        dtype=tf.float32,\n",
        "        initializer=tf.constant_initializer(value))\n",
        "\n",
        "    position = tf.to_float(tf.range(length))\n",
        "    num_timescales = channels // 2\n",
        "    log_timescale_increment = (\n",
        "        math.log(float(max_timescale) / float(min_timescale)) /\n",
        "        (tf.to_float(num_timescales) - 1))\n",
        "    inv_timescales = min_timescale * tf.exp(\n",
        "        tf.to_float(tf.range(num_timescales)) * -log_timescale_increment)\n",
        "    scaled_time = tf.expand_dims(position, 1) * tf.expand_dims(inv_timescales, 0)\n",
        "    signal = tf.concat([tf.sin(scaled_time), tf.cos(scaled_time)], axis=1)\n",
        "    signal = tf.pad(signal, [[0, 0], [0, tf.mod(channels, 2)]])\n",
        "    #signal = tf.reshape(signal, [1, length, channels])\n",
        "    signal = tf.expand_dims(signal, axis=0)\n",
        "\n",
        "    return x + _lambda * signal\n",
        "\n",
        "\n",
        "def positional_encoding_vector(x, min_timescale=1.0, max_timescale=1.0e4, value=0):\n",
        "    '''Adds a bunch of sinusoids of different frequencies to a tensor.\n",
        "\n",
        "    Args:\n",
        "        x: a tensor with shape [batch, length, channels]\n",
        "        min_timescale: a float\n",
        "        max_timescale: a float\n",
        "\n",
        "    Returns:\n",
        "        a tensor the same shape as x.\n",
        "\n",
        "    Raises:\n",
        "    '''\n",
        "    length = x.shape[1]\n",
        "    channels = x.shape[2]\n",
        "    _lambda = tf.get_variable(\n",
        "        name='lambda',\n",
        "        shape=[length],\n",
        "        dtype=tf.float32,\n",
        "        initializer=tf.constant_initializer(value))\n",
        "    _lambda = tf.expand_dims(_lambda, axis=-1)\n",
        "\n",
        "    position = tf.to_float(tf.range(length))\n",
        "    num_timescales = channels // 2\n",
        "    log_timescale_increment = (\n",
        "        math.log(float(max_timescale) / float(min_timescale)) /\n",
        "        (tf.to_float(num_timescales) - 1))\n",
        "    inv_timescales = min_timescale * tf.exp(\n",
        "        tf.to_float(tf.range(num_timescales)) * -log_timescale_increment)\n",
        "    scaled_time = tf.expand_dims(position, 1) * tf.expand_dims(inv_timescales, 0)\n",
        "    signal = tf.concat([tf.sin(scaled_time), tf.cos(scaled_time)], axis=1)\n",
        "    signal = tf.pad(signal, [[0, 0], [0, tf.mod(channels, 2)]])\n",
        "\n",
        "    signal = tf.multiply(_lambda, signal)\n",
        "    signal = tf.expand_dims(signal, axis=0)\n",
        "\n",
        "    return x + signal\n",
        "\n",
        "def opmask(row_lengths, col_lengths, max_row_length, max_col_length):\n",
        "    '''Return a mask tensor representing the first N positions of each row and each column.\n",
        "\n",
        "    Args:\n",
        "        row_lengths: a tensor with shape [batch]\n",
        "        col_lengths: a tensor with shape [batch]\n",
        "\n",
        "    Returns:\n",
        "        a mask tensor with shape [batch, max_row_length, max_col_length]\n",
        "\n",
        "    Raises:\n",
        "    '''\n",
        "    row_mask = tf.sequence_mask(row_lengths, max_row_length) #bool, [batch, max_row_len]\n",
        "    col_mask = tf.sequence_mask(col_lengths, max_col_length) #bool, [batch, max_col_len]\n",
        "\n",
        "    row_mask = tf.cast(tf.expand_dims(row_mask, -1), tf.float32)\n",
        "    col_mask = tf.cast(tf.expand_dims(col_mask, -1), tf.float32)\n",
        "\n",
        "    return tf.einsum('bik,bjk->bij', row_mask, col_mask)\n",
        "\n",
        "def weighted_sum(weight, values):\n",
        "    '''Calcualte the weighted sum.\n",
        "\n",
        "    Args:\n",
        "        weight: a tensor with shape [batch, time, dimension]\n",
        "        values: a tensor with shape [batch, dimension, values_dimension]\n",
        "\n",
        "    Return:\n",
        "        a tensor with shape [batch, time, values_dimension]\n",
        "\n",
        "    Raises:\n",
        "    '''\n",
        "    return tf.einsum('bij,bjk->bik', weight, values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkMiiEgntm2A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##layers.py\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "#import utils.operations as op\n",
        "\n",
        "def similarity(x, y, x_lengths, y_lengths):\n",
        "    '''calculate similarity with two 3d tensor.\n",
        "\n",
        "    Args:\n",
        "        x: a tensor with shape [batch, time_x, dimension]\n",
        "        y: a tensor with shape [batch, time_y, dimension]\n",
        "\n",
        "    Returns:\n",
        "        a tensor with shape [batch, time_x, time_y]\n",
        "\n",
        "    Raises:\n",
        "        ValueError: if\n",
        "            the dimenisons of x and y are not equal.\n",
        "    '''\n",
        "    with tf.variable_scope('x_attend_y'):\n",
        "        try:\n",
        "            x_a_y = block(\n",
        "                x, y, y,\n",
        "                Q_lengths=x_lengths, K_lengths=y_lengths)\n",
        "        except ValueError:\n",
        "            tf.get_variable_scope().reuse_variables()\n",
        "            x_a_y = block(\n",
        "                x, y, y,\n",
        "                Q_lengths=x_lengths, K_lengths=y_lengths)\n",
        "\n",
        "    with tf.variable_scope('y_attend_x'):\n",
        "        try:\n",
        "            y_a_x = block(\n",
        "                y, x, x,\n",
        "                Q_lengths=y_lengths, K_lengths=x_lengths)\n",
        "        except ValueError:\n",
        "            tf.get_variable_scope().reuse_variables()\n",
        "            y_a_x = block(\n",
        "                y, x, x,\n",
        "                Q_lengths=y_lengths, K_lengths=x_lengths)\n",
        "\n",
        "    return tf.matmul(x + x_a_y, y + y_a_x, transpose_b=True)\n",
        "\n",
        "\n",
        "def dynamic_L(x):\n",
        "    '''Attention machanism to combine the infomation, \n",
        "       from https://arxiv.org/pdf/1612.01627.pdf.\n",
        "\n",
        "    Args:\n",
        "        x: a tensor with shape [batch, time, dimension]\n",
        "\n",
        "    Returns:\n",
        "        a tensor with shape [batch, dimension]\n",
        "\n",
        "    Raises:\n",
        "    '''\n",
        "    key_0 = tf.get_variable(\n",
        "        name='key',\n",
        "        shape=[x.shape[-1]],\n",
        "        dtype=tf.float32,\n",
        "        initializer=tf.random_uniform_initializer(\n",
        "            -tf.sqrt(6./tf.cast(x.shape[-1], tf.float32)),\n",
        "            tf.sqrt(6./tf.cast(x.shape[-1], tf.float32))))\n",
        "\n",
        "    key = dense(x, add_bias=False) #[batch, time, dimension]\n",
        "    weight = tf.reduce_sum(tf.multiply(key, key_0), axis=-1)  #[batch, time]\n",
        "    weight = tf.expand_dims(tf.nn.softmax(weight), -1)  #[batch, time, 1]\n",
        "\n",
        "    L = tf.reduce_sum(tf.multiply(x, weight), axis=1) #[batch, dimension]\n",
        "    return L \n",
        "\n",
        "def loss(x, y, num_classes=2, is_clip=True, clip_value=10):\n",
        "    '''From info x calculate logits as return loss.\n",
        "\n",
        "    Args:\n",
        "        x: a tensor with shape [batch, dimension]\n",
        "        num_classes: a number\n",
        "\n",
        "    Returns:\n",
        "        loss: a tensor with shape [1], which is the average loss of one batch\n",
        "        logits: a tensor with shape [batch, 1]\n",
        "\n",
        "    Raises:\n",
        "        AssertionError: if\n",
        "            num_classes is not a int greater equal than 2.\n",
        "    TODO:\n",
        "        num_classes > 2 may be not adapted.\n",
        "    '''\n",
        "    assert isinstance(num_classes, int)\n",
        "    assert num_classes >= 2\n",
        "\n",
        "    W = tf.get_variable(\n",
        "        name='weights',\n",
        "        shape=[x.shape[-1], num_classes-1],\n",
        "        initializer=tf.orthogonal_initializer())\n",
        "    bias = tf.get_variable(\n",
        "        name='bias',\n",
        "        shape=[num_classes-1],\n",
        "        initializer=tf.zeros_initializer())\n",
        "\n",
        "    logits = tf.reshape(tf.matmul(x, W) + bias, [-1])\n",
        "    loss = tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "        labels=tf.cast(y, tf.float32),\n",
        "        logits=logits)\n",
        "    loss = tf.reduce_mean(tf.clip_by_value(loss, -clip_value, clip_value))\n",
        "\n",
        "    return loss, logits\n",
        "\n",
        "def attention(\n",
        "    Q, K, V, \n",
        "    Q_lengths, K_lengths, \n",
        "    attention_type='dot', \n",
        "    is_mask=True, mask_value=-2**32+1,\n",
        "    drop_prob=None):\n",
        "    '''Add attention layer.\n",
        "    Args:\n",
        "        Q: a tensor with shape [batch, Q_time, Q_dimension]\n",
        "        K: a tensor with shape [batch, time, K_dimension]\n",
        "        V: a tensor with shape [batch, time, V_dimension]\n",
        "\n",
        "        Q_length: a tensor with shape [batch]\n",
        "        K_length: a tensor with shape [batch]\n",
        "\n",
        "    Returns:\n",
        "        a tensor with shape [batch, Q_time, V_dimension]\n",
        "\n",
        "    Raises:\n",
        "        AssertionError: if\n",
        "            Q_dimension not equal to K_dimension when attention type is dot.\n",
        "    '''\n",
        "    assert attention_type in ('dot', 'bilinear')\n",
        "    if attention_type == 'dot':\n",
        "        assert Q.shape[-1] == K.shape[-1]\n",
        "\n",
        "    Q_time = Q.shape[1]\n",
        "    K_time = K.shape[1]\n",
        "\n",
        "    if attention_type == 'dot':\n",
        "        logits = dot_sim(Q, K) #[batch, Q_time, time]\n",
        "    if attention_type == 'bilinear':\n",
        "        logits = bilinear_sim(Q, K)\n",
        "\n",
        "    if is_mask:\n",
        "        mask = opmask(Q_lengths, K_lengths, Q_time, K_time) #[batch, Q_time, K_time]\n",
        "        logits = mask * logits + (1 - mask) * mask_value\n",
        "    \n",
        "    attention = tf.nn.softmax(logits)\n",
        "\n",
        "    if drop_prob is not None:\n",
        "        print('use attention drop')\n",
        "        attention = tf.nn.dropout(attention, drop_prob)\n",
        "\n",
        "    return weighted_sum(attention, V)\n",
        "\n",
        "def FFN(x, out_dimension_0=None, out_dimension_1=None):\n",
        "    '''Add two dense connected layer, max(0, x*W0+b0)*W1+b1.\n",
        "\n",
        "    Args:\n",
        "        x: a tensor with shape [batch, time, dimension]\n",
        "        out_dimension: a number which is the output dimension\n",
        "\n",
        "    Returns:\n",
        "        a tensor with shape [batch, time, out_dimension]\n",
        "\n",
        "    Raises:\n",
        "    '''\n",
        "    with tf.variable_scope('FFN_1'):\n",
        "        y = dense(x, out_dimension_0)\n",
        "        y = tf.nn.relu(y)\n",
        "    with tf.variable_scope('FFN_2'):\n",
        "        z = dense(y, out_dimension_1) #, add_bias=False)  #!!!!\n",
        "    return z\n",
        "\n",
        "def block(\n",
        "    Q, K, V, \n",
        "    Q_lengths, K_lengths, \n",
        "    attention_type='dot', \n",
        "    is_layer_norm=True, \n",
        "    is_mask=True, mask_value=-2**32+1,\n",
        "    drop_prob=None):\n",
        "    '''Add a block unit from https://arxiv.org/pdf/1706.03762.pdf.\n",
        "    Args:\n",
        "        Q: a tensor with shape [batch, Q_time, Q_dimension]\n",
        "        K: a tensor with shape [batch, time, K_dimension]\n",
        "        V: a tensor with shape [batch, time, V_dimension]\n",
        "\n",
        "        Q_length: a tensor with shape [batch]\n",
        "        K_length: a tensor with shape [batch]\n",
        "\n",
        "    Returns:\n",
        "        a tensor with shape [batch, time, dimension]\n",
        "\n",
        "    Raises:\n",
        "    '''\n",
        "    att = attention(Q, K, V, \n",
        "                    Q_lengths, K_lengths, \n",
        "                    attention_type='dot', \n",
        "                    is_mask=is_mask, mask_value=mask_value,\n",
        "                    drop_prob=drop_prob)\n",
        "    if is_layer_norm:\n",
        "        with tf.variable_scope('attention_layer_norm'):\n",
        "            y = layer_norm_debug(Q + att)\n",
        "    else:\n",
        "        y = Q + att\n",
        "\n",
        "    z = FFN(y)\n",
        "    if is_layer_norm:\n",
        "        with tf.variable_scope('FFN_layer_norm'):\n",
        "            w = layer_norm_debug(y + z)\n",
        "    else:\n",
        "        w = y + z\n",
        "    return w\n",
        "\n",
        "def CNN(x, out_channels, filter_size, pooling_size, add_relu=True):\n",
        "    '''Add a convlution layer with relu and max pooling layer.\n",
        "\n",
        "    Args:\n",
        "        x: a tensor with shape [batch, in_height, in_width, in_channels]\n",
        "        out_channels: a number\n",
        "        filter_size: a number\n",
        "        pooling_size: a number\n",
        "\n",
        "    Returns:\n",
        "        a flattened tensor with shape [batch, num_features]\n",
        "\n",
        "    Raises:\n",
        "    '''\n",
        "    #calculate the last dimension of return\n",
        "    num_features = ((tf.shape(x)[1]-filter_size+1)/pooling_size * \n",
        "        (tf.shape(x)[2]-filter_size+1)/pooling_size) * out_channels\n",
        "\n",
        "    in_channels = x.shape[-1]\n",
        "    weights = tf.get_variable(\n",
        "        name='filter',\n",
        "        shape=[filter_size, filter_size, in_channels, out_channels],\n",
        "        dtype=tf.float32,\n",
        "        initializer=tf.random_uniform_initializer(-0.01, 0.01))\n",
        "    bias = tf.get_variable(\n",
        "        name='bias',\n",
        "        shape=[out_channels],\n",
        "        dtype=tf.float32,\n",
        "        initializer=tf.zeros_initializer())\n",
        "\n",
        "    conv = tf.nn.conv2d(x, weights, strides=[1, 1, 1, 1], padding=\"VALID\")\n",
        "    conv = conv + bias\n",
        "\n",
        "    if add_relu:\n",
        "        conv = tf.nn.relu(conv)\n",
        "\n",
        "    pooling = tf.nn.max_pool(\n",
        "        conv, \n",
        "        ksize=[1, pooling_size, pooling_size, 1],\n",
        "        strides=[1, pooling_size, pooling_size, 1], \n",
        "        padding=\"VALID\")\n",
        "\n",
        "    return tf.contrib.layers.flatten(pooling)\n",
        "\n",
        "def CNN_3d(x, out_channels_0, out_channels_1, add_relu=True):\n",
        "    '''Add a 3d convlution layer with relu and max pooling layer.\n",
        "\n",
        "    Args:\n",
        "        x: a tensor with shape [batch, in_depth, in_height, in_width, in_channels]\n",
        "        out_channels: a number\n",
        "        filter_size: a number\n",
        "        pooling_size: a number\n",
        "\n",
        "    Returns:\n",
        "        a flattened tensor with shape [batch, num_features]\n",
        "\n",
        "    Raises:\n",
        "    '''\n",
        "    in_channels = x.shape[-1]\n",
        "    weights_0 = tf.get_variable(\n",
        "        name='filter_0',\n",
        "        shape=[3, 3, 3, in_channels, out_channels_0],\n",
        "        dtype=tf.float32,\n",
        "        initializer=tf.random_uniform_initializer(-0.01, 0.01))\n",
        "    bias_0 = tf.get_variable(\n",
        "        name='bias_0',\n",
        "        shape=[out_channels_0],\n",
        "        dtype=tf.float32,\n",
        "        initializer=tf.zeros_initializer())\n",
        "\n",
        "    conv_0 = tf.nn.conv3d(x, weights_0, strides=[1, 1, 1, 1, 1], padding=\"SAME\")\n",
        "    print('conv_0 shape: %s' %conv_0.shape)\n",
        "    conv_0 = conv_0 + bias_0\n",
        "\n",
        "    if add_relu:\n",
        "        conv_0 = tf.nn.elu(conv_0)\n",
        "\n",
        "    pooling_0 = tf.nn.max_pool3d(\n",
        "        conv_0, \n",
        "        ksize=[1, 3, 3, 3, 1],\n",
        "        strides=[1, 3, 3, 3, 1], \n",
        "        padding=\"SAME\")\n",
        "    print('pooling_0 shape: %s' %pooling_0.shape)\n",
        "\n",
        "    #layer_1\n",
        "    weights_1 = tf.get_variable(\n",
        "        name='filter_1',\n",
        "        shape=[3, 3, 3, out_channels_0, out_channels_1],\n",
        "        dtype=tf.float32,\n",
        "        initializer=tf.random_uniform_initializer(-0.01, 0.01))\n",
        "    bias_1 = tf.get_variable(\n",
        "        name='bias_1',\n",
        "        shape=[out_channels_1],\n",
        "        dtype=tf.float32,\n",
        "        initializer=tf.zeros_initializer())\n",
        "\n",
        "    conv_1 = tf.nn.conv3d(pooling_0, weights_1, strides=[1, 1, 1, 1, 1], padding=\"SAME\")\n",
        "    print('conv_1 shape: %s' %conv_1.shape)\n",
        "    conv_1 = conv_1 + bias_1\n",
        "\n",
        "    if add_relu:\n",
        "        conv_1 = tf.nn.elu(conv_1)\n",
        "\n",
        "    pooling_1 = tf.nn.max_pool3d(\n",
        "        conv_1, \n",
        "        ksize=[1, 3, 3, 3, 1],\n",
        "        strides=[1, 3, 3, 3, 1], \n",
        "        padding=\"SAME\")\n",
        "    print('pooling_1 shape: %s' %pooling_1.shape)\n",
        "\n",
        "    return tf.contrib.layers.flatten(pooling_1)\n",
        "\n",
        "def CNN_3d_2d(x, out_channels_0, out_channels_1, add_relu=True):\n",
        "    '''Add a 3d convlution layer with relu and max pooling layer.\n",
        "\n",
        "    Args:\n",
        "        x: a tensor with shape [batch, in_depth, in_height, in_width, in_channels]\n",
        "        out_channels: a number\n",
        "        filter_size: a number\n",
        "        pooling_size: a number\n",
        "\n",
        "    Returns:\n",
        "        a flattened tensor with shape [batch, num_features]\n",
        "\n",
        "    Raises:\n",
        "    '''\n",
        "    in_channels = x.shape[-1]\n",
        "    weights_0 = tf.get_variable(\n",
        "        name='filter_0',\n",
        "        shape=[1, 3, 3, in_channels, out_channels_0],\n",
        "        dtype=tf.float32,\n",
        "        initializer=tf.random_uniform_initializer(-0.01, 0.01))\n",
        "    bias_0 = tf.get_variable(\n",
        "        name='bias_0',\n",
        "        shape=[out_channels_0],\n",
        "        dtype=tf.float32,\n",
        "        initializer=tf.zeros_initializer())\n",
        "\n",
        "    conv_0 = tf.nn.conv3d(x, weights_0, strides=[1, 1, 1, 1, 1], padding=\"SAME\")\n",
        "    print('conv_0 shape: %s' %conv_0.shape)\n",
        "    conv_0 = conv_0 + bias_0\n",
        "\n",
        "    if add_relu:\n",
        "        conv_0 = tf.nn.elu(conv_0)\n",
        "\n",
        "    pooling_0 = tf.nn.max_pool3d(\n",
        "        conv_0, \n",
        "        ksize=[1, 1, 3, 3, 1],\n",
        "        strides=[1, 1, 3, 3, 1], \n",
        "        padding=\"SAME\")\n",
        "    print('pooling_0 shape: %s' %pooling_0.shape)\n",
        "\n",
        "    #layer_1\n",
        "    weights_1 = tf.get_variable(\n",
        "        name='filter_1',\n",
        "        shape=[1, 3, 3, out_channels_0, out_channels_1],\n",
        "        dtype=tf.float32,\n",
        "        initializer=tf.random_uniform_initializer(-0.01, 0.01))\n",
        "    bias_1 = tf.get_variable(\n",
        "        name='bias_1',\n",
        "        shape=[out_channels_1],\n",
        "        dtype=tf.float32,\n",
        "        initializer=tf.zeros_initializer())\n",
        "\n",
        "    conv_1 = tf.nn.conv3d(pooling_0, weights_1, strides=[1, 1, 1, 1, 1], padding=\"SAME\")\n",
        "    print('conv_1 shape: %s' %conv_1.shape)\n",
        "    conv_1 = conv_1 + bias_1\n",
        "\n",
        "    if add_relu:\n",
        "        conv_1 = tf.nn.elu(conv_1)\n",
        "\n",
        "    pooling_1 = tf.nn.max_pool3d(\n",
        "        conv_1, \n",
        "        ksize=[1, 1, 3, 3, 1],\n",
        "        strides=[1, 1, 3, 3, 1], \n",
        "        padding=\"SAME\")\n",
        "    print('pooling_1 shape: %s' %pooling_1.shape)\n",
        "\n",
        "    return tf.contrib.layers.flatten(pooling_1)\n",
        "\n",
        "def CNN_3d_change(x, out_channels_0, out_channels_1, add_relu=True):\n",
        "    '''Add a 3d convlution layer with relu and max pooling layer.\n",
        "\n",
        "    Args:\n",
        "        x: a tensor with shape [batch, in_depth, in_height, in_width, in_channels]\n",
        "        out_channels: a number\n",
        "        filter_size: a number\n",
        "        pooling_size: a number\n",
        "\n",
        "    Returns:\n",
        "        a flattened tensor with shape [batch, num_features]\n",
        "\n",
        "    Raises:\n",
        "    '''\n",
        "    in_channels = x.shape[-1]\n",
        "    weights_0 = tf.get_variable(\n",
        "        name='filter_0',\n",
        "        shape=[3, 3, 3, in_channels, out_channels_0],\n",
        "        dtype=tf.float32,\n",
        "        #initializer=tf.random_normal_initializer(0, 0.05))\n",
        "        initializer=tf.random_uniform_initializer(-0.01, 0.01))\n",
        "    bias_0 = tf.get_variable(\n",
        "        name='bias_0',\n",
        "        shape=[out_channels_0],\n",
        "        dtype=tf.float32,\n",
        "        initializer=tf.zeros_initializer())\n",
        "    #Todo\n",
        "    g_0 = tf.get_variable(name='scale_0',\n",
        "        shape = [out_channels_0],\n",
        "        dtype=tf.float32,\n",
        "        initializer=tf.ones_initializer())\n",
        "    weights_0 = tf.reshape(g_0, [1, 1, 1, out_channels_0]) * tf.nn.l2_normalize(weights_0, [0, 1, 2])\n",
        "\n",
        "    conv_0 = tf.nn.conv3d(x, weights_0, strides=[1, 1, 1, 1, 1], padding=\"VALID\")\n",
        "    print('conv_0 shape: %s' %conv_0.shape)\n",
        "    conv_0 = conv_0 + bias_0\n",
        "    #######\n",
        "    '''\n",
        "    with tf.variable_scope('layer_0'):\n",
        "        conv_0 = layer_norm(conv_0, axis=[1, 2, 3, 4])\n",
        "        print('layer_norm in cnn')\n",
        "    '''\n",
        "    if add_relu:\n",
        "        conv_0 = tf.nn.elu(conv_0)\n",
        "\n",
        "    pooling_0 = tf.nn.max_pool3d(\n",
        "        conv_0, \n",
        "        ksize=[1, 2, 3, 3, 1],\n",
        "        strides=[1, 2, 3, 3, 1], \n",
        "        padding=\"VALID\")\n",
        "    print('pooling_0 shape: %s' %pooling_0.shape)\n",
        "\n",
        "    #layer_1\n",
        "    weights_1 = tf.get_variable(\n",
        "        name='filter_1',\n",
        "        shape=[2, 2, 2, out_channels_0, out_channels_1],\n",
        "        dtype=tf.float32,\n",
        "        initializer=tf.random_uniform_initializer(-0.01, 0.01))\n",
        "    \n",
        "    bias_1 = tf.get_variable(\n",
        "        name='bias_1',\n",
        "        shape=[out_channels_1],\n",
        "        dtype=tf.float32,\n",
        "        initializer=tf.zeros_initializer())\n",
        "    \n",
        "    g_1 = tf.get_variable(name='scale_1',\n",
        "        shape = [out_channels_1],\n",
        "        dtype=tf.float32,\n",
        "        initializer=tf.ones_initializer())\n",
        "    weights_1 = tf.reshape(g_1, [1, 1, 1, out_channels_1]) * tf.nn.l2_normalize(weights_1, [0, 1, 2])\n",
        "\n",
        "    conv_1 = tf.nn.conv3d(pooling_0, weights_1, strides=[1, 1, 1, 1, 1], padding=\"VALID\")\n",
        "    print('conv_1 shape: %s' %conv_1.shape)\n",
        "    conv_1 = conv_1 + bias_1\n",
        "    #with tf.variable_scope('layer_1'):\n",
        "    #    conv_1 = layer_norm(conv_1, axis=[1, 2, 3, 4])\n",
        "\n",
        "    if add_relu:\n",
        "        conv_1 = tf.nn.elu(conv_1)\n",
        "\n",
        "    pooling_1 = tf.nn.max_pool3d(\n",
        "        conv_1, \n",
        "        ksize=[1, 3, 3, 3, 1],\n",
        "        strides=[1, 3, 3, 3, 1], \n",
        "        padding=\"VALID\")\n",
        "    print('pooling_1 shape: %s' %pooling_1.shape)\n",
        "\n",
        "    return tf.contrib.layers.flatten(pooling_1)\n",
        "\n",
        "def RNN_last_state(x, lengths, hidden_size):\n",
        "    '''encode x with a gru cell and return the last state.\n",
        "    \n",
        "    Args:\n",
        "        x: a tensor with shape [batch, time, dimension]\n",
        "        length: a tensor with shape [batch]\n",
        "\n",
        "    Return:\n",
        "        a tensor with shape [batch, hidden_size]\n",
        "\n",
        "    Raises:\n",
        "    '''\n",
        "    cell = tf.nn.rnn_cell.GRUCell(hidden_size)\n",
        "    outputs, last_states = tf.nn.dynamic_rnn(cell, x, lengths, dtype=tf.float32)\n",
        "    return outputs, last_states"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiLeGM56tqq0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##train_and_evaluate \n",
        "\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import time\n",
        "\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "#import utils.reader as reader\n",
        "#import utils.evaluation as eva\n",
        "\n",
        "\n",
        "def train(conf, _model):\n",
        "    \n",
        "    if conf['rand_seed'] is not None:\n",
        "        np.random.seed(conf['rand_seed'])\n",
        "\n",
        "    if not os.path.exists(conf['save_path']):\n",
        "        os.makedirs(conf['save_path'])\n",
        "\n",
        "    # load data\n",
        "    print('starting loading data')\n",
        "    print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())))\n",
        "    files = 'data_tn.pkl'    \n",
        "    with open(files, mode='rb') as f:\n",
        "        train_data, val_data, test_data = pickle.load(f)    \n",
        "    print('finish loading data')\n",
        "    print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())))\n",
        "    \n",
        "    \n",
        "    print('starting building validation batches')\n",
        "    val_batches = build_batches(val_data, conf)\n",
        "    print(\"finish building validation batches\")\n",
        "    print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())))\n",
        "\n",
        "    # refine conf\n",
        "    batch_num = len(train_data['y']) // conf[\"batch_size\"]\n",
        "    val_batch_num = len(val_batches[\"response\"])\n",
        "\n",
        "    conf[\"train_steps\"] = conf[\"num_scan_data\"] * batch_num\n",
        "    conf[\"save_step\"] = max(1, batch_num / 10)\n",
        "    conf[\"print_step\"] = max(1, batch_num / 100)\n",
        "    print('configurations: %s' %conf)\n",
        "\n",
        "    print('model sucess')\n",
        "    print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())))\n",
        "\n",
        "    _graph = _model.build_graph()\n",
        "    print('build graph sucess')\n",
        "    print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())))\n",
        "\n",
        "    with tf.Session(graph=_graph) as sess:\n",
        "        _model.init.run();\n",
        "        if conf[\"init_model\"]:\n",
        "            _model.saver.restore(sess, conf[\"init_model\"])\n",
        "            print(\"sucess init %s\" %conf[\"init_model\"])\n",
        "\n",
        "        average_loss = 0.0\n",
        "        batch_index = 0\n",
        "        step = 0\n",
        "        best_result = [0, 0, 0, 0]\n",
        "\n",
        "        for step_i in range(conf[\"num_scan_data\"]):\n",
        "            #for batch_index in rng.permutation(range(batch_num)):\n",
        "            print('starting shuffle train data')\n",
        "            shuffle_train = unison_shuffle(train_data)\n",
        "            train_batches = build_batches(shuffle_train, conf)\n",
        "            print('finish building train data')\n",
        "            for batch_index in range(batch_num):\n",
        "                feed = {\n",
        "                    _model.turns: train_batches[\"turns\"][batch_index], \n",
        "                    _model.tt_turns_len: train_batches[\"tt_turns_len\"][batch_index],\n",
        "                    _model.every_turn_len: train_batches[\"every_turn_len\"][batch_index],\n",
        "                    _model.response: train_batches[\"response\"][batch_index], \n",
        "                    _model.response_len: train_batches[\"response_len\"][batch_index],\n",
        "                    _model.label: train_batches[\"label\"][batch_index]\n",
        "                }\n",
        "\n",
        "                batch_index = (batch_index + 1) % batch_num;\n",
        "\n",
        "                _, curr_loss = sess.run([_model.g_updates, _model.loss], feed_dict = feed)\n",
        "\n",
        "                \n",
        "                average_loss += curr_loss\n",
        "                \n",
        "                step += 1\n",
        "                #print(\"Saving steps???\", step % conf[\"print_step\"] == 0)\n",
        "                \n",
        "                if step % conf[\"print_step\"] == 0 and step > 0:\n",
        "                    g_step, lr = sess.run([_model.global_step, _model.learning_rate])\n",
        "                    print('step: %s, lr: %s' %(g_step, lr))\n",
        "                    print(\"processed: [\" + str(step * 1.0 / batch_num) + \"] loss: [\" + str(average_loss / conf[\"print_step\"]) + \"]\" )\n",
        "                    average_loss = 0\n",
        "\n",
        "                #print(\"Saving steps???\", step % conf[\"save_step\"] == 0)\n",
        "                if step % conf[\"save_step\"] == 0 and step > 0:\n",
        "                    index = step // conf['save_step']\n",
        "                    score_file_path = conf['save_path'] + 'score.' + str(index)\n",
        "                    score_file = open(score_file_path, 'w')\n",
        "                    print('save step: %s' %index)\n",
        "                    print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())))\n",
        "\n",
        "                    for batch_index in range(val_batch_num):\n",
        "                \n",
        "                        feed = { \n",
        "                            _model.turns: val_batches[\"turns\"][batch_index],\n",
        "                            _model.tt_turns_len: val_batches[\"tt_turns_len\"][batch_index],\n",
        "                            _model.every_turn_len: val_batches[\"every_turn_len\"][batch_index],\n",
        "                            _model.response: val_batches[\"response\"][batch_index],\n",
        "                            _model.response_len: val_batches[\"response_len\"][batch_index],\n",
        "                            _model.label: val_batches[\"label\"][batch_index]\n",
        "                        }   \n",
        "                \n",
        "                        scores = sess.run(_model.logits, feed_dict = feed)\n",
        "                    \n",
        "                        for i in range(conf[\"batch_size\"]):\n",
        "                            score_file.write(\n",
        "                                str(scores[i]) + '\\t' + \n",
        "                                str(val_batches[\"label\"][batch_index][i]) + '\\n')\n",
        "                    score_file.close()\n",
        "\n",
        "                    #write evaluation result\n",
        "                    result = evaluate(score_file_path)\n",
        "                    result_file_path = conf[\"save_path\"] + \"result.\" + str(index)\n",
        "                    with open(result_file_path, 'w') as out_file:\n",
        "                        for p_at in result:\n",
        "                            out_file.write(str(p_at) + '\\n')\n",
        "                    print('result = ', result)\n",
        "                    print('finish evaluation')\n",
        "                    print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())))\n",
        "                    \n",
        "                    if result[1] + result[2] > best_result[1] + best_result[2]:\n",
        "                        best_result = result\n",
        "                        _save_path = _model.saver.save(sess, conf[\"save_path\"] + \"model.ckpt.\" + str(step / conf[\"save_step\"]))\n",
        "                        print(\"succ saving model in \" + _save_path)\n",
        "                        print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WcJ4ZF3tt5t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## test_and_evaluate\n",
        "\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import time\n",
        "\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "#import utils.reader as reader\n",
        "#import utils.evaluation as eva\n",
        "\n",
        "\n",
        "def test(conf, _model):\n",
        "    \n",
        "    if not os.path.exists(conf['save_path']):\n",
        "        os.makedirs(conf['save_path'])\n",
        "\n",
        "    # load data\n",
        "    print('starting loading data')\n",
        "    print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())))\n",
        "    files = 'data_tn.pkl'  \n",
        "    with open(files, mode='rb') as f:\n",
        "        train_data, val_data, test_data = pickle.load(f)    \n",
        "    print('finish loading data')\n",
        "    print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())))\n",
        "    \n",
        "    \n",
        "    print('starting building validation batches')\n",
        "    test_batches = build_batches(test_data, conf)\n",
        "    print(\"finish building validation batches\")\n",
        "    print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())))\n",
        "\n",
        "\n",
        "    # refine conf\n",
        "    test_batch_num = len(test_batches[\"response\"])\n",
        "\n",
        "    print('configurations: %s' %conf)\n",
        "\n",
        "\n",
        "    _graph = _model.build_graph()\n",
        "    print('build graph sucess')\n",
        "    print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())))\n",
        "\n",
        "    with tf.Session(graph=_graph) as sess:\n",
        "        _model.init.run()\n",
        "        _model.saver.restore(sess, conf[\"init_model\"])\n",
        "        print(\"sucess init %s\" %conf[\"init_model\"])\n",
        "\n",
        "        batch_index = 0\n",
        "        step = 0\n",
        "\n",
        "        score_file_path = conf['save_path'] + 'score.test'\n",
        "        score_file = open(score_file_path, 'w')\n",
        "\n",
        "        print('starting test')\n",
        "        print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())))\n",
        "        for batch_index in range(test_batch_num):\n",
        "                \n",
        "            feed = { \n",
        "                _model.turns: test_batches[\"turns\"][batch_index],\n",
        "                _model.tt_turns_len: test_batches[\"tt_turns_len\"][batch_index],\n",
        "                _model.every_turn_len: test_batches[\"every_turn_len\"][batch_index],\n",
        "                _model.response: test_batches[\"response\"][batch_index],\n",
        "                _model.response_len: test_batches[\"response_len\"][batch_index],\n",
        "                _model.label: test_batches[\"label\"][batch_index]\n",
        "                }   \n",
        "                \n",
        "            scores = sess.run(_model.logits, feed_dict = feed)\n",
        "                    \n",
        "            for i in range(conf[\"batch_size\"]):\n",
        "                score_file.write(\n",
        "                    str(scores[i]) + '\\t' + \n",
        "                    str(test_batches[\"label\"][batch_index][i]) + '\\n')\n",
        "                    #str(sum(test_batches[\"every_turn_len\"][batch_index][i]) / test_batches['tt_turns_len'][batch_index][i]) + '\\t' +\n",
        "                    #str(test_batches['tt_turns_len'][batch_index][i]) + '\\n') \n",
        "\n",
        "        score_file.close()\n",
        "        print('finish test')\n",
        "        print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())))\n",
        "\n",
        "        \n",
        "        #write evaluation result\n",
        "        result = evaluate(score_file_path)\n",
        "        result_file_path = conf[\"save_path\"] + \"result.test\"\n",
        "        with open(result_file_path, 'w') as out_file:\n",
        "            for p_at in result:\n",
        "                out_file.write(str(p_at) + '\\n')\n",
        "        print('finish evaluation')\n",
        "        print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8O_13TduGJ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##net.py\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "#import utils.layers as layers\n",
        "#import utils.operations as op\n",
        "\n",
        "class Net(object):\n",
        "    '''Add positional encoding(initializer lambda is 0),\n",
        "       cross-attention, cnn integrated and grad clip by value.\n",
        "\n",
        "    Attributes:\n",
        "        conf: a configuration paramaters dict\n",
        "        word_embedding_init: a 2-d array with shape [vocab_size+1, emb_size]\n",
        "    '''\n",
        "    def __init__(self, conf):\n",
        "        self._graph = tf.Graph()\n",
        "        self._conf = conf\n",
        "\n",
        "        if self._conf['word_emb_init'] is not None:\n",
        "            print('loading word emb init')\n",
        "            self._word_embedding_init = pickle.load(open(self._conf['word_emb_init'], 'rb'), encoding='latin1')\n",
        "            #self._word_embedding_init = Word2Vec.load(open(self._conf['word_emb_init'], 'rb'))\n",
        "        else:\n",
        "            print('word emb init was set to None')\n",
        "            self._word_embedding_init = None\n",
        "\n",
        "    def build_graph(self):\n",
        "        with self._graph.as_default():\n",
        "            if self._conf['rand_seed'] is not None:\n",
        "                rand_seed = self._conf['rand_seed']\n",
        "                tf.set_random_seed(rand_seed)\n",
        "                print('set tf random seed: %s' %self._conf['rand_seed'])\n",
        "\n",
        "            #word embedding\n",
        "            if self._word_embedding_init is not None:\n",
        "                word_embedding_initializer = tf.constant_initializer(self._word_embedding_init)\n",
        "            else:\n",
        "                word_embedding_initializer = tf.random_normal_initializer(stddev=0.1)\n",
        "\n",
        "            self._word_embedding = tf.get_variable(\n",
        "                name='word_embedding',\n",
        "                shape=[self._conf['vocab_size']+1, self._conf['emb_size']],\n",
        "                dtype=tf.float32,\n",
        "                initializer=word_embedding_initializer)\n",
        "\n",
        "\n",
        "            #define placehloders\n",
        "            self.turns = tf.placeholder(\n",
        "                tf.int32,\n",
        "                shape=[self._conf[\"batch_size\"], self._conf[\"max_turn_num\"], self._conf[\"max_turn_len\"]])\n",
        "\n",
        "            self.tt_turns_len = tf.placeholder(\n",
        "                tf.int32,\n",
        "                shape=[self._conf[\"batch_size\"]])\n",
        "\n",
        "            self.every_turn_len = tf.placeholder(\n",
        "                tf.int32,\n",
        "                shape=[self._conf[\"batch_size\"], self._conf[\"max_turn_num\"]])\n",
        "    \n",
        "            self.response = tf.placeholder(\n",
        "                tf.int32, \n",
        "                shape=[self._conf[\"batch_size\"], self._conf[\"max_turn_len\"]])\n",
        "\n",
        "            self.response_len = tf.placeholder(\n",
        "                tf.int32, \n",
        "                shape=[self._conf[\"batch_size\"]])\n",
        "\n",
        "            self.label = tf.placeholder(\n",
        "                tf.float32, \n",
        "                shape=[self._conf[\"batch_size\"]])\n",
        "\n",
        "\n",
        "            #define operations\n",
        "            #response part\n",
        "            Hr = tf.nn.embedding_lookup(self._word_embedding, self.response)\n",
        "\n",
        "            if self._conf['is_positional'] and self._conf['stack_num'] > 0:\n",
        "                with tf.variable_scope('positional'):\n",
        "                    Hr = positional_encoding_vector(Hr, max_timescale=10)\n",
        "            Hr_stack = [Hr]\n",
        "\n",
        "            for index in range(self._conf['stack_num']):\n",
        "                with tf.variable_scope('self_stack_' + str(index)):\n",
        "                    Hr = block(\n",
        "                        Hr, Hr, Hr, \n",
        "                        Q_lengths=self.response_len, K_lengths=self.response_len)\n",
        "                    Hr_stack.append(Hr)\n",
        "\n",
        "\n",
        "            #context part\n",
        "            #a list of length max_turn_num, every element is a tensor with shape [batch, max_turn_len]\n",
        "            list_turn_t = tf.unstack(self.turns, axis=1) \n",
        "            list_turn_length = tf.unstack(self.every_turn_len, axis=1)\n",
        "            \n",
        "            sim_turns = []\n",
        "            #for every turn_t calculate matching vector\n",
        "            for turn_t, t_turn_length in zip(list_turn_t, list_turn_length):\n",
        "                Hu = tf.nn.embedding_lookup(self._word_embedding, turn_t) #[batch, max_turn_len, emb_size]\n",
        "\n",
        "                if self._conf['is_positional'] and self._conf['stack_num'] > 0:\n",
        "                    with tf.variable_scope('positional', reuse=True):\n",
        "                        Hu = positional_encoding_vector(Hu, max_timescale=10)\n",
        "                Hu_stack = [Hu]\n",
        "\n",
        "                for index in range(self._conf['stack_num']):\n",
        "\n",
        "                    with tf.variable_scope('self_stack_' + str(index), reuse=True):\n",
        "                        Hu = block(\n",
        "                            Hu, Hu, Hu,\n",
        "                            Q_lengths=t_turn_length, K_lengths=t_turn_length)\n",
        "\n",
        "                        Hu_stack.append(Hu)\n",
        "\n",
        "\n",
        "\n",
        "                r_a_t_stack = []\n",
        "                t_a_r_stack = []\n",
        "                for index in range(self._conf['stack_num']+1):\n",
        "\n",
        "                    with tf.variable_scope('t_attend_r_' + str(index)):\n",
        "                        try:\n",
        "                            t_a_r = block(\n",
        "                                Hu_stack[index], Hr_stack[index], Hr_stack[index],\n",
        "                                Q_lengths=t_turn_length, K_lengths=self.response_len)\n",
        "                        except ValueError:\n",
        "                            tf.get_variable_scope().reuse_variables()\n",
        "                            t_a_r = block(\n",
        "                                Hu_stack[index], Hr_stack[index], Hr_stack[index],\n",
        "                                Q_lengths=t_turn_length, K_lengths=self.response_len)\n",
        "\n",
        "\n",
        "                    with tf.variable_scope('r_attend_t_' + str(index)):\n",
        "                        try:\n",
        "                            r_a_t = block(\n",
        "                                Hr_stack[index], Hu_stack[index], Hu_stack[index],\n",
        "                                Q_lengths=self.response_len, K_lengths=t_turn_length)\n",
        "                        except ValueError:\n",
        "                            tf.get_variable_scope().reuse_variables()\n",
        "                            r_a_t = block(\n",
        "                                Hr_stack[index], Hu_stack[index], Hu_stack[index],\n",
        "                                Q_lengths=self.response_len, K_lengths=t_turn_length)\n",
        "\n",
        "                    t_a_r_stack.append(t_a_r)\n",
        "                    r_a_t_stack.append(r_a_t)\n",
        "\n",
        "                t_a_r_stack.extend(Hu_stack)\n",
        "                r_a_t_stack.extend(Hr_stack)\n",
        "                \n",
        "                t_a_r = tf.stack(t_a_r_stack, axis=-1)\n",
        "                r_a_t = tf.stack(r_a_t_stack, axis=-1)\n",
        "\n",
        "                            \n",
        "                #calculate similarity matrix\n",
        "                with tf.variable_scope('similarity'):\n",
        "                    # sim shape [batch, max_turn_len, max_turn_len, 2*stack_num+1]\n",
        "                    # divide sqrt(200) to prevent gradient explosion\n",
        "                    sim = tf.einsum('biks,bjks->bijs', t_a_r, r_a_t) / tf.sqrt(200.0)\n",
        "\n",
        "                sim_turns.append(sim)\n",
        "\n",
        "\n",
        "            #cnn and aggregation\n",
        "            sim = tf.stack(sim_turns, axis=1)\n",
        "            print('sim shape: %s' %sim.shape)\n",
        "            with tf.variable_scope('cnn_aggregation'):\n",
        "                final_info = CNN_3d(sim, 32, 16)\n",
        "                #for douban\n",
        "                #final_info = CNN_3d(sim, 16, 16)\n",
        "\n",
        "            #loss and train\n",
        "            with tf.variable_scope('loss'):\n",
        "                self.loss, self.logits = loss(final_info, self.label)\n",
        "\n",
        "                self.global_step = tf.Variable(0, trainable=False)\n",
        "                initial_learning_rate = self._conf['learning_rate']\n",
        "                self.learning_rate = tf.train.exponential_decay(\n",
        "                    initial_learning_rate,\n",
        "                    global_step=self.global_step,\n",
        "                    decay_steps=400,\n",
        "                    decay_rate=0.9,\n",
        "                    staircase=True)\n",
        "\n",
        "                Optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
        "                self.optimizer = Optimizer.minimize(\n",
        "                    self.loss,\n",
        "                    global_step=self.global_step)\n",
        "\n",
        "                self.init = tf.global_variables_initializer()\n",
        "                self.saver = tf.train.Saver(max_to_keep = self._conf[\"max_to_keep\"])\n",
        "                self.all_variables = tf.global_variables() \n",
        "                self.all_operations = self._graph.get_operations()\n",
        "                self.grads_and_vars = Optimizer.compute_gradients(self.loss)\n",
        "\n",
        "                for grad, var in self.grads_and_vars:\n",
        "                    if grad is None:\n",
        "                        print (var)\n",
        "\n",
        "                self.capped_gvs = [(tf.clip_by_value(grad, -1, 1), var) for grad, var in self.grads_and_vars]\n",
        "                self.g_updates = Optimizer.apply_gradients(\n",
        "                    self.capped_gvs,\n",
        "                    global_step=self.global_step)\n",
        "    \n",
        "        return self._graph"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qC6051ypvXoe",
        "colab_type": "text"
      },
      "source": [
        "## Training & Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yc5nt2ZfuJLP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#main.py for train\n",
        "\n",
        "#import models.net as net\n",
        "#import bin.train_and_evaluate as train\n",
        "#import bin.test_and_evaluate as test\n",
        "\n",
        "# configure\n",
        "\n",
        "conf = {\n",
        "    \"data_path\": \"./data_tn.pkl\",\n",
        "    \"save_path\": \"./output_data_tn/TunisianDailect/temp/\",\n",
        "#    \"word_emb_init\": None,\n",
        "    \"word_emb_init\": \"./embedding_tn.pkl\",\n",
        "    \"init_model\": None, #should be set for test\n",
        "\n",
        "    \"rand_seed\": None, \n",
        "\n",
        "    \"drop_dense\": None,\n",
        "    \"drop_attention\": None,\n",
        "\n",
        "    \"is_mask\": True,\n",
        "    \"is_layer_norm\": True,\n",
        "    \"is_positional\": False,  \n",
        "\n",
        "    \"stack_num\": 5,  \n",
        "    \"attention_type\": \"dot\",\n",
        "\n",
        "    \"learning_rate\": 1e-3,\n",
        "    \"vocab_size\": 36506,\n",
        "    \"emb_size\": 200,\n",
        "    \"batch_size\": 32,     #200 for test\n",
        "\n",
        "    \"max_turn_num\": 10,  \n",
        "    \"max_turn_len\": 50, \n",
        "\n",
        "    \"max_to_keep\": 1,\n",
        "    \"num_scan_data\": 2,\n",
        "    \"_EOS_\": 1000000,      #1 for douban data\n",
        "    \"final_n_class\": 1,\n",
        "}\n",
        "\n",
        "\n",
        "model_tn = Net(conf)\n",
        "train(conf, model_tn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Po0B5dKJvZro",
        "colab_type": "text"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wysy1-DuM0K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#main.py for test\n",
        "\n",
        "#import models.net as net\n",
        "#import bin.train_and_evaluate as train\n",
        "#import bin.test_and_evaluate as test\n",
        "\n",
        "# configure\n",
        "\n",
        "conf = {\n",
        "    \"data_path\": \"./data_tn.pkl\",\n",
        "    \"save_path\": \"./output_data_tn/TunisianDailect/temp/\",\n",
        "#    \"word_emb_init\": None,\n",
        "    \"word_emb_init\": \"./embedding_tn.pkl\",\n",
        "    \"init_model\": \"./output_data_tn/TunisianDailect/temp/model.ckpt.20.0\", \n",
        "\n",
        "    \"rand_seed\": None, \n",
        "\n",
        "    \"drop_dense\": None,\n",
        "    \"drop_attention\": None,\n",
        "\n",
        "    \"is_mask\": True,\n",
        "    \"is_layer_norm\": True,\n",
        "    \"is_positional\": False,  \n",
        "\n",
        "    \"stack_num\": 5,  \n",
        "    \"attention_type\": \"dot\",\n",
        "\n",
        "    \"learning_rate\": 1e-3,\n",
        "    \"vocab_size\": 36506,\n",
        "    \"emb_size\": 200,\n",
        "    \"batch_size\": 32, #200 for test\n",
        "\n",
        "    \"max_turn_num\": 10,  \n",
        "    \"max_turn_len\": 50, \n",
        "\n",
        "    \"max_to_keep\": 1,\n",
        "    \"num_scan_data\": 2,\n",
        "    \"_EOS_\": 1000000, #1 for douban data\n",
        "    \"final_n_class\": 1,\n",
        "}\n",
        "\n",
        "model_tn = Net(conf)\n",
        "\n",
        "#test and evaluation, init_model in conf should be set\n",
        "test(conf, model_tn)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}