{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kbCyrBGDuTd-"
   },
   "source": [
    "# Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Eq7cB9onuajg"
   },
   "source": [
    "## Importing Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7t78S3G7sDTA"
   },
   "outputs": [],
   "source": [
    "# imports needed library\n",
    "\n",
    "import gensim \n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zg0pk7lisEda"
   },
   "outputs": [],
   "source": [
    "data_file_tn='conversations.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FEi_cjXUsHHz"
   },
   "outputs": [],
   "source": [
    "# visualize txt conversations data\n",
    "\n",
    "with open(data_file_tn, encoding=\"utf8\", errors='ignore') as f:\n",
    "    for i,line in enumerate (f):\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TyomRFc4uf8C"
   },
   "source": [
    "## Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qa5AxFb9sH5S"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Those sign of punctuation were removed: \n",
      " \"!$%&()+,-;<=>?[\\]^{|}~،؟\n"
     ]
    }
   ],
   "source": [
    "# remove specific ponctuation from txt\n",
    "import re\n",
    "\n",
    "punctuation = r\"\"\"!\"$%&'()+,-;<=>?[\\]^`{|}~،؟\"\"\"  #only \".*#@:/ \" signs are kept\n",
    "RE_PUNCT = re.compile(r'([%s])+' % re.escape(punctuation), re.UNICODE)\n",
    "\n",
    "def strip_punctuation(text):\n",
    "    txt = utils.to_unicode(text)\n",
    "    return RE_PUNCT.sub(\" \", text)\n",
    "\n",
    "print('Those sign of punctuation were removed: \\n', '\"!$%&()+,-;<=>?[\\]^{|}~،؟' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MDGmz-2vsKvC"
   },
   "outputs": [],
   "source": [
    "# install emoji package\n",
    "pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pd5HX6gPsMlF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Those emojis were removed: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# remove emoji from txt\n",
    "import emoji\n",
    "\n",
    "def strip_emoji(text):\n",
    "    allchars = [str for str in text]\n",
    "    emoji_list = [c for c in allchars if c in emoji.UNICODE_EMOJI]\n",
    "    clean_text = ' '.join([str for str in text.split() if not any(i in str for i in emoji_list)])\n",
    "    return clean_text\n",
    "  \n",
    "print('Those emojis were removed: \\n')  \n",
    "#emoji.UNICODE_EMOJI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ILkDT20WsPBD"
   },
   "outputs": [],
   "source": [
    "def strip_multichar(text):\n",
    "    clean_text = re.sub(r'(.)\\1+', r'\\1\\1', text)   \n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E-3ykEj8sSMj"
   },
   "outputs": [],
   "source": [
    "from gensim import utils\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "from gensim.parsing.preprocessing import strip_short\n",
    "\n",
    "# order in filters' function counts!\n",
    "filters = [lambda x: x.lower(), lambda x: strip_punctuation(x),  lambda x: strip_emoji(x), lambda x : strip_multichar(x), lambda x: strip_short(x, minsize=4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0_EejQP-uluT"
   },
   "source": [
    "## Tokenizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ttsnCelvsSwj"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-30 13:14:32,922 : INFO : reading file conversations.txt...this may take a while\n",
      "2019-05-30 13:14:32,923 : INFO : read 0 conversations\n",
      "2019-05-30 13:14:33,604 : INFO : read 10000 conversations\n",
      "2019-05-30 13:14:34,240 : INFO : read 20000 conversations\n",
      "2019-05-30 13:14:34,922 : INFO : read 30000 conversations\n",
      "2019-05-30 13:14:35,584 : INFO : read 40000 conversations\n",
      "2019-05-30 13:14:35,682 : INFO : Done reading Tunisian data file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Done lowcasing, removing punctuation, emojis, multiplicated characters, and words that have a size less than 3\n"
     ]
    }
   ],
   "source": [
    "def read_input(input_file, filters):\n",
    "    \"\"\"This method reads the input file which is in .txt format\"\"\"\n",
    "    \n",
    "    logging.info(\"reading file {0}...this may take a while\".format(input_file))\n",
    "    \n",
    "    with open(input_file, 'rb') as f:\n",
    "        for i, line in enumerate(f): \n",
    "            if (i%10000==0):\n",
    "                logging.info (\"read {0} conversations\".format (i))\n",
    "            # do some pre-processing and return a list of words for each review text\n",
    "            yield preprocess_string(line, filters)\n",
    "\n",
    "# read the tokenized conversations into a list\n",
    "# each review item becomes a serries of words\n",
    "documents_tn = list (read_input(data_file_tn, filters))\n",
    "logging.info (\"Done reading Tunisian data file\")\n",
    "print(\"\\n Done lowcasing, removing punctuation, emojis, multiplicated characters, and words that have a size less than 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IOyzWsEBuqgE"
   },
   "source": [
    "## Running Word2Vect Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wUXp0sqxsWUD"
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "# create vocaubulary = a set of unique words\n",
    "model_tn = Word2Vec(documents_tn, size=200, window=5, min_count=5, workers=10, sg=1, iter=10, compute_loss=True)\n",
    "# train w2v model\n",
    "model_tn.train(documents_tn,total_examples=len(documents_tn),epochs=10)\n",
    "\n",
    "# seave the trained model into .model file\n",
    "# by default, the model is saved in a binary format to save space!\n",
    "model_tn.save('model_tn.model')\n",
    "print('Saved model:', model_tn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NY3KcQ0puuBa"
   },
   "source": [
    "# Pre-Processing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XFms0EOsuw-U"
   },
   "source": [
    "## Importing Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wK_XuakosYYU"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "import gensim\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "import codecs\n",
    "logger = logging.getLogger('relevance_logger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "se9TSG-xsZNH"
   },
   "outputs": [],
   "source": [
    "data_file_labeled_tn ='conversation_labeled.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4TXBAssSsblU"
   },
   "outputs": [],
   "source": [
    "# visualize txt conversations_labeled data\n",
    "\n",
    "with open(data_file_labeled_tn, encoding=\"utf8\", errors='ignore') as f:\n",
    "    for i,line in enumerate (f):\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EfI7WdcAu0ui"
   },
   "source": [
    "## Creating Multi-Turn Response Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JwBQkQi5sdIr"
   },
   "outputs": [],
   "source": [
    "def clean(x):\n",
    "    x.lower()\n",
    "    x = strip_punctuation(x)\n",
    "    x = strip_emoji(x)\n",
    "    x = strip_multichar(x)\n",
    "    x = strip_short(x, minsize=4)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aN1O_rnVsf-0"
   },
   "outputs": [],
   "source": [
    "def build_multiturn_data_tn(trainfile, max_len=100, isshuffle=False):\n",
    "    \"\"\"\n",
    "    Transforms data.txt into a dictionary data={\"y\" : lable, \"c\":context, \"r\": response}\n",
    "    Creates vacabulary dictionary vocab={\"word\": occurance}\n",
    "    \"\"\"\n",
    "    lables = []\n",
    "    contexts = []\n",
    "    responses = []\n",
    "    vocab = defaultdict(float)\n",
    "    total = 1\n",
    "    \n",
    "    with codecs.open(trainfile,'r', encoding='utf-8-sig') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\"\\t\") # data template: label \\t conversation utterances (splited by \\t) \\t response \n",
    "            if (len(parts)>=3) :\n",
    "                lable = int(parts[0])  #int 0 or 1    \n",
    "            \n",
    "                utterance = \"\"\n",
    "                words = set()\n",
    "                for i in range(1,len(parts)-1,1):\n",
    "                    utterance += \" _t_ \"\n",
    "                    utterance += clean(str(parts[i]))\n",
    "                \n",
    "                    list_utterance = [utterance]\n",
    "                    words.update(set( clean(str(parts[i])).split()))\n",
    "                \n",
    "                response = clean(str(parts[-1]))\n",
    "                list_response = [response] \n",
    "            \n",
    "                lables.append(lable)\n",
    "                contexts.append(list_utterance)\n",
    "                responses.append(list_response)\n",
    "            \n",
    "                total += 1\n",
    "                if total % 1000 == 0:\n",
    "                    print (total)\n",
    "\n",
    "                words.update(set(response.split()))\n",
    "            \n",
    "                for word in words:\n",
    "                    vocab[word] += 1                      \n",
    "                       \n",
    "        data = {\"y\" : lables, \"c\":contexts,\"r\": responses}\n",
    "                \n",
    "    return data, vocab, max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qnkWZozQsi4T"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-30 13:20:30,389 : INFO : multiturn Tunisian dataset built!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "data_tn, vocab_tn, max_len = build_multiturn_data_tn(data_file_labeled_tn,isshuffle=False)\n",
    "logger.info(\"multiturn Tunisian dataset built!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A4FLeqf0sknO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "y = np.array(data_tn['y'])\n",
    "c = np.array(data_tn['c'])\n",
    "r = np.array(data_tn['r'])\n",
    "\n",
    "print(len(y) == len(c) == len(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gEhp3lxXu5Sz"
   },
   "source": [
    "# Building Training, Validation and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0a105ozSsnL3"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def build_train_set(data):\n",
    "    \"\"\"\n",
    "    Creates train with 1:1 positive-negative ratios\n",
    "    \"\"\"\n",
    "    #responses\n",
    "    negative_sample = [] #creat all negative samples from context messages \n",
    "    for i in range(len(data_tn['c'])):\n",
    "        nsample = data_tn['c'][i][0].split(\"_t_\")\n",
    "        negative_sample.append(nsample)\n",
    "        \n",
    "    responses = [] #genertae random negative responses from negative samples  \n",
    "    for i in range(len(data['r'])):\n",
    "        responses.append(data['r'][i])\n",
    "        randi = random.randint(1,len(negative_sample)-1) #generate a random i number\n",
    "        randj = random.randint(1,len(negative_sample[randi])-1) #generate a random j number  \n",
    "        responses.append([negative_sample[randi][randj]])\n",
    "            \n",
    "    #labeles\n",
    "    labeles = []\n",
    "    for i in range(len(data['y'])):\n",
    "        labeles.append(data['y'][i])\n",
    "        labeles.append(0)\n",
    "    \n",
    "    #contextes\n",
    "    contextes = []\n",
    "    for i in range(len(data['c'])):\n",
    "        contextes.append(data['c'][i])\n",
    "        contextes.append(data['c'][i])\n",
    "    \n",
    "    train_data = {\"y\":labeles, \"c\":contextes,\"r\":responses}\n",
    "    \n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O60ZIQdbsph8"
   },
   "outputs": [],
   "source": [
    "train= {\"y\":data_tn['y'][:8400], \"c\":data_tn['c'][:8400],\"r\":data_tn['r'][:8400]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wIaOTGlRsrIK"
   },
   "outputs": [],
   "source": [
    "train_data_tn = build_train_set(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k5KwTFZnssp0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "y = np.array(train_data_tn['y'])\n",
    "c = np.array(train_data_tn['c'])\n",
    "r = np.array(train_data_tn['r'])\n",
    "\n",
    "print(len(y) == len(c) == len(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kLEFclrosu_l"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def build_prediction_set(data):\n",
    "    \"\"\"\n",
    "    Creates train with 1:9 positive-negative ratios\n",
    "    \"\"\"    \n",
    "    \n",
    "    #creat all negative samples from context messages \n",
    "    negative_sample = [] \n",
    "    for i in range(len(data_tn['c'])):\n",
    "        nsample = data_tn['c'][i][0].split(\"_t_\")\n",
    "        negative_sample.append(nsample)       \n",
    "\n",
    "    #responses            \n",
    "    responses = [] #genertae random negative responses from negative samples  \n",
    "    for i in range(len(data['r'])):\n",
    "        responses.append(data['r'][i])\n",
    "        for k in range(9):\n",
    "            randi = random.randint(1,len(negative_sample)-1) #generate a random i number\n",
    "            randj = random.randint(1,len(negative_sample[randi])-1) #generate a random j number  \n",
    "            responses.append([negative_sample[randi][randj]])\n",
    "    \n",
    "    #labeles\n",
    "    labeles = []\n",
    "    for i in range(len(data['y'])):\n",
    "        labeles.append(data['y'][i])\n",
    "        for i in range(9):\n",
    "            labeles.append(0)\n",
    "      \n",
    "    #context\n",
    "    contextes = []\n",
    "    for i in range(len(data['c'])):\n",
    "        for k in range(10):\n",
    "            contextes.append(data['c'][i])\n",
    "    \n",
    "    prediction_data = {\"y\":labeles, \"c\":contextes,\"r\":responses}\n",
    "    \n",
    "    return prediction_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sooIVYT4sxLq"
   },
   "outputs": [],
   "source": [
    "valid= {\"y\":data_tn['y'][8400:10200], \"c\":data_tn['c'][8400:10200],\"r\":data_tn['r'][8400:10200]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1800"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uAwwS8OSszD9"
   },
   "outputs": [],
   "source": [
    "valid_data_tn = build_prediction_set(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YXmzp_aZs0ou"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "y = np.array(valid_data_tn['y'])\n",
    "c = np.array(valid_data_tn['c'])\n",
    "r = np.array(valid_data_tn['r'])\n",
    "\n",
    "print(len(y) == len(c) == len(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JFQLTb3Ws2Tu"
   },
   "outputs": [],
   "source": [
    "test= {\"y\":data_tn['y'][10200:], \"c\":data_tn['c'][10200:],\"r\":data_tn['r'][10200:]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2059"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m377N0-cs4ON"
   },
   "outputs": [],
   "source": [
    "test_data_tn = build_prediction_set(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QlKq3T0ss5qW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "y = np.array(test_data_tn['y'])\n",
    "c = np.array(test_data_tn['c'])\n",
    "r = np.array(test_data_tn['r'])\n",
    "\n",
    "print(len(y) == len(c) == len(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "06Y-Bnaru_M6"
   },
   "source": [
    "# Creating Word to Vect Indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X0Y8YUAqs8Qi"
   },
   "outputs": [],
   "source": [
    "class WordVecs(object):\n",
    "    def __init__(self, fname, vocab, binary, gensim):\n",
    "        if gensim:\n",
    "            word_vecs = self.load_gensim(fname,vocab)\n",
    "        self.k =  len(list(word_vecs.values())[0])\n",
    "        self.W, self.word_idx_map = self.get_W(word_vecs, k=self.k)\n",
    "\n",
    "    def load_gensim(self, fname, vocab):\n",
    "        model = Word2Vec.load(fname)\n",
    "        weights = [[0.] * model.vector_size]\n",
    "        word_vecs = {}\n",
    "        total_inside_new_embed = 0\n",
    "        miss= 0\n",
    "        for pair in vocab:\n",
    "            word = gensim.utils.to_unicode(pair)\n",
    "            if word in model:\n",
    "                total_inside_new_embed += 1\n",
    "                word_vecs[pair] = np.array([w for w in model[word]])\n",
    "                #weights.append([w for w in model[word]])\n",
    "            else:\n",
    "                miss = miss + 1\n",
    "                word_vecs[pair] = np.array([0.] * model.vector_size)\n",
    "                #weights.append([0.] * model.vector_size)\n",
    "        print ('transfer', total_inside_new_embed, 'words from the embedding file, with a total of', len(vocab), 'candidate')\n",
    "        print ('missing word2vec', miss)\n",
    "        return word_vecs\n",
    "      \n",
    "      \n",
    "    def get_W(self, word_vecs, k=200):\n",
    "        \"\"\"\n",
    "        Get word matrix. W[i] is the vector for the ith word indexed by i\n",
    "        \"\"\"\n",
    "        vocab_size = len(word_vecs)\n",
    "        word_idx_map = dict()\n",
    "        W = np.zeros(shape=(vocab_size+1, k))\n",
    "        W[0] = np.zeros(k)\n",
    "        i = 1\n",
    "        for word in word_vecs:\n",
    "            W[i] = word_vecs[word]\n",
    "            word_idx_map[word] = i\n",
    "            i += 1\n",
    "        return W, word_idx_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FV-1lDnptGd9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-30 13:21:14,935 : INFO : loading Word2Vec object from model_tn.model\n",
      "2019-05-30 13:21:14,936 : WARNING : this function is deprecated, use smart_open.open instead\n",
      "2019-05-30 13:21:15,116 : INFO : loading trainables recursively from model_tn.model.trainables.* with mmap=None\n",
      "2019-05-30 13:21:15,118 : INFO : loading vocabulary recursively from model_tn.model.vocabulary.* with mmap=None\n",
      "2019-05-30 13:21:15,120 : INFO : loading wv recursively from model_tn.model.wv.* with mmap=None\n",
      "2019-05-30 13:21:15,122 : INFO : setting ignored attribute vectors_norm to None\n",
      "2019-05-30 13:21:15,124 : INFO : setting ignored attribute cum_table to None\n",
      "2019-05-30 13:21:15,126 : INFO : loaded model_tn.model\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:16: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  app.launch_new_instance()\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:18: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "2019-05-30 13:21:16,271 : INFO : data_tn.test file uploaded! it contains a dictionary of question-answer pairs, WordVec object and max_len \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transfer 8158 words from the embedding file, with a total of 36506 candidate\n",
      "missing word2vec 28348\n",
      "WordVec object created, it contains: word_vectors, word_idx_map\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "word2vec_tn = WordVecs('model_tn.model', vocab_tn, True, True)\n",
    "print('WordVec object created, it contains: word_vectors, word_idx_map')\n",
    "pickle.dump([data_tn, word2vec_tn, max_len], open(\"data_tn.test\",'wb'))\n",
    "logger.info(\"data_tn.test file uploaded! it contains a dictionary of question-answer pairs, WordVec object and max_len \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W01gGpqEs-x0"
   },
   "outputs": [],
   "source": [
    "pickle.dump(word2vec_tn.W, open(\"embedding_tn.pkl\",'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "msonSfR9vEEw"
   },
   "source": [
    "## Creating Indexed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YpUH_bkPtJV2"
   },
   "outputs": [],
   "source": [
    "def get_idx_r(response, word_idx_map):\n",
    "    \"\"\"\n",
    "    Transforms a response into a list of indexs. \n",
    "    \"\"\"\n",
    "    words = response.split() #list of words from response\n",
    "    r_idx = []  \n",
    "    for i, word in enumerate(words):\n",
    "        if word in word_idx_map:\n",
    "            r_idx.append(word_idx_map[word])\n",
    "        else:\n",
    "            r_idx.append(0)\n",
    "    return r_idx\n",
    "\n",
    "\n",
    "def get_idx_c(context, word_idx_map):\n",
    "    \"\"\"\n",
    "    Transforms a response into a list of indexs. \n",
    "    \"\"\"\n",
    "    words = context.split() #list of words from context\n",
    "    c_idx = []  \n",
    "    for i, word in enumerate(words):\n",
    "        if word in word_idx_map:\n",
    "            c_idx.append(word_idx_map[word])\n",
    "        elif word == \"_t_\":\n",
    "            c_idx.append(1000000)\n",
    "        else:\n",
    "            c_idx.append(0)\n",
    "    return c_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sHqeTCcEtLcr"
   },
   "outputs": [],
   "source": [
    "def make_indexed_data_tn(data_tn, word_idx_map, max_l=50, filter_h=3, val_test_splits=[2,3],max_turn=10):\n",
    "    \"\"\"\n",
    "    Transforms data into lists of indices.\n",
    "    \"\"\"\n",
    "    #response\n",
    "    response_idx = []\n",
    "    for i in range(len(data_tn['r'])):\n",
    "        for responses in data_tn['r'][i]:\n",
    "            response_idx.append(get_idx_r(responses, word_idx_map))\n",
    "    \n",
    "    #context\n",
    "    context_idx = []\n",
    "    for i in range(len(data_tn['c'])):\n",
    "        for contexts in data_tn['c'][i]:\n",
    "            context_idx.append(get_idx_c(contexts, word_idx_map))\n",
    "    \n",
    "    #lable\n",
    "    lable = data_tn['y']\n",
    "    \n",
    "    #final indexed data\n",
    "    data = {\"y\" : lable, \"c\":context_idx,\"r\": response_idx}\n",
    "    \n",
    "    print('Indexed data prepeared!')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CtZ265WJtNjS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed data prepeared!\n"
     ]
    }
   ],
   "source": [
    "max_word_per_utterence = 50\n",
    "dataset = r\"data_tn.test\"\n",
    "x = pickle.load(open(dataset,\"rb\"))\n",
    "data_tn, word2vec_tn, max_len = x[0], x[1], x[2]\n",
    "indexed_data_tn = make_indexed_data_tn(data_tn,word2vec_tn.word_idx_map,max_l=max_word_per_utterence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f3ILElT8tPW_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed data prepeared!\n"
     ]
    }
   ],
   "source": [
    "# Building indexed training data\n",
    "indexed_train_tn = make_indexed_data_tn(train_data_tn,word2vec_tn.word_idx_map,max_l=max_word_per_utterence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YBNNMMlktRfL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed data prepeared!\n"
     ]
    }
   ],
   "source": [
    "# Building indexed validation data\n",
    "indexed_valid_tn = make_indexed_data_tn(valid_data_tn,word2vec_tn.word_idx_map,max_l=max_word_per_utterence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cIQAaaCUtTMf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed data prepeared!\n"
     ]
    }
   ],
   "source": [
    "# Buildinging indexed test data\n",
    "indexed_test_tn = make_indexed_data_tn(test_data_tn,word2vec_tn.word_idx_map,max_l=max_word_per_utterence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A0-DgnWvtU3Q"
   },
   "outputs": [],
   "source": [
    "#pickle hole final data\n",
    "pickle.dump([indexed_train_tn, indexed_valid_tn, indexed_test_tn], open(\"data_tn.pkl\",'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gdme8IgZtVmG"
   },
   "source": [
    "# DAM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y_0_jWeAvQBz"
   },
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "InaIMvN-tW2k"
   },
   "outputs": [],
   "source": [
    "## evaluation.py\n",
    "\n",
    "\n",
    "import sys\n",
    "\n",
    "def get_p_at_n_in_m(data, n, m, ind):\n",
    "    pos_score = data[ind][0]\n",
    "    curr = data[ind:ind+m]\n",
    "    curr = sorted(curr, key = lambda x:x[0], reverse=True)\n",
    "\n",
    "    if curr[n-1][0] <= pos_score:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def evaluate(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            tokens = line.split(\"\\t\")\n",
    "        \n",
    "            if len(tokens) != 2:\n",
    "                continue\n",
    "            \n",
    "            data.append((float(tokens[0]), int(tokens[1])))\n",
    "\n",
    "\t#assert len(data) % 10 == 0\n",
    "\n",
    "    p_at_1_in_2  = 0.0\n",
    "    p_at_1_in_10 = 0.0\n",
    "    p_at_2_in_10 = 0.0\n",
    "    p_at_5_in_10 = 0.0\n",
    "    \n",
    "    length = len(data) // 10\n",
    "\n",
    "    for i in range(0, length):\n",
    "        ind = i * 10\n",
    "        assert data[ind][1] == 1\n",
    "        \n",
    "        p_at_1_in_2  += get_p_at_n_in_m(data, 1, 2, ind)\n",
    "        p_at_1_in_10 += get_p_at_n_in_m(data, 1, 10, ind)\n",
    "        p_at_2_in_10 += get_p_at_n_in_m(data, 2, 10, ind)\n",
    "        p_at_5_in_10 += get_p_at_n_in_m(data, 5, 10, ind)\n",
    "        \n",
    "    return (p_at_1_in_2/length, p_at_1_in_10/length, p_at_2_in_10/length, p_at_5_in_10/length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xXs25H8qta3n"
   },
   "outputs": [],
   "source": [
    "## reader.py\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "def unison_shuffle(data, seed=None):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    y = np.array(data['y'])\n",
    "    c = np.array(data['c'])\n",
    "    r = np.array(data['r'])\n",
    "\n",
    "    assert len(y) == len(c) == len(r)\n",
    "    p = np.random.permutation(len(y))\n",
    "    shuffle_data = {'y': y[p], 'c': c[p], 'r': r[p]}\n",
    "    return shuffle_data\n",
    "\n",
    "def split_c(c, split_id):\n",
    "    '''c is a list, example context\n",
    "       split_id is a integer, conf[_EOS_]\n",
    "       return nested list\n",
    "    '''\n",
    "    turns = [[]]\n",
    "    for _id in c:\n",
    "        if _id != split_id:\n",
    "            turns[-1].append(_id)\n",
    "        else:\n",
    "            turns.append([])\n",
    "    if turns[-1] == [] and len(turns) > 1:\n",
    "        turns.pop()\n",
    "    return turns\n",
    "\n",
    "def normalize_length(_list, length, cut_type='tail'):\n",
    "    '''_list is a list or nested list, example turns/r/single turn c\n",
    "       cut_type is head or tail, if _list len > length is used\n",
    "       return a list len=length and min(read_length, length)\n",
    "    '''\n",
    "    real_length = len(_list)\n",
    "    if real_length == 0:\n",
    "        return [0]*length, 0\n",
    "\n",
    "    if real_length <= length:\n",
    "        if not isinstance(_list[0], list):\n",
    "            _list.extend([0]*(length - real_length))\n",
    "        else:\n",
    "            _list.extend([[]]*(length - real_length))\n",
    "        return _list, real_length\n",
    "\n",
    "    if cut_type == 'head':\n",
    "        return _list[:length], length\n",
    "    if cut_type == 'tail':\n",
    "        return _list[-length:], length\n",
    "\n",
    "def produce_one_sample(data, index, split_id, max_turn_num, max_turn_len, turn_cut_type='tail', term_cut_type='tail'):\n",
    "    '''max_turn_num=10\n",
    "       max_turn_len=50\n",
    "       return y, nor_turns_nor_c, nor_r, turn_len, term_len, r_len\n",
    "    '''\n",
    "    c = data['c'][index]\n",
    "    r = data['r'][index][:]\n",
    "    y = data['y'][index]\n",
    "\n",
    "    turns = split_c(c, split_id)\n",
    "    #normalize turns_c length, nor_turns length is max_turn_num\n",
    "    nor_turns, turn_len = normalize_length(turns, max_turn_num, turn_cut_type)\n",
    "\n",
    "    nor_turns_nor_c = []\n",
    "    term_len = []\n",
    "    #nor_turn_nor_c length is max_turn_num, element is a list length is max_turn_len\n",
    "    for c in nor_turns:\n",
    "        #nor_c length is max_turn_len\n",
    "        nor_c, nor_c_len = normalize_length(c, max_turn_len, term_cut_type)\n",
    "        nor_turns_nor_c.append(nor_c)\n",
    "        term_len.append(nor_c_len)\n",
    "\n",
    "    nor_r, r_len = normalize_length(r, max_turn_len, term_cut_type)\n",
    "\n",
    "    return y, nor_turns_nor_c, nor_r, turn_len, term_len, r_len\n",
    "\n",
    "def build_one_batch(data, batch_index, conf, turn_cut_type='tail', term_cut_type='tail'):\n",
    "    _turns = []\n",
    "    _tt_turns_len = []\n",
    "    _every_turn_len = []\n",
    "\n",
    "    _response = []\n",
    "    _response_len = []\n",
    "\n",
    "    _label = []\n",
    "\n",
    "    for i in range(conf['batch_size']):\n",
    "        index = batch_index * conf['batch_size'] + i\n",
    "        y, nor_turns_nor_c, nor_r, turn_len, term_len, r_len = produce_one_sample(data, index, conf['_EOS_'], conf['max_turn_num'],\n",
    "                conf['max_turn_len'], turn_cut_type, term_cut_type)\n",
    "\n",
    "        _label.append(y)\n",
    "        _turns.append(nor_turns_nor_c)\n",
    "        _response.append(nor_r)\n",
    "        _every_turn_len.append(term_len)\n",
    "        _tt_turns_len.append(turn_len)\n",
    "        _response_len.append(r_len)\n",
    "\n",
    "    return _turns, _tt_turns_len, _every_turn_len, _response, _response_len, _label\n",
    "\n",
    "def build_one_batch_dict(data, batch_index, conf, turn_cut_type='tail', term_cut_type='tail'):\n",
    "    _turns, _tt_turns_len, _every_turn_len, _response, _response_len, _label = build_one_batch(data, batch_index, conf, turn_cut_type, term_cut_type)\n",
    "    ans = {'turns': _turns,\n",
    "            'tt_turns_len': _tt_turns_len,\n",
    "            'every_turn_len': _every_turn_len,\n",
    "            'response': _response,\n",
    "            'response_len': _response_len,\n",
    "            'label': _label}\n",
    "    return ans\n",
    "    \n",
    "\n",
    "def build_batches(data, conf, turn_cut_type='tail', term_cut_type='tail'):\n",
    "    _turns_batches = []\n",
    "    _tt_turns_len_batches = []\n",
    "    _every_turn_len_batches = []\n",
    "\n",
    "    _response_batches = []\n",
    "    _response_len_batches = []\n",
    "\n",
    "    _label_batches = []\n",
    "\n",
    "    batch_len = len(data['y'])//conf['batch_size']\n",
    "    for batch_index in range(batch_len):\n",
    "        _turns, _tt_turns_len, _every_turn_len, _response, _response_len, _label = build_one_batch(data, batch_index, conf, turn_cut_type='tail', term_cut_type='tail')\n",
    "\n",
    "        _turns_batches.append(_turns)\n",
    "        _tt_turns_len_batches.append(_tt_turns_len)\n",
    "        _every_turn_len_batches.append(_every_turn_len)\n",
    "\n",
    "        _response_batches.append(_response)\n",
    "        _response_len_batches.append(_response_len)\n",
    "\n",
    "        _label_batches.append(_label)\n",
    "\n",
    "    ans = { \n",
    "        \"turns\": _turns_batches, \"tt_turns_len\": _tt_turns_len_batches, \"every_turn_len\":_every_turn_len_batches,\n",
    "        \"response\": _response_batches, \"response_len\": _response_len_batches, \"label\": _label_batches\n",
    "    }   \n",
    "\n",
    "    return ans "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zK0g7gnlthHd"
   },
   "outputs": [],
   "source": [
    "##operations.py\n",
    "\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "import tensorflow as tf\n",
    "\n",
    "def learning_rate(step_num, d_model=512, warmup_steps=4000):\n",
    "    a = step_num**(-0.5)\n",
    "    b = step_num*warmup_steps**(-1.5)\n",
    "    return a, b, d_model**(-0.5) * min(step_num**(-0.5), step_num*(warmup_steps**(-1.5))) \n",
    "\n",
    "def selu(x):\n",
    "    alpha = 1.6732632423543772848170429916717\n",
    "    scale = 1.0507009873554804934193349852946\n",
    "    print('use selu')\n",
    "    return scale*tf.where(x>=0.0, x, alpha*tf.nn.elu(x))\n",
    "\n",
    "def bilinear_sim_4d(x, y, is_nor=True):\n",
    "    '''calulate bilinear similarity with two 4d tensor.\n",
    "    \n",
    "    Args:\n",
    "        x: a tensor with shape [batch, time_x, dimension_x, num_stacks]\n",
    "        y: a tensor with shape [batch, time_y, dimension_y, num_stacks]\n",
    "\n",
    "    Returns:\n",
    "        a tensor with shape [batch, time_x, time_y, num_stacks]\n",
    "\n",
    "    Raises:\n",
    "        ValueError: if\n",
    "            the shapes of x and y are not match;\n",
    "            bilinear matrix reuse error.\n",
    "    '''\n",
    "    M = tf.get_variable(\n",
    "        name=\"bilinear_matrix\", \n",
    "        shape=[x.shape[2], y.shape[2], x.shape[3]],\n",
    "        dtype=tf.float32,\n",
    "        initializer=tf.orthogonal_initializer())\n",
    "    sim = tf.einsum('biks,kls,bjls->bijs', x, M, y)\n",
    "\n",
    "    if is_nor:\n",
    "        scale = tf.sqrt(tf.cast(x.shape[2] * y.shape[2], tf.float32))\n",
    "        scale = tf.maximum(1.0, scale)\n",
    "        return sim / scale\n",
    "    else:\n",
    "        return sim\n",
    "\n",
    "\n",
    "def bilinear_sim(x, y, is_nor=True):\n",
    "    '''calculate bilinear similarity with two tensor.\n",
    "    Args:\n",
    "        x: a tensor with shape [batch, time_x, dimension_x]\n",
    "        y: a tensor with shape [batch, time_y, dimension_y]\n",
    "    \n",
    "    Returns:\n",
    "        a tensor with shape [batch, time_x, time_y]\n",
    "    Raises:\n",
    "        ValueError: if\n",
    "            the shapes of x and y are not match;\n",
    "            bilinear matrix reuse error.\n",
    "    '''\n",
    "    M = tf.get_variable(\n",
    "        name=\"bilinear_matrix\", \n",
    "        shape=[x.shape[-1], y.shape[-1]],\n",
    "        dtype=tf.float32,\n",
    "        initializer=tf.orthogonal_initializer())\n",
    "    sim = tf.einsum('bik,kl,bjl->bij', x, M, y)\n",
    "\n",
    "    if is_nor:\n",
    "        scale = tf.sqrt(tf.cast(x.shape[-1] * y.shape[-1], tf.float32))\n",
    "        scale = tf.maximum(1.0, scale)\n",
    "        return sim / scale\n",
    "    else:\n",
    "        return sim\n",
    "\n",
    "def dot_sim(x, y, is_nor=True):\n",
    "    '''calculate dot similarity with two tensor.\n",
    "\n",
    "    Args:\n",
    "        x: a tensor with shape [batch, time_x, dimension]\n",
    "        y: a tensor with shape [batch, time_y, dimension]\n",
    "    \n",
    "    Returns:\n",
    "        a tensor with shape [batch, time_x, time_y]\n",
    "    Raises:\n",
    "        AssertionError: if\n",
    "            the shapes of x and y are not match.\n",
    "    '''\n",
    "    assert x.shape[-1] == y.shape[-1]\n",
    "\n",
    "    sim = tf.einsum('bik,bjk->bij', x, y)\n",
    "\n",
    "    if is_nor:\n",
    "        scale = tf.sqrt(tf.cast(x.shape[-1], tf.float32))\n",
    "        scale = tf.maximum(1.0, scale)\n",
    "        return sim / scale\n",
    "    else:\n",
    "        return sim\n",
    "\n",
    "def layer_norm(x, axis=None, epsilon=1e-6):\n",
    "    '''Add layer normalization.\n",
    "\n",
    "    Args:\n",
    "        x: a tensor\n",
    "        axis: the dimensions to normalize\n",
    "\n",
    "    Returns:\n",
    "        a tensor the same shape as x.\n",
    "\n",
    "    Raises:\n",
    "    '''\n",
    "    print('wrong version of layer_norm')\n",
    "    scale = tf.get_variable(\n",
    "        name='scale',\n",
    "        shape=[1],\n",
    "        dtype=tf.float32,\n",
    "        initializer=tf.ones_initializer())\n",
    "    bias = tf.get_variable(\n",
    "        name='bias',\n",
    "        shape=[1],\n",
    "        dtype=tf.float32,\n",
    "        initializer=tf.zeros_initializer())\n",
    "\n",
    "    if axis is None:\n",
    "        axis = [-1]\n",
    "\n",
    "    mean = tf.reduce_mean(x, axis=axis, keep_dims=True)\n",
    "    variance = tf.reduce_mean(tf.square(x - mean), axis=axis, keep_dims=True)\n",
    "    norm = (x-mean) * tf.rsqrt(variance + epsilon)\n",
    "    return scale * norm + bias\n",
    "\n",
    "def layer_norm_debug(x, axis = None, epsilon=1e-6):\n",
    "    '''Add layer normalization.\n",
    "\n",
    "    Args:\n",
    "        x: a tensor\n",
    "        axis: the dimensions to normalize\n",
    "\n",
    "    Returns:\n",
    "        a tensor the same shape as x.\n",
    "\n",
    "    Raises:\n",
    "    '''\n",
    "    if axis is None:\n",
    "        axis = [-1]\n",
    "    shape = [x.shape[i] for i in axis]\n",
    "\n",
    "    scale = tf.get_variable(\n",
    "        name='scale',\n",
    "        shape=shape,\n",
    "        dtype=tf.float32,\n",
    "        initializer=tf.ones_initializer())\n",
    "    bias = tf.get_variable(\n",
    "        name='bias',\n",
    "        shape=shape,\n",
    "        dtype=tf.float32,\n",
    "        initializer=tf.zeros_initializer())\n",
    "\n",
    "    mean = tf.reduce_mean(x, axis=axis, keep_dims=True)\n",
    "    variance = tf.reduce_mean(tf.square(x - mean), axis=axis, keep_dims=True)\n",
    "    norm = (x-mean) * tf.rsqrt(variance + epsilon)\n",
    "    return scale * norm + bias\n",
    "\n",
    "def dense(x, out_dimension=None, add_bias=True):\n",
    "    '''Add dense connected layer, Wx + b.\n",
    "\n",
    "    Args:\n",
    "        x: a tensor with shape [batch, time, dimension]\n",
    "        out_dimension: a number which is the output dimension\n",
    "\n",
    "    Return:\n",
    "        a tensor with shape [batch, time, out_dimension]\n",
    "\n",
    "    Raises:\n",
    "    '''\n",
    "    if out_dimension is None:\n",
    "        out_dimension = x.shape[-1]\n",
    "\n",
    "    W = tf.get_variable(\n",
    "        name='weights',\n",
    "        shape=[x.shape[-1], out_dimension],\n",
    "        dtype=tf.float32,\n",
    "        initializer=tf.orthogonal_initializer())\n",
    "    if add_bias:\n",
    "        bias = tf.get_variable(\n",
    "            name='bias',\n",
    "            shape=[1],\n",
    "            dtype=tf.float32,\n",
    "            initializer=tf.zeros_initializer())\n",
    "        return tf.einsum('bik,kj->bij', x, W) + bias\n",
    "    else:\n",
    "        return tf.einsum('bik,kj->bij', x, W)\n",
    "\n",
    "def matmul_2d(x, out_dimension, drop_prob=None):\n",
    "    '''Multiplies 2-d tensor by weights.\n",
    "\n",
    "    Args:\n",
    "        x: a tensor with shape [batch, dimension]\n",
    "        out_dimension: a number\n",
    "\n",
    "    Returns:\n",
    "        a tensor with shape [batch, out_dimension]\n",
    "\n",
    "    Raises:\n",
    "    '''\n",
    "    W = tf.get_variable(\n",
    "        name='weights',\n",
    "        shape=[x.shape[1], out_dimension],\n",
    "        dtype=tf.float32,\n",
    "        initializer=tf.orthogonal_initializer())\n",
    "    if drop_prob is not None:\n",
    "        W = tf.nn.dropout(W, drop_prob)\n",
    "        print('W is dropout')\n",
    "\n",
    "    return tf.matmul(x, W)\n",
    "\n",
    "def gauss_positional_encoding_vector(x, role=0, value=0):\n",
    "    position = int(x.shape[1])\n",
    "    dimension = int(x.shape[2])\n",
    "    print('position: %s' %position)\n",
    "    print('dimension: %s' %dimension)\n",
    "\n",
    "    _lambda = tf.get_variable(\n",
    "        name='lambda',\n",
    "        shape=[position],\n",
    "        dtype=tf.float32,\n",
    "        initializer=tf.constant_initializer(value))\n",
    "    _lambda = tf.expand_dims(_lambda, axis=-1)\n",
    "\n",
    "    mean = [position/2.0, dimension/2.0]\n",
    "\n",
    "    #cov = [[position/3.0, 0], [0, dimension/3.0]]\n",
    "    sigma_x = position/math.sqrt(4.0*dimension)\n",
    "    sigma_y = math.sqrt(dimension/4.0)\n",
    "    cov = [[sigma_x*sigma_x, role*sigma_x*sigma_y], \n",
    "            [role*sigma_x*sigma_y, sigma_y*sigma_y]]\n",
    "\n",
    "    pos = np.dstack(np.mgrid[0:position, 0:dimension])\n",
    "\n",
    "    \n",
    "    rv = multivariate_normal(mean, cov)\n",
    "    signal = rv.pdf(pos) \n",
    "    signal = signal - np.max(signal)/2.0\n",
    "\n",
    "    signal = tf.multiply(_lambda, signal)\n",
    "    signal = tf.expand_dims(signal, axis=0)\n",
    "\n",
    "    print('gauss positional encoding')\n",
    "\n",
    "    return x + _lambda * signal\n",
    "\n",
    "def positional_encoding(x, min_timescale=1.0, max_timescale=1.0e4, value=0):\n",
    "    '''Adds a bunch of sinusoids of different frequencies to a tensor.\n",
    "\n",
    "    Args:\n",
    "        x: a tensor with shape [batch, length, channels]\n",
    "        min_timescale: a float\n",
    "        max_timescale: a float\n",
    "\n",
    "    Returns:\n",
    "        a tensor the same shape as x.\n",
    "\n",
    "    Raises:\n",
    "    '''\n",
    "    length = x.shape[1]\n",
    "    channels = x.shape[2]\n",
    "    _lambda = tf.get_variable(\n",
    "        name='lambda',\n",
    "        shape=[1],\n",
    "        dtype=tf.float32,\n",
    "        initializer=tf.constant_initializer(value))\n",
    "\n",
    "    position = tf.to_float(tf.range(length))\n",
    "    num_timescales = channels // 2\n",
    "    log_timescale_increment = (\n",
    "        math.log(float(max_timescale) / float(min_timescale)) /\n",
    "        (tf.to_float(num_timescales) - 1))\n",
    "    inv_timescales = min_timescale * tf.exp(\n",
    "        tf.to_float(tf.range(num_timescales)) * -log_timescale_increment)\n",
    "    scaled_time = tf.expand_dims(position, 1) * tf.expand_dims(inv_timescales, 0)\n",
    "    signal = tf.concat([tf.sin(scaled_time), tf.cos(scaled_time)], axis=1)\n",
    "    signal = tf.pad(signal, [[0, 0], [0, tf.mod(channels, 2)]])\n",
    "    #signal = tf.reshape(signal, [1, length, channels])\n",
    "    signal = tf.expand_dims(signal, axis=0)\n",
    "\n",
    "    return x + _lambda * signal\n",
    "\n",
    "\n",
    "def positional_encoding_vector(x, min_timescale=1.0, max_timescale=1.0e4, value=0):\n",
    "    '''Adds a bunch of sinusoids of different frequencies to a tensor.\n",
    "\n",
    "    Args:\n",
    "        x: a tensor with shape [batch, length, channels]\n",
    "        min_timescale: a float\n",
    "        max_timescale: a float\n",
    "\n",
    "    Returns:\n",
    "        a tensor the same shape as x.\n",
    "\n",
    "    Raises:\n",
    "    '''\n",
    "    length = x.shape[1]\n",
    "    channels = x.shape[2]\n",
    "    _lambda = tf.get_variable(\n",
    "        name='lambda',\n",
    "        shape=[length],\n",
    "        dtype=tf.float32,\n",
    "        initializer=tf.constant_initializer(value))\n",
    "    _lambda = tf.expand_dims(_lambda, axis=-1)\n",
    "\n",
    "    position = tf.to_float(tf.range(length))\n",
    "    num_timescales = channels // 2\n",
    "    log_timescale_increment = (\n",
    "        math.log(float(max_timescale) / float(min_timescale)) /\n",
    "        (tf.to_float(num_timescales) - 1))\n",
    "    inv_timescales = min_timescale * tf.exp(\n",
    "        tf.to_float(tf.range(num_timescales)) * -log_timescale_increment)\n",
    "    scaled_time = tf.expand_dims(position, 1) * tf.expand_dims(inv_timescales, 0)\n",
    "    signal = tf.concat([tf.sin(scaled_time), tf.cos(scaled_time)], axis=1)\n",
    "    signal = tf.pad(signal, [[0, 0], [0, tf.mod(channels, 2)]])\n",
    "\n",
    "    signal = tf.multiply(_lambda, signal)\n",
    "    signal = tf.expand_dims(signal, axis=0)\n",
    "\n",
    "    return x + signal\n",
    "\n",
    "def opmask(row_lengths, col_lengths, max_row_length, max_col_length):\n",
    "    '''Return a mask tensor representing the first N positions of each row and each column.\n",
    "\n",
    "    Args:\n",
    "        row_lengths: a tensor with shape [batch]\n",
    "        col_lengths: a tensor with shape [batch]\n",
    "\n",
    "    Returns:\n",
    "        a mask tensor with shape [batch, max_row_length, max_col_length]\n",
    "\n",
    "    Raises:\n",
    "    '''\n",
    "    row_mask = tf.sequence_mask(row_lengths, max_row_length) #bool, [batch, max_row_len]\n",
    "    col_mask = tf.sequence_mask(col_lengths, max_col_length) #bool, [batch, max_col_len]\n",
    "\n",
    "    row_mask = tf.cast(tf.expand_dims(row_mask, -1), tf.float32)\n",
    "    col_mask = tf.cast(tf.expand_dims(col_mask, -1), tf.float32)\n",
    "\n",
    "    return tf.einsum('bik,bjk->bij', row_mask, col_mask)\n",
    "\n",
    "def weighted_sum(weight, values):\n",
    "    '''Calcualte the weighted sum.\n",
    "\n",
    "    Args:\n",
    "        weight: a tensor with shape [batch, time, dimension]\n",
    "        values: a tensor with shape [batch, dimension, values_dimension]\n",
    "\n",
    "    Return:\n",
    "        a tensor with shape [batch, time, values_dimension]\n",
    "\n",
    "    Raises:\n",
    "    '''\n",
    "    return tf.einsum('bij,bjk->bik', weight, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rkMiiEgntm2A"
   },
   "outputs": [],
   "source": [
    "##layers.py\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "#import utils.operations as op\n",
    "\n",
    "def similarity(x, y, x_lengths, y_lengths):\n",
    "    '''calculate similarity with two 3d tensor.\n",
    "\n",
    "    Args:\n",
    "        x: a tensor with shape [batch, time_x, dimension]\n",
    "        y: a tensor with shape [batch, time_y, dimension]\n",
    "\n",
    "    Returns:\n",
    "        a tensor with shape [batch, time_x, time_y]\n",
    "\n",
    "    Raises:\n",
    "        ValueError: if\n",
    "            the dimenisons of x and y are not equal.\n",
    "    '''\n",
    "    with tf.variable_scope('x_attend_y'):\n",
    "        try:\n",
    "            x_a_y = block(\n",
    "                x, y, y,\n",
    "                Q_lengths=x_lengths, K_lengths=y_lengths)\n",
    "        except ValueError:\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "            x_a_y = block(\n",
    "                x, y, y,\n",
    "                Q_lengths=x_lengths, K_lengths=y_lengths)\n",
    "\n",
    "    with tf.variable_scope('y_attend_x'):\n",
    "        try:\n",
    "            y_a_x = block(\n",
    "                y, x, x,\n",
    "                Q_lengths=y_lengths, K_lengths=x_lengths)\n",
    "        except ValueError:\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "            y_a_x = block(\n",
    "                y, x, x,\n",
    "                Q_lengths=y_lengths, K_lengths=x_lengths)\n",
    "\n",
    "    return tf.matmul(x + x_a_y, y + y_a_x, transpose_b=True)\n",
    "\n",
    "\n",
    "def dynamic_L(x):\n",
    "    '''Attention machanism to combine the infomation, \n",
    "       from https://arxiv.org/pdf/1612.01627.pdf.\n",
    "\n",
    "    Args:\n",
    "        x: a tensor with shape [batch, time, dimension]\n",
    "\n",
    "    Returns:\n",
    "        a tensor with shape [batch, dimension]\n",
    "\n",
    "    Raises:\n",
    "    '''\n",
    "    key_0 = tf.get_variable(\n",
    "        name='key',\n",
    "        shape=[x.shape[-1]],\n",
    "        dtype=tf.float32,\n",
    "        initializer=tf.random_uniform_initializer(\n",
    "            -tf.sqrt(6./tf.cast(x.shape[-1], tf.float32)),\n",
    "            tf.sqrt(6./tf.cast(x.shape[-1], tf.float32))))\n",
    "\n",
    "    key = dense(x, add_bias=False) #[batch, time, dimension]\n",
    "    weight = tf.reduce_sum(tf.multiply(key, key_0), axis=-1)  #[batch, time]\n",
    "    weight = tf.expand_dims(tf.nn.softmax(weight), -1)  #[batch, time, 1]\n",
    "\n",
    "    L = tf.reduce_sum(tf.multiply(x, weight), axis=1) #[batch, dimension]\n",
    "    return L \n",
    "\n",
    "def loss(x, y, num_classes=2, is_clip=True, clip_value=10):\n",
    "    '''From info x calculate logits as return loss.\n",
    "\n",
    "    Args:\n",
    "        x: a tensor with shape [batch, dimension]\n",
    "        num_classes: a number\n",
    "\n",
    "    Returns:\n",
    "        loss: a tensor with shape [1], which is the average loss of one batch\n",
    "        logits: a tensor with shape [batch, 1]\n",
    "\n",
    "    Raises:\n",
    "        AssertionError: if\n",
    "            num_classes is not a int greater equal than 2.\n",
    "    TODO:\n",
    "        num_classes > 2 may be not adapted.\n",
    "    '''\n",
    "    assert isinstance(num_classes, int)\n",
    "    assert num_classes >= 2\n",
    "\n",
    "    W = tf.get_variable(\n",
    "        name='weights',\n",
    "        shape=[x.shape[-1], num_classes-1],\n",
    "        initializer=tf.orthogonal_initializer())\n",
    "    bias = tf.get_variable(\n",
    "        name='bias',\n",
    "        shape=[num_classes-1],\n",
    "        initializer=tf.zeros_initializer())\n",
    "\n",
    "    logits = tf.reshape(tf.matmul(x, W) + bias, [-1])\n",
    "    loss = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "        labels=tf.cast(y, tf.float32),\n",
    "        logits=logits)\n",
    "    loss = tf.reduce_mean(tf.clip_by_value(loss, -clip_value, clip_value))\n",
    "\n",
    "    return loss, logits\n",
    "\n",
    "def attention(\n",
    "    Q, K, V, \n",
    "    Q_lengths, K_lengths, \n",
    "    attention_type='dot', \n",
    "    is_mask=True, mask_value=-2**32+1,\n",
    "    drop_prob=None):\n",
    "    '''Add attention layer.\n",
    "    Args:\n",
    "        Q: a tensor with shape [batch, Q_time, Q_dimension]\n",
    "        K: a tensor with shape [batch, time, K_dimension]\n",
    "        V: a tensor with shape [batch, time, V_dimension]\n",
    "\n",
    "        Q_length: a tensor with shape [batch]\n",
    "        K_length: a tensor with shape [batch]\n",
    "\n",
    "    Returns:\n",
    "        a tensor with shape [batch, Q_time, V_dimension]\n",
    "\n",
    "    Raises:\n",
    "        AssertionError: if\n",
    "            Q_dimension not equal to K_dimension when attention type is dot.\n",
    "    '''\n",
    "    assert attention_type in ('dot', 'bilinear')\n",
    "    if attention_type == 'dot':\n",
    "        assert Q.shape[-1] == K.shape[-1]\n",
    "\n",
    "    Q_time = Q.shape[1]\n",
    "    K_time = K.shape[1]\n",
    "\n",
    "    if attention_type == 'dot':\n",
    "        logits = dot_sim(Q, K) #[batch, Q_time, time]\n",
    "    if attention_type == 'bilinear':\n",
    "        logits = bilinear_sim(Q, K)\n",
    "\n",
    "    if is_mask:\n",
    "        mask = opmask(Q_lengths, K_lengths, Q_time, K_time) #[batch, Q_time, K_time]\n",
    "        logits = mask * logits + (1 - mask) * mask_value\n",
    "    \n",
    "    attention = tf.nn.softmax(logits)\n",
    "\n",
    "    if drop_prob is not None:\n",
    "        print('use attention drop')\n",
    "        attention = tf.nn.dropout(attention, drop_prob)\n",
    "\n",
    "    return weighted_sum(attention, V)\n",
    "\n",
    "def FFN(x, out_dimension_0=None, out_dimension_1=None):\n",
    "    '''Add two dense connected layer, max(0, x*W0+b0)*W1+b1.\n",
    "\n",
    "    Args:\n",
    "        x: a tensor with shape [batch, time, dimension]\n",
    "        out_dimension: a number which is the output dimension\n",
    "\n",
    "    Returns:\n",
    "        a tensor with shape [batch, time, out_dimension]\n",
    "\n",
    "    Raises:\n",
    "    '''\n",
    "    with tf.variable_scope('FFN_1'):\n",
    "        y = dense(x, out_dimension_0)\n",
    "        y = tf.nn.relu(y)\n",
    "    with tf.variable_scope('FFN_2'):\n",
    "        z = dense(y, out_dimension_1) #, add_bias=False)  #!!!!\n",
    "    return z\n",
    "\n",
    "def block(\n",
    "    Q, K, V, \n",
    "    Q_lengths, K_lengths, \n",
    "    attention_type='dot', \n",
    "    is_layer_norm=True, \n",
    "    is_mask=True, mask_value=-2**32+1,\n",
    "    drop_prob=None):\n",
    "    '''Add a block unit from https://arxiv.org/pdf/1706.03762.pdf.\n",
    "    Args:\n",
    "        Q: a tensor with shape [batch, Q_time, Q_dimension]\n",
    "        K: a tensor with shape [batch, time, K_dimension]\n",
    "        V: a tensor with shape [batch, time, V_dimension]\n",
    "\n",
    "        Q_length: a tensor with shape [batch]\n",
    "        K_length: a tensor with shape [batch]\n",
    "\n",
    "    Returns:\n",
    "        a tensor with shape [batch, time, dimension]\n",
    "\n",
    "    Raises:\n",
    "    '''\n",
    "    att = attention(Q, K, V, \n",
    "                    Q_lengths, K_lengths, \n",
    "                    attention_type='dot', \n",
    "                    is_mask=is_mask, mask_value=mask_value,\n",
    "                    drop_prob=drop_prob)\n",
    "    if is_layer_norm:\n",
    "        with tf.variable_scope('attention_layer_norm'):\n",
    "            y = layer_norm_debug(Q + att)\n",
    "    else:\n",
    "        y = Q + att\n",
    "\n",
    "    z = FFN(y)\n",
    "    if is_layer_norm:\n",
    "        with tf.variable_scope('FFN_layer_norm'):\n",
    "            w = layer_norm_debug(y + z)\n",
    "    else:\n",
    "        w = y + z\n",
    "    return w\n",
    "\n",
    "def CNN(x, out_channels, filter_size, pooling_size, add_relu=True):\n",
    "    '''Add a convlution layer with relu and max pooling layer.\n",
    "\n",
    "    Args:\n",
    "        x: a tensor with shape [batch, in_height, in_width, in_channels]\n",
    "        out_channels: a number\n",
    "        filter_size: a number\n",
    "        pooling_size: a number\n",
    "\n",
    "    Returns:\n",
    "        a flattened tensor with shape [batch, num_features]\n",
    "\n",
    "    Raises:\n",
    "    '''\n",
    "    #calculate the last dimension of return\n",
    "    num_features = ((tf.shape(x)[1]-filter_size+1)/pooling_size * \n",
    "        (tf.shape(x)[2]-filter_size+1)/pooling_size) * out_channels\n",
    "\n",
    "    in_channels = x.shape[-1]\n",
    "    weights = tf.get_variable(\n",
    "        name='filter',\n",
    "        shape=[filter_size, filter_size, in_channels, out_channels],\n",
    "        dtype=tf.float32,\n",
    "        initializer=tf.random_uniform_initializer(-0.01, 0.01))\n",
    "    bias = tf.get_variable(\n",
    "        name='bias',\n",
    "        shape=[out_channels],\n",
    "        dtype=tf.float32,\n",
    "        initializer=tf.zeros_initializer())\n",
    "\n",
    "    conv = tf.nn.conv2d(x, weights, strides=[1, 1, 1, 1], padding=\"VALID\")\n",
    "    conv = conv + bias\n",
    "\n",
    "    if add_relu:\n",
    "        conv = tf.nn.relu(conv)\n",
    "\n",
    "    pooling = tf.nn.max_pool(\n",
    "        conv, \n",
    "        ksize=[1, pooling_size, pooling_size, 1],\n",
    "        strides=[1, pooling_size, pooling_size, 1], \n",
    "        padding=\"VALID\")\n",
    "\n",
    "    return tf.contrib.layers.flatten(pooling)\n",
    "\n",
    "def CNN_3d(x, out_channels_0, out_channels_1, add_relu=True):\n",
    "    '''Add a 3d convlution layer with relu and max pooling layer.\n",
    "\n",
    "    Args:\n",
    "        x: a tensor with shape [batch, in_depth, in_height, in_width, in_channels]\n",
    "        out_channels: a number\n",
    "        filter_size: a number\n",
    "        pooling_size: a number\n",
    "\n",
    "    Returns:\n",
    "        a flattened tensor with shape [batch, num_features]\n",
    "\n",
    "    Raises:\n",
    "    '''\n",
    "    in_channels = x.shape[-1]\n",
    "    weights_0 = tf.get_variable(\n",
    "        name='filter_0',\n",
    "        shape=[3, 3, 3, in_channels, out_channels_0],\n",
    "        dtype=tf.float32,\n",
    "        initializer=tf.random_uniform_initializer(-0.01, 0.01))\n",
    "    bias_0 = tf.get_variable(\n",
    "        name='bias_0',\n",
    "        shape=[out_channels_0],\n",
    "        dtype=tf.float32,\n",
    "        initializer=tf.zeros_initializer())\n",
    "\n",
    "    conv_0 = tf.nn.conv3d(x, weights_0, strides=[1, 1, 1, 1, 1], padding=\"SAME\")\n",
    "    print('conv_0 shape: %s' %conv_0.shape)\n",
    "    conv_0 = conv_0 + bias_0\n",
    "\n",
    "    if add_relu:\n",
    "        conv_0 = tf.nn.elu(conv_0)\n",
    "\n",
    "    pooling_0 = tf.nn.max_pool3d(\n",
    "        conv_0, \n",
    "        ksize=[1, 3, 3, 3, 1],\n",
    "        strides=[1, 3, 3, 3, 1], \n",
    "        padding=\"SAME\")\n",
    "    print('pooling_0 shape: %s' %pooling_0.shape)\n",
    "\n",
    "    #layer_1\n",
    "    weights_1 = tf.get_variable(\n",
    "        name='filter_1',\n",
    "        shape=[3, 3, 3, out_channels_0, out_channels_1],\n",
    "        dtype=tf.float32,\n",
    "        initializer=tf.random_uniform_initializer(-0.01, 0.01))\n",
    "    bias_1 = tf.get_variable(\n",
    "        name='bias_1',\n",
    "        shape=[out_channels_1],\n",
    "        dtype=tf.float32,\n",
    "        initializer=tf.zeros_initializer())\n",
    "\n",
    "    conv_1 = tf.nn.conv3d(pooling_0, weights_1, strides=[1, 1, 1, 1, 1], padding=\"SAME\")\n",
    "    print('conv_1 shape: %s' %conv_1.shape)\n",
    "    conv_1 = conv_1 + bias_1\n",
    "\n",
    "    if add_relu:\n",
    "        conv_1 = tf.nn.elu(conv_1)\n",
    "\n",
    "    pooling_1 = tf.nn.max_pool3d(\n",
    "        conv_1, \n",
    "        ksize=[1, 3, 3, 3, 1],\n",
    "        strides=[1, 3, 3, 3, 1], \n",
    "        padding=\"SAME\")\n",
    "    print('pooling_1 shape: %s' %pooling_1.shape)\n",
    "\n",
    "    return tf.contrib.layers.flatten(pooling_1)\n",
    "\n",
    "def CNN_3d_2d(x, out_channels_0, out_channels_1, add_relu=True):\n",
    "    '''Add a 3d convlution layer with relu and max pooling layer.\n",
    "\n",
    "    Args:\n",
    "        x: a tensor with shape [batch, in_depth, in_height, in_width, in_channels]\n",
    "        out_channels: a number\n",
    "        filter_size: a number\n",
    "        pooling_size: a number\n",
    "\n",
    "    Returns:\n",
    "        a flattened tensor with shape [batch, num_features]\n",
    "\n",
    "    Raises:\n",
    "    '''\n",
    "    in_channels = x.shape[-1]\n",
    "    weights_0 = tf.get_variable(\n",
    "        name='filter_0',\n",
    "        shape=[1, 3, 3, in_channels, out_channels_0],\n",
    "        dtype=tf.float32,\n",
    "        initializer=tf.random_uniform_initializer(-0.01, 0.01))\n",
    "    bias_0 = tf.get_variable(\n",
    "        name='bias_0',\n",
    "        shape=[out_channels_0],\n",
    "        dtype=tf.float32,\n",
    "        initializer=tf.zeros_initializer())\n",
    "\n",
    "    conv_0 = tf.nn.conv3d(x, weights_0, strides=[1, 1, 1, 1, 1], padding=\"SAME\")\n",
    "    print('conv_0 shape: %s' %conv_0.shape)\n",
    "    conv_0 = conv_0 + bias_0\n",
    "\n",
    "    if add_relu:\n",
    "        conv_0 = tf.nn.elu(conv_0)\n",
    "\n",
    "    pooling_0 = tf.nn.max_pool3d(\n",
    "        conv_0, \n",
    "        ksize=[1, 1, 3, 3, 1],\n",
    "        strides=[1, 1, 3, 3, 1], \n",
    "        padding=\"SAME\")\n",
    "    print('pooling_0 shape: %s' %pooling_0.shape)\n",
    "\n",
    "    #layer_1\n",
    "    weights_1 = tf.get_variable(\n",
    "        name='filter_1',\n",
    "        shape=[1, 3, 3, out_channels_0, out_channels_1],\n",
    "        dtype=tf.float32,\n",
    "        initializer=tf.random_uniform_initializer(-0.01, 0.01))\n",
    "    bias_1 = tf.get_variable(\n",
    "        name='bias_1',\n",
    "        shape=[out_channels_1],\n",
    "        dtype=tf.float32,\n",
    "        initializer=tf.zeros_initializer())\n",
    "\n",
    "    conv_1 = tf.nn.conv3d(pooling_0, weights_1, strides=[1, 1, 1, 1, 1], padding=\"SAME\")\n",
    "    print('conv_1 shape: %s' %conv_1.shape)\n",
    "    conv_1 = conv_1 + bias_1\n",
    "\n",
    "    if add_relu:\n",
    "        conv_1 = tf.nn.elu(conv_1)\n",
    "\n",
    "    pooling_1 = tf.nn.max_pool3d(\n",
    "        conv_1, \n",
    "        ksize=[1, 1, 3, 3, 1],\n",
    "        strides=[1, 1, 3, 3, 1], \n",
    "        padding=\"SAME\")\n",
    "    print('pooling_1 shape: %s' %pooling_1.shape)\n",
    "\n",
    "    return tf.contrib.layers.flatten(pooling_1)\n",
    "\n",
    "def CNN_3d_change(x, out_channels_0, out_channels_1, add_relu=True):\n",
    "    '''Add a 3d convlution layer with relu and max pooling layer.\n",
    "\n",
    "    Args:\n",
    "        x: a tensor with shape [batch, in_depth, in_height, in_width, in_channels]\n",
    "        out_channels: a number\n",
    "        filter_size: a number\n",
    "        pooling_size: a number\n",
    "\n",
    "    Returns:\n",
    "        a flattened tensor with shape [batch, num_features]\n",
    "\n",
    "    Raises:\n",
    "    '''\n",
    "    in_channels = x.shape[-1]\n",
    "    weights_0 = tf.get_variable(\n",
    "        name='filter_0',\n",
    "        shape=[3, 3, 3, in_channels, out_channels_0],\n",
    "        dtype=tf.float32,\n",
    "        #initializer=tf.random_normal_initializer(0, 0.05))\n",
    "        initializer=tf.random_uniform_initializer(-0.01, 0.01))\n",
    "    bias_0 = tf.get_variable(\n",
    "        name='bias_0',\n",
    "        shape=[out_channels_0],\n",
    "        dtype=tf.float32,\n",
    "        initializer=tf.zeros_initializer())\n",
    "    #Todo\n",
    "    g_0 = tf.get_variable(name='scale_0',\n",
    "        shape = [out_channels_0],\n",
    "        dtype=tf.float32,\n",
    "        initializer=tf.ones_initializer())\n",
    "    weights_0 = tf.reshape(g_0, [1, 1, 1, out_channels_0]) * tf.nn.l2_normalize(weights_0, [0, 1, 2])\n",
    "\n",
    "    conv_0 = tf.nn.conv3d(x, weights_0, strides=[1, 1, 1, 1, 1], padding=\"VALID\")\n",
    "    print('conv_0 shape: %s' %conv_0.shape)\n",
    "    conv_0 = conv_0 + bias_0\n",
    "    #######\n",
    "    '''\n",
    "    with tf.variable_scope('layer_0'):\n",
    "        conv_0 = layer_norm(conv_0, axis=[1, 2, 3, 4])\n",
    "        print('layer_norm in cnn')\n",
    "    '''\n",
    "    if add_relu:\n",
    "        conv_0 = tf.nn.elu(conv_0)\n",
    "\n",
    "    pooling_0 = tf.nn.max_pool3d(\n",
    "        conv_0, \n",
    "        ksize=[1, 2, 3, 3, 1],\n",
    "        strides=[1, 2, 3, 3, 1], \n",
    "        padding=\"VALID\")\n",
    "    print('pooling_0 shape: %s' %pooling_0.shape)\n",
    "\n",
    "    #layer_1\n",
    "    weights_1 = tf.get_variable(\n",
    "        name='filter_1',\n",
    "        shape=[2, 2, 2, out_channels_0, out_channels_1],\n",
    "        dtype=tf.float32,\n",
    "        initializer=tf.random_uniform_initializer(-0.01, 0.01))\n",
    "    \n",
    "    bias_1 = tf.get_variable(\n",
    "        name='bias_1',\n",
    "        shape=[out_channels_1],\n",
    "        dtype=tf.float32,\n",
    "        initializer=tf.zeros_initializer())\n",
    "    \n",
    "    g_1 = tf.get_variable(name='scale_1',\n",
    "        shape = [out_channels_1],\n",
    "        dtype=tf.float32,\n",
    "        initializer=tf.ones_initializer())\n",
    "    weights_1 = tf.reshape(g_1, [1, 1, 1, out_channels_1]) * tf.nn.l2_normalize(weights_1, [0, 1, 2])\n",
    "\n",
    "    conv_1 = tf.nn.conv3d(pooling_0, weights_1, strides=[1, 1, 1, 1, 1], padding=\"VALID\")\n",
    "    print('conv_1 shape: %s' %conv_1.shape)\n",
    "    conv_1 = conv_1 + bias_1\n",
    "    #with tf.variable_scope('layer_1'):\n",
    "    #    conv_1 = layer_norm(conv_1, axis=[1, 2, 3, 4])\n",
    "\n",
    "    if add_relu:\n",
    "        conv_1 = tf.nn.elu(conv_1)\n",
    "\n",
    "    pooling_1 = tf.nn.max_pool3d(\n",
    "        conv_1, \n",
    "        ksize=[1, 3, 3, 3, 1],\n",
    "        strides=[1, 3, 3, 3, 1], \n",
    "        padding=\"VALID\")\n",
    "    print('pooling_1 shape: %s' %pooling_1.shape)\n",
    "\n",
    "    return tf.contrib.layers.flatten(pooling_1)\n",
    "\n",
    "def RNN_last_state(x, lengths, hidden_size):\n",
    "    '''encode x with a gru cell and return the last state.\n",
    "    \n",
    "    Args:\n",
    "        x: a tensor with shape [batch, time, dimension]\n",
    "        length: a tensor with shape [batch]\n",
    "\n",
    "    Return:\n",
    "        a tensor with shape [batch, hidden_size]\n",
    "\n",
    "    Raises:\n",
    "    '''\n",
    "    cell = tf.nn.rnn_cell.GRUCell(hidden_size)\n",
    "    outputs, last_states = tf.nn.dynamic_rnn(cell, x, lengths, dtype=tf.float32)\n",
    "    return outputs, last_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XiLeGM56tqq0"
   },
   "outputs": [],
   "source": [
    "##train_and_evaluate \n",
    "\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "#import utils.reader as reader\n",
    "#import utils.evaluation as eva\n",
    "\n",
    "\n",
    "def train(conf, _model):\n",
    "    \n",
    "    if conf['rand_seed'] is not None:\n",
    "        np.random.seed(conf['rand_seed'])\n",
    "\n",
    "    if not os.path.exists(conf['save_path']):\n",
    "        os.makedirs(conf['save_path'])\n",
    "\n",
    "    # load data\n",
    "    print('starting loading data')\n",
    "    print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())))\n",
    "    files = 'data3_tn.pkl'    \n",
    "    with open(files, mode='rb') as f:\n",
    "        train_data, val_data, test_data = pickle.load(f)    \n",
    "    print('finish loading data')\n",
    "    print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())))\n",
    "    \n",
    "    \n",
    "    print('starting building validation batches')\n",
    "    val_batches = build_batches(val_data, conf)\n",
    "    print(\"finish building validation batches\")\n",
    "    print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())))\n",
    "\n",
    "    # refine conf\n",
    "    batch_num = len(train_data['y']) // conf[\"batch_size\"]\n",
    "    val_batch_num = len(val_batches[\"response\"])\n",
    "\n",
    "    conf[\"train_steps\"] = conf[\"num_scan_data\"] * batch_num\n",
    "    conf[\"save_step\"] = max(1, batch_num / 10)\n",
    "    conf[\"print_step\"] = max(1, batch_num / 100)\n",
    "    print('configurations: %s' %conf)\n",
    "\n",
    "    print('model sucess')\n",
    "    print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())))\n",
    "\n",
    "    _graph = _model.build_graph()\n",
    "    print('build graph sucess')\n",
    "    print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())))\n",
    "\n",
    "    with tf.Session(graph=_graph) as sess:\n",
    "        _model.init.run();\n",
    "        if conf[\"init_model\"]:\n",
    "            _model.saver.restore(sess, conf[\"init_model\"])\n",
    "            print(\"sucess init %s\" %conf[\"init_model\"])\n",
    "\n",
    "        average_loss = 0.0\n",
    "        batch_index = 0\n",
    "        step = 0\n",
    "        best_result = [0, 0, 0, 0]\n",
    "\n",
    "        for step_i in range(conf[\"num_scan_data\"]):\n",
    "            #for batch_index in rng.permutation(range(batch_num)):\n",
    "            print('starting shuffle train data')\n",
    "            shuffle_train = unison_shuffle(train_data)\n",
    "            train_batches = build_batches(shuffle_train, conf)\n",
    "            print('finish building train data')\n",
    "            for batch_index in range(batch_num):\n",
    "                feed = {\n",
    "                    _model.turns: train_batches[\"turns\"][batch_index], \n",
    "                    _model.tt_turns_len: train_batches[\"tt_turns_len\"][batch_index],\n",
    "                    _model.every_turn_len: train_batches[\"every_turn_len\"][batch_index],\n",
    "                    _model.response: train_batches[\"response\"][batch_index], \n",
    "                    _model.response_len: train_batches[\"response_len\"][batch_index],\n",
    "                    _model.label: train_batches[\"label\"][batch_index]\n",
    "                }\n",
    "\n",
    "                batch_index = (batch_index + 1) % batch_num;\n",
    "\n",
    "                _, curr_loss = sess.run([_model.g_updates, _model.loss], feed_dict = feed)\n",
    "\n",
    "                \n",
    "                average_loss += curr_loss\n",
    "                \n",
    "                step += 1\n",
    "                #print(\"Saving steps???\", step % conf[\"print_step\"] == 0)\n",
    "                \n",
    "                if step % conf[\"print_step\"] == 0 and step > 0:\n",
    "                    g_step, lr = sess.run([_model.global_step, _model.learning_rate])\n",
    "                    print('step: %s, lr: %s' %(g_step, lr))\n",
    "                    print(\"processed: [\" + str(step * 1.0 / batch_num) + \"] loss: [\" + str(average_loss / conf[\"print_step\"]) + \"]\" )\n",
    "                    average_loss = 0\n",
    "\n",
    "                #print(\"Saving steps???\", step % conf[\"save_step\"] == 0)\n",
    "                if step % conf[\"save_step\"] == 0 and step > 0:\n",
    "                    index = step // conf['save_step']\n",
    "                    score_file_path = conf['save_path'] + 'score.' + str(index)\n",
    "                    score_file = open(score_file_path, 'w')\n",
    "                    print('save step: %s' %index)\n",
    "                    print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())))\n",
    "\n",
    "                    for batch_index in range(val_batch_num):\n",
    "                \n",
    "                        feed = { \n",
    "                            _model.turns: val_batches[\"turns\"][batch_index],\n",
    "                            _model.tt_turns_len: val_batches[\"tt_turns_len\"][batch_index],\n",
    "                            _model.every_turn_len: val_batches[\"every_turn_len\"][batch_index],\n",
    "                            _model.response: val_batches[\"response\"][batch_index],\n",
    "                            _model.response_len: val_batches[\"response_len\"][batch_index],\n",
    "                            _model.label: val_batches[\"label\"][batch_index]\n",
    "                        }   \n",
    "                \n",
    "                        scores = sess.run(_model.logits, feed_dict = feed)\n",
    "                    \n",
    "                        for i in range(conf[\"batch_size\"]):\n",
    "                            score_file.write(\n",
    "                                str(scores[i]) + '\\t' + \n",
    "                                str(val_batches[\"label\"][batch_index][i]) + '\\n')\n",
    "                    score_file.close()\n",
    "\n",
    "                    #write evaluation result\n",
    "                    result = evaluate(score_file_path)\n",
    "                    result_file_path = conf[\"save_path\"] + \"result.\" + str(index)\n",
    "                    with open(result_file_path, 'w') as out_file:\n",
    "                        for p_at in result:\n",
    "                            out_file.write(str(p_at) + '\\n')\n",
    "                    print('result = ', result)\n",
    "                    print('finish evaluation')\n",
    "                    print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())))\n",
    "                    \n",
    "                    if result[1] + result[2] > best_result[1] + best_result[2]:\n",
    "                        best_result = result\n",
    "                        _save_path = _model.saver.save(sess, conf[\"save_path\"] + \"model.ckpt.\" + str(step / conf[\"save_step\"]))\n",
    "                        print(\"succ saving model in \" + _save_path)\n",
    "                        print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1WcJ4ZF3tt5t"
   },
   "outputs": [],
   "source": [
    "## test_and_evaluate\n",
    "\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "#import utils.reader as reader\n",
    "#import utils.evaluation as eva\n",
    "\n",
    "\n",
    "def test(conf, _model):\n",
    "    \n",
    "    if not os.path.exists(conf['save_path']):\n",
    "        os.makedirs(conf['save_path'])\n",
    "\n",
    "    # load data\n",
    "    print('starting loading data')\n",
    "    print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())))\n",
    "    files = 'data3_tn.pkl'  \n",
    "    with open(files, mode='rb') as f:\n",
    "        train_data, val_data, test_data = pickle.load(f)    \n",
    "    print('finish loading data')\n",
    "    print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())))\n",
    "    \n",
    "    \n",
    "    print('starting building validation batches')\n",
    "    test_batches = build_batches(test_data, conf)\n",
    "    print(\"finish building validation batches\")\n",
    "    print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())))\n",
    "\n",
    "\n",
    "    # refine conf\n",
    "    test_batch_num = len(test_batches[\"response\"])\n",
    "\n",
    "    print('configurations: %s' %conf)\n",
    "\n",
    "\n",
    "    _graph = _model.build_graph()\n",
    "    print('build graph sucess')\n",
    "    print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())))\n",
    "\n",
    "    with tf.Session(graph=_graph) as sess:\n",
    "        _model.init.run()\n",
    "        _model.saver.restore(sess, conf[\"init_model\"])\n",
    "        print(\"sucess init %s\" %conf[\"init_model\"])\n",
    "\n",
    "        batch_index = 0\n",
    "        step = 0\n",
    "\n",
    "        score_file_path = conf['save_path'] + 'score.test'\n",
    "        score_file = open(score_file_path, 'w')\n",
    "\n",
    "        print('starting test')\n",
    "        print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())))\n",
    "        for batch_index in range(test_batch_num):\n",
    "                \n",
    "            feed = { \n",
    "                _model.turns: test_batches[\"turns\"][batch_index],\n",
    "                _model.tt_turns_len: test_batches[\"tt_turns_len\"][batch_index],\n",
    "                _model.every_turn_len: test_batches[\"every_turn_len\"][batch_index],\n",
    "                _model.response: test_batches[\"response\"][batch_index],\n",
    "                _model.response_len: test_batches[\"response_len\"][batch_index],\n",
    "                _model.label: test_batches[\"label\"][batch_index]\n",
    "                }   \n",
    "                \n",
    "            scores = sess.run(_model.logits, feed_dict = feed)\n",
    "                    \n",
    "            for i in range(conf[\"batch_size\"]):\n",
    "                score_file.write(\n",
    "                    str(scores[i]) + '\\t' + \n",
    "                    str(test_batches[\"label\"][batch_index][i]) + '\\n')\n",
    "                    #str(sum(test_batches[\"every_turn_len\"][batch_index][i]) / test_batches['tt_turns_len'][batch_index][i]) + '\\t' +\n",
    "                    #str(test_batches['tt_turns_len'][batch_index][i]) + '\\n') \n",
    "\n",
    "        score_file.close()\n",
    "        print('finish test')\n",
    "        print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())))\n",
    "\n",
    "        \n",
    "        #write evaluation result\n",
    "        result = evaluate(score_file_path)\n",
    "        result_file_path = conf[\"save_path\"] + \"result.test\"\n",
    "        with open(result_file_path, 'w') as out_file:\n",
    "            for p_at in result:\n",
    "                out_file.write(str(p_at) + '\\n')\n",
    "        print('finish evaluation')\n",
    "        print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D8O_13TduGJ2"
   },
   "outputs": [],
   "source": [
    "##net.py\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "#import utils.layers as layers\n",
    "#import utils.operations as op\n",
    "\n",
    "class Net(object):\n",
    "    '''Add positional encoding(initializer lambda is 0),\n",
    "       cross-attention, cnn integrated and grad clip by value.\n",
    "\n",
    "    Attributes:\n",
    "        conf: a configuration paramaters dict\n",
    "        word_embedding_init: a 2-d array with shape [vocab_size+1, emb_size]\n",
    "    '''\n",
    "    def __init__(self, conf):\n",
    "        self._graph = tf.Graph()\n",
    "        self._conf = conf\n",
    "\n",
    "        if self._conf['word_emb_init'] is not None:\n",
    "            print('loading word emb init')\n",
    "            self._word_embedding_init = pickle.load(open(self._conf['word_emb_init'], 'rb'), encoding='latin1')\n",
    "            #self._word_embedding_init = Word2Vec.load(open(self._conf['word_emb_init'], 'rb'))\n",
    "        else:\n",
    "            print('word emb init was set to None')\n",
    "            self._word_embedding_init = None\n",
    "\n",
    "    def build_graph(self):\n",
    "        with self._graph.as_default():\n",
    "            if self._conf['rand_seed'] is not None:\n",
    "                rand_seed = self._conf['rand_seed']\n",
    "                tf.set_random_seed(rand_seed)\n",
    "                print('set tf random seed: %s' %self._conf['rand_seed'])\n",
    "\n",
    "            #word embedding\n",
    "            if self._word_embedding_init is not None:\n",
    "                word_embedding_initializer = tf.constant_initializer(self._word_embedding_init)\n",
    "            else:\n",
    "                word_embedding_initializer = tf.random_normal_initializer(stddev=0.1)\n",
    "\n",
    "            self._word_embedding = tf.get_variable(\n",
    "                name='word_embedding',\n",
    "                shape=[self._conf['vocab_size']+1, self._conf['emb_size']],\n",
    "                dtype=tf.float32,\n",
    "                initializer=word_embedding_initializer)\n",
    "\n",
    "\n",
    "            #define placehloders\n",
    "            self.turns = tf.placeholder(\n",
    "                tf.int32,\n",
    "                shape=[self._conf[\"batch_size\"], self._conf[\"max_turn_num\"], self._conf[\"max_turn_len\"]])\n",
    "\n",
    "            self.tt_turns_len = tf.placeholder(\n",
    "                tf.int32,\n",
    "                shape=[self._conf[\"batch_size\"]])\n",
    "\n",
    "            self.every_turn_len = tf.placeholder(\n",
    "                tf.int32,\n",
    "                shape=[self._conf[\"batch_size\"], self._conf[\"max_turn_num\"]])\n",
    "    \n",
    "            self.response = tf.placeholder(\n",
    "                tf.int32, \n",
    "                shape=[self._conf[\"batch_size\"], self._conf[\"max_turn_len\"]])\n",
    "\n",
    "            self.response_len = tf.placeholder(\n",
    "                tf.int32, \n",
    "                shape=[self._conf[\"batch_size\"]])\n",
    "\n",
    "            self.label = tf.placeholder(\n",
    "                tf.float32, \n",
    "                shape=[self._conf[\"batch_size\"]])\n",
    "\n",
    "\n",
    "            #define operations\n",
    "            #response part\n",
    "            Hr = tf.nn.embedding_lookup(self._word_embedding, self.response)\n",
    "\n",
    "            if self._conf['is_positional'] and self._conf['stack_num'] > 0:\n",
    "                with tf.variable_scope('positional'):\n",
    "                    Hr = positional_encoding_vector(Hr, max_timescale=10)\n",
    "            Hr_stack = [Hr]\n",
    "\n",
    "            for index in range(self._conf['stack_num']):\n",
    "                with tf.variable_scope('self_stack_' + str(index)):\n",
    "                    Hr = block(\n",
    "                        Hr, Hr, Hr, \n",
    "                        Q_lengths=self.response_len, K_lengths=self.response_len)\n",
    "                    Hr_stack.append(Hr)\n",
    "\n",
    "\n",
    "            #context part\n",
    "            #a list of length max_turn_num, every element is a tensor with shape [batch, max_turn_len]\n",
    "            list_turn_t = tf.unstack(self.turns, axis=1) \n",
    "            list_turn_length = tf.unstack(self.every_turn_len, axis=1)\n",
    "            \n",
    "            sim_turns = []\n",
    "            #for every turn_t calculate matching vector\n",
    "            for turn_t, t_turn_length in zip(list_turn_t, list_turn_length):\n",
    "                Hu = tf.nn.embedding_lookup(self._word_embedding, turn_t) #[batch, max_turn_len, emb_size]\n",
    "\n",
    "                if self._conf['is_positional'] and self._conf['stack_num'] > 0:\n",
    "                    with tf.variable_scope('positional', reuse=True):\n",
    "                        Hu = positional_encoding_vector(Hu, max_timescale=10)\n",
    "                Hu_stack = [Hu]\n",
    "\n",
    "                for index in range(self._conf['stack_num']):\n",
    "\n",
    "                    with tf.variable_scope('self_stack_' + str(index), reuse=True):\n",
    "                        Hu = block(\n",
    "                            Hu, Hu, Hu,\n",
    "                            Q_lengths=t_turn_length, K_lengths=t_turn_length)\n",
    "\n",
    "                        Hu_stack.append(Hu)\n",
    "\n",
    "\n",
    "\n",
    "                r_a_t_stack = []\n",
    "                t_a_r_stack = []\n",
    "                for index in range(self._conf['stack_num']+1):\n",
    "\n",
    "                    with tf.variable_scope('t_attend_r_' + str(index)):\n",
    "                        try:\n",
    "                            t_a_r = block(\n",
    "                                Hu_stack[index], Hr_stack[index], Hr_stack[index],\n",
    "                                Q_lengths=t_turn_length, K_lengths=self.response_len)\n",
    "                        except ValueError:\n",
    "                            tf.get_variable_scope().reuse_variables()\n",
    "                            t_a_r = block(\n",
    "                                Hu_stack[index], Hr_stack[index], Hr_stack[index],\n",
    "                                Q_lengths=t_turn_length, K_lengths=self.response_len)\n",
    "\n",
    "\n",
    "                    with tf.variable_scope('r_attend_t_' + str(index)):\n",
    "                        try:\n",
    "                            r_a_t = block(\n",
    "                                Hr_stack[index], Hu_stack[index], Hu_stack[index],\n",
    "                                Q_lengths=self.response_len, K_lengths=t_turn_length)\n",
    "                        except ValueError:\n",
    "                            tf.get_variable_scope().reuse_variables()\n",
    "                            r_a_t = block(\n",
    "                                Hr_stack[index], Hu_stack[index], Hu_stack[index],\n",
    "                                Q_lengths=self.response_len, K_lengths=t_turn_length)\n",
    "\n",
    "                    t_a_r_stack.append(t_a_r)\n",
    "                    r_a_t_stack.append(r_a_t)\n",
    "\n",
    "                t_a_r_stack.extend(Hu_stack)\n",
    "                r_a_t_stack.extend(Hr_stack)\n",
    "                \n",
    "                t_a_r = tf.stack(t_a_r_stack, axis=-1)\n",
    "                r_a_t = tf.stack(r_a_t_stack, axis=-1)\n",
    "\n",
    "                            \n",
    "                #calculate similarity matrix\n",
    "                with tf.variable_scope('similarity'):\n",
    "                    # sim shape [batch, max_turn_len, max_turn_len, 2*stack_num+1]\n",
    "                    # divide sqrt(200) to prevent gradient explosion\n",
    "                    sim = tf.einsum('biks,bjks->bijs', t_a_r, r_a_t) / tf.sqrt(200.0)\n",
    "\n",
    "                sim_turns.append(sim)\n",
    "\n",
    "\n",
    "            #cnn and aggregation\n",
    "            sim = tf.stack(sim_turns, axis=1)\n",
    "            print('sim shape: %s' %sim.shape)\n",
    "            with tf.variable_scope('cnn_aggregation'):\n",
    "                final_info = CNN_3d(sim, 32, 16)\n",
    "                #for douban\n",
    "                #final_info = CNN_3d(sim, 16, 16)\n",
    "\n",
    "            #loss and train\n",
    "            with tf.variable_scope('loss'):\n",
    "                self.loss, self.logits = loss(final_info, self.label)\n",
    "\n",
    "                self.global_step = tf.Variable(0, trainable=False)\n",
    "                initial_learning_rate = self._conf['learning_rate']\n",
    "                self.learning_rate = tf.train.exponential_decay(\n",
    "                    initial_learning_rate,\n",
    "                    global_step=self.global_step,\n",
    "                    decay_steps=400,\n",
    "                    decay_rate=0.9,\n",
    "                    staircase=True)\n",
    "\n",
    "                Optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "                self.optimizer = Optimizer.minimize(\n",
    "                    self.loss,\n",
    "                    global_step=self.global_step)\n",
    "\n",
    "                self.init = tf.global_variables_initializer()\n",
    "                self.saver = tf.train.Saver(max_to_keep = self._conf[\"max_to_keep\"])\n",
    "                self.all_variables = tf.global_variables() \n",
    "                self.all_operations = self._graph.get_operations()\n",
    "                self.grads_and_vars = Optimizer.compute_gradients(self.loss)\n",
    "\n",
    "                for grad, var in self.grads_and_vars:\n",
    "                    if grad is None:\n",
    "                        print (var)\n",
    "\n",
    "                self.capped_gvs = [(tf.clip_by_value(grad, -1, 1), var) for grad, var in self.grads_and_vars]\n",
    "                self.g_updates = Optimizer.apply_gradients(\n",
    "                    self.capped_gvs,\n",
    "                    global_step=self.global_step)\n",
    "    \n",
    "        return self._graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qC6051ypvXoe"
   },
   "source": [
    "## Training & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yc5nt2ZfuJLP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word emb init\n",
      "starting loading data\n",
      "2019-05-31 11:57:00\n",
      "finish loading data\n",
      "2019-05-31 11:57:00\n",
      "starting building validation batches\n",
      "finish building validation batches\n",
      "2019-05-31 11:57:01\n",
      "configurations: {'is_mask': True, 'save_path': './output_data_tn_final/TunisianDailect/temp/', 'attention_type': 'dot', 'max_turn_len': 35, 'final_n_class': 1, 'is_positional': False, 'max_turn_num': 2, 'save_step': 52.5, 'num_scan_data': 2, 'data_path': './data_tn.pkl', 'word_emb_init': './embedding_tn.pkl', 'drop_attention': None, 'drop_dense': None, 'is_layer_norm': True, 'init_model': None, '_EOS_': 1000000, 'learning_rate': 0.001, 'stack_num': 6, 'batch_size': 32, 'rand_seed': None, 'max_to_keep': 1, 'vocab_size': 36506, 'train_steps': 1050, 'print_step': 5.25, 'emb_size': 200}\n",
      "model sucess\n",
      "2019-05-31 11:57:01\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "sim shape: (32, 2, 35, 35, 14)\n",
      "conv_0 shape: (32, 2, 35, 35, 32)\n",
      "pooling_0 shape: (32, 1, 12, 12, 32)\n",
      "conv_1 shape: (32, 1, 12, 12, 16)\n",
      "pooling_1 shape: (32, 1, 4, 4, 16)\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/layers/python/layers/layers.py:1624: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "build graph sucess\n",
      "2019-05-31 11:57:24\n",
      "starting shuffle train data\n",
      "finish building train data\n",
      "step: 21, lr: 0.001\n",
      "processed: [0.04] loss: [2.689485799698603]\n",
      "step: 42, lr: 0.001\n",
      "processed: [0.08] loss: [2.293461873417809]\n",
      "step: 63, lr: 0.001\n",
      "processed: [0.12] loss: [2.2029484737487066]\n",
      "step: 84, lr: 0.001\n",
      "processed: [0.16] loss: [2.091901427223569]\n",
      "step: 105, lr: 0.001\n",
      "processed: [0.2] loss: [2.081429640452067]\n",
      "save step: 2.0\n",
      "2019-05-31 11:57:40\n",
      "result =  (0.8487208008898777, 0.44549499443826474, 0.6590656284760845, 0.9193548387096774)\n",
      "finish evaluation\n",
      "2019-05-31 11:57:50\n",
      "succ saving model in ./output_data_tn_final/TunisianDailect/temp/model.ckpt.2.0\n",
      "2019-05-31 11:57:53\n",
      "step: 126, lr: 0.001\n",
      "processed: [0.24] loss: [1.9214640969321841]\n",
      "step: 147, lr: 0.001\n",
      "processed: [0.28] loss: [1.9254071769260226]\n",
      "step: 168, lr: 0.001\n",
      "processed: [0.32] loss: [2.0532865126927695]\n",
      "step: 189, lr: 0.001\n",
      "processed: [0.36] loss: [1.8577304851441157]\n",
      "step: 210, lr: 0.001\n",
      "processed: [0.4] loss: [1.9953430550439017]\n",
      "save step: 4.0\n",
      "2019-05-31 11:57:58\n",
      "result =  (0.864293659621802, 0.5255839822024472, 0.7219132369299222, 0.9327030033370411)\n",
      "finish evaluation\n",
      "2019-05-31 11:58:07\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "succ saving model in ./output_data_tn_final/TunisianDailect/temp/model.ckpt.4.0\n",
      "2019-05-31 11:58:09\n",
      "step: 231, lr: 0.001\n",
      "processed: [0.44] loss: [1.8154362780707223]\n",
      "step: 252, lr: 0.001\n",
      "processed: [0.48] loss: [1.7855431579408192]\n",
      "step: 273, lr: 0.001\n",
      "processed: [0.52] loss: [1.6875871079308646]\n",
      "step: 294, lr: 0.001\n",
      "processed: [0.56] loss: [1.7388467334565663]\n",
      "step: 315, lr: 0.001\n",
      "processed: [0.6] loss: [1.4292859491847811]\n",
      "save step: 6.0\n",
      "2019-05-31 11:58:14\n",
      "result =  (0.9065628476084538, 0.585650723025584, 0.778642936596218, 0.9566184649610678)\n",
      "finish evaluation\n",
      "2019-05-31 11:58:23\n",
      "succ saving model in ./output_data_tn_final/TunisianDailect/temp/model.ckpt.6.0\n",
      "2019-05-31 11:58:25\n",
      "step: 336, lr: 0.001\n",
      "processed: [0.64] loss: [1.728546795390901]\n",
      "step: 357, lr: 0.001\n",
      "processed: [0.68] loss: [1.624770340465364]\n",
      "step: 378, lr: 0.001\n",
      "processed: [0.72] loss: [1.6378523877688818]\n",
      "step: 399, lr: 0.001\n",
      "processed: [0.76] loss: [1.4397394486836024]\n",
      "step: 420, lr: 0.00090000004\n",
      "processed: [0.8] loss: [1.4594043322971888]\n",
      "save step: 8.0\n",
      "2019-05-31 11:58:30\n",
      "result =  (0.8976640711902113, 0.6006674082313682, 0.7903225806451613, 0.9477196885428254)\n",
      "finish evaluation\n",
      "2019-05-31 11:58:39\n",
      "succ saving model in ./output_data_tn_final/TunisianDailect/temp/model.ckpt.8.0\n",
      "2019-05-31 11:58:41\n",
      "step: 441, lr: 0.00090000004\n",
      "processed: [0.84] loss: [1.445789172535851]\n",
      "step: 462, lr: 0.00090000004\n",
      "processed: [0.88] loss: [1.3637910996164595]\n",
      "step: 483, lr: 0.00090000004\n",
      "processed: [0.92] loss: [1.612571963242122]\n",
      "step: 504, lr: 0.00090000004\n",
      "processed: [0.96] loss: [1.2658481243110837]\n",
      "step: 525, lr: 0.00090000004\n",
      "processed: [1.0] loss: [1.3743059791269756]\n",
      "save step: 10.0\n",
      "2019-05-31 11:58:45\n",
      "result =  (0.9010011123470523, 0.5962180200222469, 0.7825361512791991, 0.9555061179087876)\n",
      "finish evaluation\n",
      "2019-05-31 11:58:54\n",
      "starting shuffle train data\n",
      "finish building train data\n",
      "step: 546, lr: 0.00090000004\n",
      "processed: [1.04] loss: [0.8021203116292045]\n",
      "step: 567, lr: 0.00090000004\n",
      "processed: [1.08] loss: [0.6867582159382957]\n",
      "step: 588, lr: 0.00090000004\n",
      "processed: [1.12] loss: [0.7348907675061908]\n",
      "step: 609, lr: 0.00090000004\n",
      "processed: [1.16] loss: [0.8410643346252895]\n",
      "step: 630, lr: 0.00090000004\n",
      "processed: [1.2] loss: [0.7526529289427257]\n",
      "save step: 12.0\n",
      "2019-05-31 11:59:00\n",
      "result =  (0.9076751946607341, 0.6201334816462737, 0.8031145717463849, 0.9549499443826474)\n",
      "finish evaluation\n",
      "2019-05-31 11:59:08\n",
      "succ saving model in ./output_data_tn_final/TunisianDailect/temp/model.ckpt.12.0\n",
      "2019-05-31 11:59:11\n",
      "step: 651, lr: 0.00090000004\n",
      "processed: [1.24] loss: [0.714429738620917]\n",
      "step: 672, lr: 0.00090000004\n",
      "processed: [1.28] loss: [0.8346564216273171]\n",
      "step: 693, lr: 0.00090000004\n",
      "processed: [1.32] loss: [0.9397971246923719]\n",
      "step: 714, lr: 0.00090000004\n",
      "processed: [1.36] loss: [0.6822494594823747]\n",
      "step: 735, lr: 0.00090000004\n",
      "processed: [1.4] loss: [0.7494287235396249]\n",
      "save step: 14.0\n",
      "2019-05-31 11:59:15\n",
      "result =  (0.8959955506117909, 0.5867630700778643, 0.78587319243604, 0.9510567296996663)\n",
      "finish evaluation\n",
      "2019-05-31 11:59:24\n",
      "step: 756, lr: 0.00090000004\n",
      "processed: [1.44] loss: [0.777479601757867]\n",
      "step: 777, lr: 0.00090000004\n",
      "processed: [1.48] loss: [0.5763343622287115]\n",
      "step: 798, lr: 0.00090000004\n",
      "processed: [1.52] loss: [0.6643633523157665]\n",
      "step: 819, lr: 0.00081\n",
      "processed: [1.56] loss: [0.7179735700289408]\n",
      "step: 840, lr: 0.00081\n",
      "processed: [1.6] loss: [0.720368545679819]\n",
      "save step: 16.0\n",
      "2019-05-31 11:59:29\n",
      "result =  (0.9026696329254728, 0.5917686318131257, 0.7903225806451613, 0.9588431590656284)\n",
      "finish evaluation\n",
      "2019-05-31 11:59:38\n",
      "step: 861, lr: 0.00081\n",
      "processed: [1.64] loss: [0.7520370951720646]\n",
      "step: 882, lr: 0.00081\n",
      "processed: [1.68] loss: [0.6824538977373213]\n",
      "step: 903, lr: 0.00081\n",
      "processed: [1.72] loss: [0.49966307552087874]\n",
      "step: 924, lr: 0.00081\n",
      "processed: [1.76] loss: [0.5524444537503379]\n",
      "step: 945, lr: 0.00081\n",
      "processed: [1.8] loss: [0.7295662249837603]\n",
      "save step: 18.0\n",
      "2019-05-31 11:59:43\n",
      "result =  (0.9087875417130145, 0.6173526140155728, 0.7986651835372637, 0.9532814238042269)\n",
      "finish evaluation\n",
      "2019-05-31 11:59:52\n",
      "step: 966, lr: 0.00081\n",
      "processed: [1.84] loss: [0.5954654543172746]\n",
      "step: 987, lr: 0.00081\n",
      "processed: [1.88] loss: [0.5650723817802611]\n",
      "step: 1008, lr: 0.00081\n",
      "processed: [1.92] loss: [0.6827241749990554]\n",
      "step: 1029, lr: 0.00081\n",
      "processed: [1.96] loss: [0.6119330929858344]\n",
      "step: 1050, lr: 0.00081\n",
      "processed: [2.0] loss: [0.4478862966809954]\n",
      "save step: 20.0\n",
      "2019-05-31 11:59:57\n",
      "result =  (0.9026696329254728, 0.6190211345939933, 0.7969966629588432, 0.9505005561735261)\n",
      "finish evaluation\n",
      "2019-05-31 12:00:06\n"
     ]
    }
   ],
   "source": [
    "#main.py for train\n",
    "\n",
    "#import models.net as net\n",
    "#import bin.train_and_evaluate as train\n",
    "#import bin.test_and_evaluate as test\n",
    "\n",
    "# configure\n",
    "\n",
    "conf = {\n",
    "    \"data_path\": \"./data_tn.pkl\",\n",
    "    \"save_path\": \"./output_data_tn_final/TunisianDailect/temp/\",\n",
    "#    \"word_emb_init\": None,\n",
    "    \"word_emb_init\": \"./embedding_tn.pkl\",\n",
    "    \"init_model\": None, #should be set for test\n",
    "\n",
    "    \"rand_seed\": None, \n",
    "\n",
    "    \"drop_dense\": None,\n",
    "    \"drop_attention\": None,\n",
    "\n",
    "    \"is_mask\": True,\n",
    "    \"is_layer_norm\": True,\n",
    "    \"is_positional\": False,  \n",
    "\n",
    "    \"stack_num\": 6,  \n",
    "    \"attention_type\": \"dot\",\n",
    "\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"vocab_size\": 36506,\n",
    "    \"emb_size\": 200,\n",
    "    \"batch_size\": 32,     #200 for test\n",
    "\n",
    "    \"max_turn_num\": 2,  \n",
    "    \"max_turn_len\": 35, \n",
    "\n",
    "    \"max_to_keep\": 1,\n",
    "    \"num_scan_data\": 2,\n",
    "    \"_EOS_\": 1000000,      #1 for douban data\n",
    "    \"final_n_class\": 1,\n",
    "}\n",
    "\n",
    "\n",
    "model_tn = Net(conf)\n",
    "train(conf, model_tn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Po0B5dKJvZro"
   },
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7wysy1-DuM0K"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word emb init\n",
      "starting loading data\n",
      "2019-05-31 12:03:00\n",
      "finish loading data\n",
      "2019-05-31 12:03:00\n",
      "starting building validation batches\n",
      "finish building validation batches\n",
      "2019-05-31 12:03:00\n",
      "configurations: {'is_mask': True, 'save_path': './output_data_tn_final/TunisianDailect/temp/', 'attention_type': 'dot', 'max_turn_len': 35, 'final_n_class': 1, 'is_positional': False, 'max_turn_num': 2, 'num_scan_data': 2, 'data_path': './data_tn.pkl', 'word_emb_init': './embedding_tn.pkl', 'drop_attention': None, 'drop_dense': None, 'is_layer_norm': True, 'init_model': './output_data_tn_final/TunisianDailect/temp/model.ckpt.12.0', '_EOS_': 1000000, 'learning_rate': 0.001, 'stack_num': 6, 'batch_size': 32, 'rand_seed': None, 'max_to_keep': 1, 'vocab_size': 36506, 'emb_size': 200}\n",
      "sim shape: (32, 2, 35, 35, 14)\n",
      "conv_0 shape: (32, 2, 35, 35, 32)\n",
      "pooling_0 shape: (32, 1, 12, 12, 32)\n",
      "conv_1 shape: (32, 1, 12, 12, 16)\n",
      "pooling_1 shape: (32, 1, 4, 4, 16)\n",
      "build graph sucess\n",
      "2019-05-31 12:03:25\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from ./output_data_tn_final/TunisianDailect/temp/model.ckpt.12.0\n",
      "sucess init ./output_data_tn_final/TunisianDailect/temp/model.ckpt.12.0\n",
      "starting test\n",
      "2019-05-31 12:03:27\n",
      "finish test\n",
      "2019-05-31 12:03:38\n",
      "finish evaluation\n",
      "2019-05-31 12:03:38\n"
     ]
    }
   ],
   "source": [
    "#main.py for test\n",
    "\n",
    "#import models.net as net\n",
    "#import bin.train_and_evaluate as train\n",
    "#import bin.test_and_evaluate as test\n",
    "\n",
    "# configure\n",
    "\n",
    "conf = {\n",
    "    \"data_path\": \"./data_tn.pkl\",\n",
    "    \"save_path\": \"./output_data_tn_final/TunisianDailect/temp/\",\n",
    "#    \"word_emb_init\": None,\n",
    "    \"word_emb_init\": \"./embedding_tn.pkl\",\n",
    "    \"init_model\": \"./output_data_tn_final/TunisianDailect/temp/model.ckpt.12.0\", \n",
    "\n",
    "    \"rand_seed\": None, \n",
    "\n",
    "    \"drop_dense\": None,\n",
    "    \"drop_attention\": None,\n",
    "\n",
    "    \"is_mask\": True,\n",
    "    \"is_layer_norm\": True,\n",
    "    \"is_positional\": False,  \n",
    "\n",
    "    \"stack_num\": 6,  \n",
    "    \"attention_type\": \"dot\",\n",
    "\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"vocab_size\": 36506,\n",
    "    \"emb_size\": 200,\n",
    "    \"batch_size\": 32, #200 for test\n",
    "\n",
    "    \"max_turn_num\": 2,  \n",
    "    \"max_turn_len\": 35, \n",
    "\n",
    "    \"max_to_keep\": 1,\n",
    "    \"num_scan_data\": 2,\n",
    "    \"_EOS_\": 1000000, #1 for douban data\n",
    "    \"final_n_class\": 1,\n",
    "}\n",
    "\n",
    "model_tn = Net(conf)\n",
    "\n",
    "#test and evaluation, init_model in conf should be set\n",
    "test(conf, model_tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [5, 10, 15, 20, 25, 30, 35, 40]\n",
    "\n",
    "R2_1 = [0.853852967227635,  0.8848538529672276, 0.8883968113374667, 0.8981399468556245, 0.9032258064516129, 0.8963684676705048, 0.9093437152391546, 0.8910540301151462]\n",
    "\n",
    "R10_1 = [0.525243578387954, 0.6111603188662533, 0.6120460584588131,  0.6129317980513729, 0.6140155728587319, 0.6155890168290522, 0.6101223581757509, 0.620903454384411]\n",
    "\n",
    "R10_2 = [0.7263064658990257, 0.7697077059344553, 0.7759078830823738, 0.7980513728963685, 0.7975528364849833, 0.7838795394154119, 0.7936596218020022, 0.7927369353410098]\n",
    "\n",
    "R10_5 = [0.9255978742249779, 0.9503985828166519, 0.9521700620017715, 0.9627989371124889, 0.9510567296996663, 0.9645704162976085, 0.9632925472747497, 0.9565987599645704]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "plt.plot(x, R2_1, label=\"R2@1\", c=\"r\")\n",
    "plt.plot(x, R10_1, label=\"R10@1\", c=\"g\")\n",
    "plt.plot(x, R10_2, label=\"R10@2\", c=\"m\")\n",
    "plt.plot(x, R10_5, label=\"R10@5\", c=\"c\")\n",
    "\n",
    "plt.xlabel(\"Maximum number of words per utterance\")\n",
    "plt.ylabel(\"Recall value\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "step = [21, 399, 441, 798, 819, 1051]\n",
    "lr = [0.001, 0.001, 0.00090000004, 0.00090000004, 0.00081, 0.00081]\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "plt.plot(step, lr, label=\"Learning Rate\", c=\"m\")\n",
    "\n",
    "\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Learning rate value\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "step =[21, 42, 63, 84, 105, 126, 147, 168, 189, 210, 231,252,273,294,315,336,357,378,399,420,441,462,483,504,525,546,567,588,609,630,651,672,693,714,735,756,777,798,819,840,861,882,903,924,945,966,987,1008,1029,1050]       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss= [2.8770870027087985,2.667029448917934,2.347165419941857,2.3695212489082698,2.4799336819421676,2.220272257214501,2.182331096558344,2.1039409126554216,2.1204018536068143,2.089290675662813,1.9607815061296736,1.8823109921954928,1.9707018988473075,1.6940540614582242,1.6640597638629733,1.5242644945780437,1.6171314318974812,1.5577048290343511,1.6042509504726954,1.7586784987222581,1.5633538251831418,1.5504274879183089,1.714630229132516,1.4031765148753212,1.3981914548646837,0.8928183501675016,0.8235293186846233,0.9344999761808486,0.9463290728273845,0.9035560602233523,0.8927205204963684,0.9718258778254191,0.9321696914377666,0.8085945163454328,0.9904912610848745,0.8839144266787029,0.8517989416917165,0.8598621345701671,0.8031353212538219,1.1045230357419877,0.8631675669125148,0.7940478693871271,0.7573546426636832,0.7633672952651978,0.6982252271402449,0.7130223037231536,0.7032639547472909,0.5746024946371714,0.6957006248689833,0.70598817723138]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "plt.plot(step, loss, label=\"Loss\", c=\"c\")\n",
    "\n",
    "\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Loss value\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-folders Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "dataset = r\"data_tn.test\"\n",
    "data_tn, word2vec_tn, max_len = pickle.load(open(dataset,\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "test3 = {\"y\":data_tn['y'][7400:9459], \"c\":data_tn['c'][7400:9459],\"r\":data_tn['r'][7400:9459]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "test3_data_tn = build_prediction_set(test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid3 = {\"y\":data_tn['y'][2800:4600], \"c\":data_tn['c'][2800:4600],\"r\":data_tn['r'][2800:4600]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid3_data_tn = build_prediction_set(valid3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1= {\"y\":data_tn['y'][3859:], \"c\":data_tn['c'][3859:],\"r\":data_tn['r'][3859:]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "train3= {\"y\":data_tn['y'][:2800]+data_tn['y'][4600:7400]+data_tn['y'][9459:], \"c\":data_tn['c'][:2800]+data_tn['c'][4600:7400]+data_tn['c'][9459:],\"r\":data_tn['r'][:2800]+data_tn['r'][4600:7400]+data_tn['r'][9459:]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "train3_data_tn = build_train_set(train3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed data prepeared!\n"
     ]
    }
   ],
   "source": [
    "indexed_valid3_tn = make_indexed_data_tn(valid3_data_tn,word2vec_tn.word_idx_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed data prepeared!\n"
     ]
    }
   ],
   "source": [
    "indexed_test3_tn = make_indexed_data_tn(test3_data_tn,word2vec_tn.word_idx_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed data prepeared!\n"
     ]
    }
   ],
   "source": [
    "indexed_train3_tn = make_indexed_data_tn(train3_data_tn,word2vec_tn.word_idx_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle hole final data\n",
    "pickle.dump([indexed_train3_tn, indexed_valid3_tn, indexed_test3_tn], open(\"data3_tn.pkl\",'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word emb init\n",
      "starting loading data\n",
      "2019-06-04 10:50:13\n",
      "finish loading data\n",
      "2019-06-04 10:50:13\n",
      "starting building validation batches\n",
      "finish building validation batches\n",
      "2019-06-04 10:50:13\n",
      "configurations: {'stack_num': 6, 'attention_type': 'dot', 'print_step': 5.25, 'data_path': './data3_tn.pkl', 'drop_dense': None, 'drop_attention': None, 'save_step': 52.5, 'emb_size': 200, 'is_layer_norm': True, 'is_positional': False, 'word_emb_init': './embedding_tn.pkl', 'learning_rate': 0.001, 'max_turn_num': 2, 'num_scan_data': 2, '_EOS_': 1000000, 'train_steps': 1050, 'save_path': './output_data3_tn_final/TunisianDailect/temp/', 'max_to_keep': 1, 'max_turn_len': 35, 'batch_size': 32, 'vocab_size': 36506, 'final_n_class': 1, 'is_mask': True, 'rand_seed': None, 'init_model': None}\n",
      "model sucess\n",
      "2019-06-04 10:50:13\n",
      "sim shape: (32, 2, 35, 35, 14)\n",
      "conv_0 shape: (32, 2, 35, 35, 32)\n",
      "pooling_0 shape: (32, 1, 12, 12, 32)\n",
      "conv_1 shape: (32, 1, 12, 12, 16)\n",
      "pooling_1 shape: (32, 1, 4, 4, 16)\n",
      "build graph sucess\n",
      "2019-06-04 10:50:38\n",
      "starting shuffle train data\n",
      "finish building train data\n",
      "step: 21, lr: 0.001\n",
      "processed: [0.04] loss: [2.7521927016122]\n",
      "step: 42, lr: 0.001\n",
      "processed: [0.08] loss: [2.332269066856021]\n",
      "step: 63, lr: 0.001\n",
      "processed: [0.12] loss: [2.291295880363101]\n",
      "step: 84, lr: 0.001\n",
      "processed: [0.16] loss: [2.02731036004566]\n",
      "step: 105, lr: 0.001\n",
      "processed: [0.2] loss: [2.2553403888429915]\n",
      "save step: 2.0\n",
      "2019-06-04 10:50:51\n",
      "result =  (0.8331479421579533, 0.4599555061179088, 0.6629588431590656, 0.9171301446051168)\n",
      "finish evaluation\n",
      "2019-06-04 10:51:01\n",
      "succ saving model in ./output_data3_tn_final/TunisianDailect/temp/model.ckpt.2.0\n",
      "2019-06-04 10:51:04\n",
      "step: 126, lr: 0.001\n",
      "processed: [0.24] loss: [2.082772277650379]\n",
      "step: 147, lr: 0.001\n",
      "processed: [0.28] loss: [1.9242441540672666]\n",
      "step: 168, lr: 0.001\n",
      "processed: [0.32] loss: [1.9538929348900205]\n",
      "step: 189, lr: 0.001\n",
      "processed: [0.36] loss: [1.897956513223194]\n",
      "step: 210, lr: 0.001\n",
      "processed: [0.4] loss: [1.8173013868786039]\n",
      "save step: 4.0\n",
      "2019-06-04 10:51:09\n",
      "result =  (0.864293659621802, 0.4838709677419355, 0.6991101223581757, 0.9432703003337041)\n",
      "finish evaluation\n",
      "2019-06-04 10:51:18\n",
      "succ saving model in ./output_data3_tn_final/TunisianDailect/temp/model.ckpt.4.0\n",
      "2019-06-04 10:51:20\n",
      "step: 231, lr: 0.001\n",
      "processed: [0.44] loss: [1.7614012899852933]\n",
      "step: 252, lr: 0.001\n",
      "processed: [0.48] loss: [1.736012373651777]\n",
      "step: 273, lr: 0.001\n",
      "processed: [0.52] loss: [1.8765439476285661]\n",
      "step: 294, lr: 0.001\n",
      "processed: [0.56] loss: [1.8978575468063354]\n",
      "step: 315, lr: 0.001\n",
      "processed: [0.6] loss: [1.6317006690161568]\n",
      "save step: 6.0\n",
      "2019-06-04 10:51:25\n",
      "result =  (0.8615127919911012, 0.5216907675194661, 0.7096774193548387, 0.9238042269187987)\n",
      "finish evaluation\n",
      "2019-06-04 10:51:34\n",
      "succ saving model in ./output_data3_tn_final/TunisianDailect/temp/model.ckpt.6.0\n",
      "2019-06-04 10:51:36\n",
      "step: 336, lr: 0.001\n",
      "processed: [0.64] loss: [1.6512770368939353]\n",
      "step: 357, lr: 0.001\n",
      "processed: [0.68] loss: [1.7083495287668138]\n",
      "step: 378, lr: 0.001\n",
      "processed: [0.72] loss: [1.437489146278018]\n",
      "step: 399, lr: 0.001\n",
      "processed: [0.76] loss: [1.5829658281235468]\n",
      "step: 420, lr: 0.00090000004\n",
      "processed: [0.8] loss: [1.557283966314225]\n",
      "save step: 8.0\n",
      "2019-06-04 10:51:40\n",
      "result =  (0.8826473859844272, 0.5305895439377085, 0.7380422691879867, 0.9421579532814238)\n",
      "finish evaluation\n",
      "2019-06-04 10:51:49\n",
      "succ saving model in ./output_data3_tn_final/TunisianDailect/temp/model.ckpt.8.0\n",
      "2019-06-04 10:51:51\n",
      "step: 441, lr: 0.00090000004\n",
      "processed: [0.84] loss: [1.4092198071025668]\n",
      "step: 462, lr: 0.00090000004\n",
      "processed: [0.88] loss: [1.375274223940713]\n",
      "step: 483, lr: 0.00090000004\n",
      "processed: [0.92] loss: [1.4085054000218709]\n",
      "step: 504, lr: 0.00090000004\n",
      "processed: [0.96] loss: [1.3073219458262126]\n",
      "step: 525, lr: 0.00090000004\n",
      "processed: [1.0] loss: [1.4142533796174186]\n",
      "save step: 10.0\n",
      "2019-06-04 10:51:56\n",
      "result =  (0.8932146829810901, 0.5717463848720801, 0.756952169076752, 0.9471635150166852)\n",
      "finish evaluation\n",
      "2019-06-04 10:52:05\n",
      "succ saving model in ./output_data3_tn_final/TunisianDailect/temp/model.ckpt.10.0\n",
      "2019-06-04 10:52:07\n",
      "starting shuffle train data\n",
      "finish building train data\n",
      "step: 546, lr: 0.00090000004\n",
      "processed: [1.04] loss: [0.7906909443083263]\n",
      "step: 567, lr: 0.00090000004\n",
      "processed: [1.08] loss: [0.9627110418819246]\n",
      "step: 588, lr: 0.00090000004\n",
      "processed: [1.12] loss: [0.8500787828649793]\n",
      "step: 609, lr: 0.00090000004\n",
      "processed: [1.16] loss: [0.83463111945561]\n",
      "step: 630, lr: 0.00090000004\n",
      "processed: [1.2] loss: [0.7418552906740279]\n",
      "save step: 12.0\n",
      "2019-06-04 10:52:12\n",
      "result =  (0.8932146829810901, 0.5667408231368187, 0.7541713014460512, 0.9488320355951056)\n",
      "finish evaluation\n",
      "2019-06-04 10:52:21\n",
      "step: 651, lr: 0.00090000004\n",
      "processed: [1.24] loss: [0.6065670819509597]\n",
      "step: 672, lr: 0.00090000004\n",
      "processed: [1.28] loss: [0.835622618595759]\n",
      "step: 693, lr: 0.00090000004\n",
      "processed: [1.32] loss: [0.9221553241922742]\n",
      "step: 714, lr: 0.00090000004\n",
      "processed: [1.36] loss: [0.785515179946309]\n",
      "step: 735, lr: 0.00090000004\n",
      "processed: [1.4] loss: [0.7416698336601257]\n",
      "save step: 14.0\n",
      "2019-06-04 10:52:26\n",
      "result =  (0.8882091212458287, 0.5978865406006674, 0.756952169076752, 0.9349276974416018)\n",
      "finish evaluation\n",
      "2019-06-04 10:52:35\n",
      "succ saving model in ./output_data3_tn_final/TunisianDailect/temp/model.ckpt.14.0\n",
      "2019-06-04 10:52:37\n",
      "step: 756, lr: 0.00090000004\n",
      "processed: [1.44] loss: [0.7573138177394867]\n",
      "step: 777, lr: 0.00090000004\n",
      "processed: [1.48] loss: [0.7083707480203538]\n",
      "step: 798, lr: 0.00090000004\n",
      "processed: [1.52] loss: [0.8134870898155939]\n",
      "step: 819, lr: 0.00081\n",
      "processed: [1.56] loss: [0.732062830101876]\n",
      "step: 840, lr: 0.00081\n",
      "processed: [1.6] loss: [0.6673562760863986]\n",
      "save step: 16.0\n",
      "2019-06-04 10:52:42\n",
      "result =  (0.8976640711902113, 0.6101223581757509, 0.7764182424916574, 0.9482758620689655)\n",
      "finish evaluation\n",
      "2019-06-04 10:52:51\n",
      "succ saving model in ./output_data3_tn_final/TunisianDailect/temp/model.ckpt.16.0\n",
      "2019-06-04 10:52:53\n",
      "step: 861, lr: 0.00081\n",
      "processed: [1.64] loss: [0.67159151889029]\n",
      "step: 882, lr: 0.00081\n",
      "processed: [1.68] loss: [0.6672190520025435]\n",
      "step: 903, lr: 0.00081\n",
      "processed: [1.72] loss: [0.6849897567714963]\n",
      "step: 924, lr: 0.00081\n",
      "processed: [1.76] loss: [0.8325960700001035]\n",
      "step: 945, lr: 0.00081\n",
      "processed: [1.8] loss: [0.7377465182826632]\n",
      "save step: 18.0\n",
      "2019-06-04 10:52:57\n",
      "result =  (0.9054505005561735, 0.610678531701891, 0.7825361512791991, 0.9610678531701891)\n",
      "finish evaluation\n",
      "2019-06-04 10:53:06\n",
      "succ saving model in ./output_data3_tn_final/TunisianDailect/temp/model.ckpt.18.0\n",
      "2019-06-04 10:53:08\n",
      "step: 966, lr: 0.00081\n",
      "processed: [1.84] loss: [0.7978509962558746]\n",
      "step: 987, lr: 0.00081\n",
      "processed: [1.88] loss: [0.7166177002446992]\n",
      "step: 1008, lr: 0.00081\n",
      "processed: [1.92] loss: [0.6001607789879754]\n",
      "step: 1029, lr: 0.00081\n",
      "processed: [1.96] loss: [0.5747095956688836]\n",
      "step: 1050, lr: 0.00081\n",
      "processed: [2.0] loss: [0.6188887684118181]\n",
      "save step: 20.0\n",
      "2019-06-04 10:53:13\n",
      "result =  (0.8959955506117909, 0.6140155728587319, 0.7719688542825361, 0.9482758620689655)\n",
      "finish evaluation\n",
      "2019-06-04 10:53:22\n"
     ]
    }
   ],
   "source": [
    "#main.py for train\n",
    "\n",
    "#import models.net as net\n",
    "#import bin.train_and_evaluate as train\n",
    "#import bin.test_and_evaluate as test\n",
    "\n",
    "# configure\n",
    "\n",
    "conf = {\n",
    "    \"data_path\": \"./data3_tn.pkl\",\n",
    "    \"save_path\": \"./output_data3_tn_final/TunisianDailect/temp/\",\n",
    "#    \"word_emb_init\": None,\n",
    "    \"word_emb_init\": \"./embedding_tn.pkl\",\n",
    "    \"init_model\": None, #should be set for test\n",
    "\n",
    "    \"rand_seed\": None, \n",
    "\n",
    "    \"drop_dense\": None,\n",
    "    \"drop_attention\": None,\n",
    "\n",
    "    \"is_mask\": True,\n",
    "    \"is_layer_norm\": True,\n",
    "    \"is_positional\": False,  \n",
    "\n",
    "    \"stack_num\": 6,  \n",
    "    \"attention_type\": \"dot\",\n",
    "\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"vocab_size\": 36506,\n",
    "    \"emb_size\": 200,\n",
    "    \"batch_size\": 32,     #200 for test\n",
    "\n",
    "    \"max_turn_num\": 2,  \n",
    "    \"max_turn_len\": 35, \n",
    "\n",
    "    \"max_to_keep\": 1,\n",
    "    \"num_scan_data\": 2,\n",
    "    \"_EOS_\": 1000000,      #1 for douban data\n",
    "    \"final_n_class\": 1,\n",
    "}\n",
    "\n",
    "\n",
    "model_tn = Net(conf)\n",
    "train(conf, model_tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word emb init\n",
      "starting loading data\n",
      "2019-06-04 10:55:51\n",
      "finish loading data\n",
      "2019-06-04 10:55:51\n",
      "starting building validation batches\n",
      "finish building validation batches\n",
      "2019-06-04 10:55:51\n",
      "configurations: {'stack_num': 6, 'attention_type': 'dot', 'data_path': './data3_tn.pkl', 'drop_dense': None, 'drop_attention': None, 'emb_size': 200, 'is_layer_norm': True, 'is_positional': False, 'word_emb_init': './embedding_tn.pkl', 'learning_rate': 0.001, 'max_turn_num': 2, 'num_scan_data': 2, '_EOS_': 1000000, 'save_path': './output_data3_tn_final/TunisianDailect/temp/', 'max_to_keep': 1, 'max_turn_len': 35, 'batch_size': 32, 'vocab_size': 36506, 'final_n_class': 1, 'is_mask': True, 'rand_seed': None, 'init_model': './output_data3_tn_final/TunisianDailect/temp/model.ckpt.18.0'}\n",
      "sim shape: (32, 2, 35, 35, 14)\n",
      "conv_0 shape: (32, 2, 35, 35, 32)\n",
      "pooling_0 shape: (32, 1, 12, 12, 32)\n",
      "conv_1 shape: (32, 1, 12, 12, 16)\n",
      "pooling_1 shape: (32, 1, 4, 4, 16)\n",
      "build graph sucess\n",
      "2019-06-04 10:56:17\n",
      "INFO:tensorflow:Restoring parameters from ./output_data3_tn_final/TunisianDailect/temp/model.ckpt.18.0\n",
      "sucess init ./output_data3_tn_final/TunisianDailect/temp/model.ckpt.18.0\n",
      "starting test\n",
      "2019-06-04 10:56:20\n",
      "finish test\n",
      "2019-06-04 10:56:31\n",
      "finish evaluation\n",
      "2019-06-04 10:56:31\n"
     ]
    }
   ],
   "source": [
    "#main.py for test\n",
    "\n",
    "#import models.net as net\n",
    "#import bin.train_and_evaluate as train\n",
    "#import bin.test_and_evaluate as test\n",
    "\n",
    "# configure\n",
    "\n",
    "conf = {\n",
    "    \"data_path\": \"./data3_tn.pkl\",\n",
    "    \"save_path\": \"./output_data3_tn_final/TunisianDailect/temp/\",\n",
    "#    \"word_emb_init\": None,\n",
    "    \"word_emb_init\": \"./embedding_tn.pkl\",\n",
    "    \"init_model\": \"./output_data3_tn_final/TunisianDailect/temp/model.ckpt.18.0\", \n",
    "\n",
    "    \"rand_seed\": None, \n",
    "\n",
    "    \"drop_dense\": None,\n",
    "    \"drop_attention\": None,\n",
    "\n",
    "    \"is_mask\": True,\n",
    "    \"is_layer_norm\": True,\n",
    "    \"is_positional\": False,  \n",
    "\n",
    "    \"stack_num\": 6,  \n",
    "    \"attention_type\": \"dot\",\n",
    "\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"vocab_size\": 36506,\n",
    "    \"emb_size\": 200,\n",
    "    \"batch_size\": 32, #200 for test\n",
    "\n",
    "    \"max_turn_num\": 2,  \n",
    "    \"max_turn_len\": 35, \n",
    "\n",
    "    \"max_to_keep\": 1,\n",
    "    \"num_scan_data\": 2,\n",
    "    \"_EOS_\": 1000000, #1 for douban data\n",
    "    \"final_n_class\": 1,\n",
    "}\n",
    "\n",
    "model_tn = Net(conf)\n",
    "\n",
    "#test and evaluation, init_model in conf should be set\n",
    "test(conf, model_tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "PFE_NourcheneFerchichi.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
